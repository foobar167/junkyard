{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# How to Build an AI Agent from Scratch in Raw Python\n","\n","Self-link: https://colab.research.google.com/drive/1g31aYu3ylEurdQjAJtnSxGFcx9aSZa_D\n","\n","Tutorial by Shrinivasan Sankar: https://www.ai-bites.net/lets-build-an-ai-agent-from-scratch-in-raw-python/?ref=ai-bites-newsletter\n","\n","GitHub scource code: https://github.com/ai-bites/langgraph-crash-course/blob/main/0_simple_python_agent.ipynb?ref=ai-bites.net\n","\n","Video: https://youtu.be/_TzW6F1NVsc"],"metadata":{"id":"iIaNC_RO_NYM"}},{"cell_type":"markdown","source":["## Tasks\n","\n","Run the Colab. Understand the main idea. Understand the source code.\n","\n","**Task-1:** Try to recognize the language and **automatically switching** to Chinese, English, or Russian. For example, you can ask the chatbot to automatically detect the language of the message in advance: Chinese, English or Russian. And then switch between URL types.\n","\n","**Task-2 (hard):** **Weather forecast.** Add functionality to the chatbot to answer questions about the upcoming weather.\n","\n","*Optional task:*  Try similar API for Chinese chatbot [QwenAI](https://chat.qwen.ai). Use [QwenAI Python API library](https://pypi.org/project/qwenai/) for tests.\n","\n","*Optional task:* Try similar API for Chinese chatbot [DeepSeek](https://www.deepseek.com/). You can use the following links: [Your First API Call](https://api-docs.deepseek.com/), [An In-Depth Step-by-Step Guide](https://dev.to/auden/how-to-call-the-deepseek-r1-api-using-python-an-in-depth-step-by-step-guide-311o), [langchain_deepseek](https://python.langchain.com/api_reference/deepseek/chat_models/langchain_deepseek.chat_models.ChatDeepSeek.html) library.\n","\n","*Optional task:* Conduct a comparative analysis of the API of three chatbots: DeepSeek, OpenAI, and Qwen. Describe strengths and weaknesses of each of them. Which one do you personally find more convenient to work with?"],"metadata":{"id":"WD-6nPgdMeEK"}},{"cell_type":"markdown","source":["## Install necessary libraries"],"metadata":{"id":"o-uFbIr_R3uG"}},{"cell_type":"code","source":["!pip install openai httpx dotenv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IW3-zNndgBY2","executionInfo":{"status":"ok","timestamp":1753775624493,"user_tz":-180,"elapsed":9318,"user":{"displayName":"Dzmitry Paulenka","userId":"15727351917421840757"}},"outputId":"ba946814-df01-43d6-8bfb-9c645047d1d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.97.1)\n","Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (0.28.1)\n","Requirement already satisfied: dotenv in /usr/local/lib/python3.11/dist-packages (0.9.9)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n","Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx) (2025.7.14)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx) (1.0.9)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx) (3.10)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx) (0.16.0)\n","Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from dotenv) (1.1.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"]}]},{"cell_type":"markdown","source":["## Import necessary packages"],"metadata":{"id":"d-mEIl4bZSBG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OwUnMhTc-WEt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1753775625446,"user_tz":-180,"elapsed":949,"user":{"displayName":"Dzmitry Paulenka","userId":"15727351917421840757"}},"outputId":"4fc17967-f993-4791-f86e-acbc0150ace6"},"outputs":[{"output_type":"stream","name":"stdout","text":["3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n","Python version: 3.11.13\n"]}],"source":["from openai import OpenAI\n","import re\n","import httpx\n","import requests\n","import xml.etree.ElementTree as ET\n","import json\n","\n","# Print the full version information string.\n","import sys\n","print(sys.version)\n","\n","# Show the Python version.\n","import platform\n","print(f\"Python version: {platform.python_version()}\")"]},{"cell_type":"markdown","source":["**NOTE:** You have to set your OPENAI_API_KEY in a `my_openai_key.env` file and load it on Google Colab."],"metadata":{"id":"-VKQFT5MKhZZ"}},{"cell_type":"code","source":["import os\n","import openai\n","\n","from dotenv import load_dotenv\n","\n","\n","MODEL = \"gpt-4o-mini\"  # Chat-GPT model name\n","\n","# Set your OPENAI_API_KEY in a \"my_openai_key.env\" file and load it.\n","load_dotenv(\"my_openai_key_2.env\")  # OPENAI_API_KEY\n","# !echo $OPENAI_API_KEY\n","\n","client = OpenAI(api_key=os.environ['OPENAI_API_KEY'],)\n","\n","# Check to see if the LLM works.\n","chat_completion = client.chat.completions.create(\n","    model=MODEL,\n","    messages=[{\"role\": \"user\", \"content\": \"Hello there!\"}],\n",")\n","\n","# The anser should be: \"Hello! How can I assist you today?\"\n","print(chat_completion.choices[0].message.content)"],"metadata":{"id":"wwhnWgzKCXxc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1753775626547,"user_tz":-180,"elapsed":1102,"user":{"displayName":"Dzmitry Paulenka","userId":"15727351917421840757"}},"outputId":"a09d315f-95cd-4e4f-8488-897f37adafd0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello! How can I assist you today?\n"]}]},{"cell_type":"markdown","source":["## Chatbot\n","Create a chatbot that takes raw messages, stacks them together, and uses the LLM to respond to the messages."],"metadata":{"id":"y37HgspAwETP"}},{"cell_type":"code","source":["class ChatBot:\n","    def __init__(self, system=\"\"):\n","        self.system = system\n","        self.messages = []\n","        if self.system:\n","            self.messages.append({\"role\": \"system\", \"content\": self.system})\n","\n","\n","    def __call__(self, message):\n","        self.messages.append({\"role\": \"user\", \"content\": message})\n","        result = self.run_llm()\n","        self.messages.append({\"role\": \"assistant\", \"content\": result})\n","        return result\n","\n","\n","    def run_llm(self):\n","        completion = client.chat.completions.create(\n","            model=MODEL,\n","            temperature=0,\n","            messages=self.messages,\n","        )\n","        return completion.choices[0].message.content"],"metadata":{"id":"30bvGkKpcIgJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Agent\n","Wrap the above chatbot in Agent class to create ReAct Agent."],"metadata":{"id":"sOWyHh07w58Y"}},{"cell_type":"code","source":["# abot = Agent(prompt)\n","action_re = re.compile('^Action: (\\w+): (.*)$')\n","\n","\n","class Agent:\n","    def __init__(self, system_prompt=\"\", max_turns=1, known_actions=None):\n","        self.max_turns = max_turns\n","        self.bot = ChatBot(system_prompt)\n","        self.known_actions = known_actions\n","\n","\n","    def run(self, question):\n","        i = 0  # iteration\n","        next_prompt = question\n","\n","        while i < self.max_turns:  # loop through iterations\n","            i += 1\n","            result = self.bot(next_prompt)\n","            print(f\"\\n Iteration: {i} \\n\\n\"\n","                  f\"{result}\\n\\n\")\n","            actions = [action_re.match(a) for a in result.split('\\n') if action_re.match(a)]\n","\n","            if actions:  # there is an action to run\n","                action, action_input = actions[0].groups()\n","                if action not in self.known_actions:\n","                    raise Exception(f\"Unknown action: {action}: {action_input}\")\n","                print(f\" -- running {action} {action_input} --\")\n","\n","                # Make a request to Wikipedia, ArXiv or calculator using action functions.\n","                # print(self.known_actions)\n","                observation = self.known_actions[action](action_input)\n","\n","                next_prompt = f\"Observation: {observation}\"\n","                print(\"Next prompt:\", next_prompt)\n","            else:\n","                return"],"metadata":{"id":"RrUecbQWrNFY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Actions\n","Define actions as functions.\n","\n","The arxiv_search function will invoke arxiv API to fetch data for the agent.\n","\n","The Wikipedia function will use httpx to query Wikipedia and fetch knowledge about any topic unknown to the agent.\n","\n","The calculate function is a Python hack to evaluate any equations\n","given to it."],"metadata":{"id":"WDJ3TMvJyT4V"}},{"cell_type":"code","source":["def wikipedia(q):\n","    try:\n","        return httpx.get(\"https://en.wikipedia.org/w/api.php\", params={\n","            \"action\": \"query\",\n","            \"list\": \"search\",\n","            \"srsearch\": q,\n","            \"format\": \"json\"\n","        }).json()[\"query\"][\"search\"][0][\"snippet\"]\n","\n","    except IndexError:  # try Russian language instead\n","        return httpx.get(\"https://ru.wikipedia.org/w/api.php\", params={\n","            \"action\": \"query\",\n","            \"list\": \"search\",\n","            \"srsearch\": q,\n","            \"format\": \"json\"\n","        }).json()[\"query\"][\"search\"][0][\"snippet\"]\n","\n","\n","def arxiv_search(q):\n","    ARXIV_NAMESPACE = '{http://www.w3.org/2005/Atom}'\n","\n","    url = f'http://export.arxiv.org/api/query?search_query=all:{q}&start=0&max_results=1'\n","    res = requests.get(url)\n","    et_root = ET.fromstring(res.content)\n","\n","    for entry in et_root.findall(f\"{ARXIV_NAMESPACE}entry\"):\n","        title = entry.find(f\"{ARXIV_NAMESPACE}title\").text.strip()\n","        summary = entry.find(f\"{ARXIV_NAMESPACE}summary\").text.strip()\n","    return json.dumps({\"title\" : title, \"summary\" : summary})\n","\n","\n","def calculate(what):\n","    return eval(what)"],"metadata":{"id":"FNKUHXkarYDr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Prompt\n","Prompting is quite crucial for ReAct agents. They generally work by few-shot prompting. Meaning, we need to provide a few examples of what is expected of the agent in the prompt itself. For this reason, we need to write a detailed prompt with a few examples. Below is the prompt for the task.\n","\n","Nothing fancy here other than pro-level prompting. We have listed all the actions, explicitly mentioned, “run in a loop of Thought, Action, PAUSE, Observation” so that the agent can invoke actions and think of the perceived observations."],"metadata":{"id":"J-H5M5eAzBxx"}},{"cell_type":"code","source":["prompt = \"\"\"\n","You run in a loop of Thought, Action, PAUSE, Observation.\n","At the end of the loop you output an Answer\n","Use Thought to describe your thoughts about the question you have been asked.\n","Use Action to run one of the actions available to you - then return PAUSE.\n","Observation will be the result of running those actions.\n","\n","Your available actions are:\n","\n","calculate:\n","e.g. calculate: 4 * 7 / 3\n","Runs a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\n","\n","wikipedia:\n","e.g. wikipedia:\n","Returns a summary from searching Wikipedia\n","\n","arxiv_search:\n","e.g. arxiv_search:\n","Returns a summary of research papers\n","\n","Example session:\n","\n","Question: What is the capital of France?\n","Thought: I should look up France on Wikipedia\n","Action: wikipedia: France\n","PAUSE\n","\n","You will be called again with this:\n","\n","Observation: France is a country. The capital is Paris.\n","\n","You then output:\n","\n","Answer: The capital of France is Paris\n","\"\"\".strip()"],"metadata":{"id":"lb0hIIFLy4nU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test the Agent\n","It's time to test out our ReAct agent. Let's collate all the actions in a dictionary, create an instance of the agent, and invoke its run method to run the agent."],"metadata":{"id":"rLatnn0CzouZ"}},{"cell_type":"code","source":["known_actions = {\n","    \"wikipedia\": wikipedia,\n","    \"calculate\": calculate,\n","    \"arxiv_search\": arxiv_search\n","}\n","\n","agent = Agent(prompt, max_turns=3, known_actions=known_actions)\n","\n","\n","def run_tests(question):\n","    bot = ChatBot(question)  # run chatbot without agent\n","    print(bot.run_llm())\n","\n","    print(f\"\\n----------\")\n","    agent.run(question)  # run agent"],"metadata":{"id":"yBxQn310zdh2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1st Question -- Easy"],"metadata":{"id":"9Dff2bZEDl4-"}},{"cell_type":"code","source":["question = \"What is the capital of Indonesia?\"\n","run_tests(question)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qBPIib_GCJD4","executionInfo":{"status":"ok","timestamp":1753775650421,"user_tz":-180,"elapsed":23479,"user":{"displayName":"Dzmitry Paulenka","userId":"15727351917421840757"}},"outputId":"7464fe7e-d8e7-42b7-cc4a-90043a599ed7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The capital of Indonesia is Jakarta.\n","\n","----------\n","\n"," Iteration: 1 \n","\n","Thought: I should look up Indonesia on Wikipedia to find out its capital.  \n","Action: wikipedia: Indonesia  \n","PAUSE\n","\n","\n"," -- running wikipedia Indonesia   --\n","Next prompt: Observation: <span class=\"searchmatch\">Indonesia</span>, officially the Republic of <span class=\"searchmatch\">Indonesia</span>, is a country in Southeast Asia and Oceania, between the Indian and Pacific oceans. Comprising over 17\n","\n"," Iteration: 2 \n","\n","Answer: The capital of Indonesia is Jakarta.\n","\n","\n"]}]},{"cell_type":"markdown","source":["### 2nd Question -- Easy"],"metadata":{"id":"lOrGQ_pBDsdV"}},{"cell_type":"code","source":["question = \"what is the sum of 7 and 5?\"\n","run_tests(question)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8iEEHXIoDD4s","executionInfo":{"status":"ok","timestamp":1753775718409,"user_tz":-180,"elapsed":67995,"user":{"displayName":"Dzmitry Paulenka","userId":"15727351917421840757"}},"outputId":"34fdc178-cbd3-4f50-da34-6200bd7ceedb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The sum of 7 and 5 is 12.\n","\n","----------\n","\n"," Iteration: 1 \n","\n","Thought: I can calculate the sum of 7 and 5 directly.  \n","Action: calculate: 7 + 5  \n","PAUSE\n","\n","\n"," -- running calculate 7 + 5   --\n","Next prompt: Observation: 12\n","\n"," Iteration: 2 \n","\n","Answer: The sum of 7 and 5 is 12.\n","\n","\n"]}]},{"cell_type":"markdown","source":["### 3rd Question -- Difficult"],"metadata":{"id":"fYY-dg4QD34Y"}},{"cell_type":"code","source":["question = \"explain the contrastive learning paper\"\n","run_tests(question)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DWYR53unDPvz","executionInfo":{"status":"ok","timestamp":1753775780022,"user_tz":-180,"elapsed":61616,"user":{"displayName":"Dzmitry Paulenka","userId":"15727351917421840757"}},"outputId":"bef5fe16-29d1-471a-9fe9-95ea271156c0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Contrastive learning is a type of self-supervised learning technique that aims to learn representations by contrasting positive and negative pairs of data points. The key idea is to bring similar data points closer together in the representation space while pushing dissimilar points apart. This approach has gained significant attention in various domains, including computer vision, natural language processing, and audio processing.\n","\n","While I don't have access to specific papers published after October 2023, I can provide a general overview of the typical structure and findings of contrastive learning papers based on the existing literature up to that date.\n","\n","### Key Components of Contrastive Learning Papers\n","\n","1. **Introduction**:\n","   - The paper usually starts with an introduction to the problem being addressed and the motivation for using contrastive learning. It often highlights the limitations of supervised learning and the potential of self-supervised methods.\n","\n","2. **Methodology**:\n","   - **Data Augmentation**: The paper describes how data augmentation techniques are used to create positive pairs. For example, two different augmentations of the same image can be considered a positive pair, while images from different classes are treated as negative pairs.\n","   - **Contrastive Loss**: The core of the methodology is the contrastive loss function, such as the InfoNCE loss, which encourages the model to maximize the similarity between positive pairs and minimize the similarity between negative pairs.\n","   - **Architecture**: The paper may detail the neural network architecture used for feature extraction, such as convolutional neural networks (CNNs) for images or transformers for text.\n","\n","3. **Experiments**:\n","   - The authors typically conduct experiments to evaluate the effectiveness of their approach. This may include:\n","     - Comparing the learned representations on downstream tasks (e.g., classification, retrieval).\n","     - Evaluating the impact of different augmentation strategies.\n","     - Analyzing the performance with varying numbers of negative samples.\n","\n","4. **Results**:\n","   - The results section presents quantitative metrics (e.g., accuracy, F1 score) and qualitative analyses (e.g., visualizations of learned representations). The authors often compare their method against baseline models and state-of-the-art approaches.\n","\n","5. **Discussion**:\n","   - The discussion section interprets the results, addressing the strengths and weaknesses of the proposed method. It may also explore the implications of the findings and potential future work.\n","\n","6. **Conclusion**:\n","   - The paper concludes by summarizing the contributions and suggesting directions for future research in contrastive learning.\n","\n","### Applications\n","Contrastive learning has been successfully applied in various fields, including:\n","- **Computer Vision**: Learning visual representations for tasks like image classification and object detection.\n","- **Natural Language Processing**: Learning sentence or document embeddings for tasks like semantic similarity and text classification.\n","- **Audio Processing**: Learning representations for audio signals for tasks like speech recognition and music classification.\n","\n","### Conclusion\n","Contrastive learning has emerged as a powerful framework for self-supervised representation learning, enabling models to learn meaningful features from unlabeled data. The ongoing research in this area continues to refine techniques, improve performance, and expand applications across different domains. If you have a specific paper in mind, I can help summarize its contributions and findings based on the general principles of contrastive learning.\n","\n","----------\n","\n"," Iteration: 1 \n","\n","Thought: I should search for research papers on contrastive learning to provide a summary of the relevant literature.  \n","Action: arxiv_search: contrastive learning  \n","PAUSE\n","\n","\n"," -- running arxiv_search contrastive learning   --\n","Next prompt: Observation: {\"title\": \"Heterogeneous Contrastive Learning for Foundation Models and Beyond\", \"summary\": \"In the era of big data and Artificial Intelligence, an emerging paradigm is\\nto utilize contrastive self-supervised learning to model large-scale\\nheterogeneous data. Many existing foundation models benefit from the\\ngeneralization capability of contrastive self-supervised learning by learning\\ncompact and high-quality representations without relying on any label\\ninformation. Amidst the explosive advancements in foundation models across\\nmultiple domains, including natural language processing and computer vision, a\\nthorough survey on heterogeneous contrastive learning for the foundation model\\nis urgently needed. In response, this survey critically evaluates the current\\nlandscape of heterogeneous contrastive learning for foundation models,\\nhighlighting the open challenges and future trends of contrastive learning. In\\nparticular, we first present how the recent advanced contrastive learning-based\\nmethods deal with view heterogeneity and how contrastive learning is applied to\\ntrain and fine-tune the multi-view foundation models. Then, we move to\\ncontrastive learning methods for task heterogeneity, including pretraining\\ntasks and downstream tasks, and show how different tasks are combined with\\ncontrastive learning loss for different purposes. Finally, we conclude this\\nsurvey by discussing the open challenges and shedding light on the future\\ndirections of contrastive learning.\"}\n","\n"," Iteration: 2 \n","\n","Answer: The paper titled \"Heterogeneous Contrastive Learning for Foundation Models and Beyond\" discusses the use of contrastive self-supervised learning to model large-scale heterogeneous data. It highlights how foundation models in various domains, such as natural language processing and computer vision, benefit from contrastive learning by generating high-quality representations without needing labeled data. The paper surveys the current landscape of heterogeneous contrastive learning, addressing challenges and future trends. It covers how advanced methods handle view heterogeneity and apply contrastive learning to multi-view models, as well as how it integrates with different tasks. The paper concludes by discussing open challenges and future directions in contrastive learning.\n","\n","\n"]}]},{"cell_type":"code","source":["question = \"explain the lightrag paper\"\n","run_tests(question)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V3we7PCc5iqA","executionInfo":{"status":"ok","timestamp":1753775834566,"user_tz":-180,"elapsed":54543,"user":{"displayName":"Dzmitry Paulenka","userId":"15727351917421840757"}},"outputId":"3bd081e4-b9be-4996-bb9c-7c79c43082b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The \"LightrAG\" paper refers to a research work that presents a novel approach to the problem of image generation and manipulation using a lightweight architecture. While I don't have access to the specific content of the paper, I can provide a general overview of what such a paper might cover based on common themes in the field of generative models and lightweight architectures.\n","\n","### Key Concepts Likely Covered in the LightrAG Paper:\n","\n","1. **Generative Models**: The paper likely discusses generative models, which are algorithms that can generate new data instances that resemble a given dataset. Common examples include Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs).\n","\n","2. **Lightweight Architecture**: The term \"lightweight\" suggests that the authors have designed a model that is efficient in terms of computational resources. This could involve reducing the number of parameters, optimizing the architecture for faster inference, or using techniques like pruning and quantization.\n","\n","3. **Applications**: The paper may explore various applications of the proposed model, such as image synthesis, style transfer, or data augmentation. It might also discuss how the lightweight nature of the model makes it suitable for deployment on edge devices or in real-time applications.\n","\n","4. **Performance Metrics**: The authors would likely present experiments comparing their model's performance against existing state-of-the-art models. This could include metrics such as image quality, generation speed, and resource consumption.\n","\n","5. **Training Techniques**: The paper might detail the training process, including the dataset used, loss functions, and any novel training techniques that improve the model's performance.\n","\n","6. **Results and Discussion**: The results section would present quantitative and qualitative findings, showcasing the model's capabilities. The discussion might address the implications of the findings, potential limitations, and future work.\n","\n","7. **Conclusion**: The paper would typically conclude with a summary of the contributions made by the research and suggestions for future research directions.\n","\n","### Importance of Lightweight Models:\n","\n","Lightweight models are increasingly important in the field of machine learning, especially for applications that require real-time processing or deployment on devices with limited computational power, such as mobile phones or IoT devices. By focusing on efficiency without sacrificing performance, researchers can make advanced AI technologies more accessible and practical for a wider range of applications.\n","\n","If you have specific sections or details from the LightrAG paper that you would like to discuss, feel free to share, and I can provide more targeted insights!\n","\n","----------\n","\n"," Iteration: 1 \n","\n","Thought: I should search for research papers on \"lightrag\" to provide a summary of the relevant literature.  \n","Action: arxiv_search: lightrag  \n","PAUSE\n","\n","\n"," -- running arxiv_search lightrag   --\n","Next prompt: Observation: {\"title\": \"LightRAG: Simple and Fast Retrieval-Augmented Generation\", \"summary\": \"Retrieval-Augmented Generation (RAG) systems enhance large language models\\n(LLMs) by integrating external knowledge sources, enabling more accurate and\\ncontextually relevant responses tailored to user needs. However, existing RAG\\nsystems have significant limitations, including reliance on flat data\\nrepresentations and inadequate contextual awareness, which can lead to\\nfragmented answers that fail to capture complex inter-dependencies. To address\\nthese challenges, we propose LightRAG, which incorporates graph structures into\\ntext indexing and retrieval processes. This innovative framework employs a\\ndual-level retrieval system that enhances comprehensive information retrieval\\nfrom both low-level and high-level knowledge discovery. Additionally, the\\nintegration of graph structures with vector representations facilitates\\nefficient retrieval of related entities and their relationships, significantly\\nimproving response times while maintaining contextual relevance. This\\ncapability is further enhanced by an incremental update algorithm that ensures\\nthe timely integration of new data, allowing the system to remain effective and\\nresponsive in rapidly changing data environments. Extensive experimental\\nvalidation demonstrates considerable improvements in retrieval accuracy and\\nefficiency compared to existing approaches. We have made our LightRAG\\nopen-source and available at the link: https://github.com/HKUDS/LightRAG\"}\n","\n"," Iteration: 2 \n","\n","Answer: The paper titled \"LightRAG: Simple and Fast Retrieval-Augmented Generation\" introduces a new framework for Retrieval-Augmented Generation (RAG) systems that enhances large language models (LLMs) by integrating external knowledge sources. LightRAG addresses limitations of existing RAG systems, such as reliance on flat data representations and lack of contextual awareness, which can lead to fragmented answers. The proposed framework incorporates graph structures into text indexing and retrieval, employing a dual-level retrieval system for comprehensive information retrieval. This integration improves the efficiency of retrieving related entities and their relationships while maintaining contextual relevance. Additionally, an incremental update algorithm allows for timely integration of new data, ensuring effectiveness in dynamic environments. Experimental results show significant improvements in retrieval accuracy and efficiency compared to existing methods. The LightRAG framework is open-source and available for use.\n","\n","\n"]}]},{"cell_type":"markdown","source":["### 4th Qestion -- Difficult"],"metadata":{"id":"vYyyYxBUElZ9"}},{"cell_type":"code","source":["question = \"What is AI Bites?\"\n","run_tests(question)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_4EI_EeeE3Az","executionInfo":{"status":"ok","timestamp":1753775899801,"user_tz":-180,"elapsed":65241,"user":{"displayName":"Dzmitry Paulenka","userId":"15727351917421840757"}},"outputId":"58801512-330b-4dd0-a787-db82b0800d29"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["AI Bites is a term that can refer to short, digestible pieces of information or insights related to artificial intelligence. This could include quick facts, tips, news updates, or summaries of AI concepts and technologies. The idea is to provide easily understandable and concise content that can help individuals stay informed about the rapidly evolving field of AI. If you have a specific context or topic in mind regarding AI Bites, feel free to share!\n","\n","----------\n","\n"," Iteration: 1 \n","\n","Thought: I should look up \"AI Bites\" on Wikipedia to find out what it is.  \n","Action: wikipedia: AI Bites  \n","PAUSE\n","\n","\n"," -- running wikipedia AI Bites   --\n","Next prompt: Observation: &quot;Retrieval Augmented Generation(RAG) — A quick and comprehensive introduction&quot;. <span class=\"searchmatch\">ai</span>-<span class=\"searchmatch\">bites</span>.net. Kiela Douwe, Ho Alan (Oct 13, 2023). &quot;Where did Retrieval Augmented\n","\n"," Iteration: 2 \n","\n","Answer: AI Bites appears to be a platform or resource that provides quick and comprehensive introductions to topics related to artificial intelligence, such as Retrieval Augmented Generation (RAG). However, specific details about its content or structure were not found in the search. For more precise information, visiting the website directly may be beneficial.\n","\n","\n"]}]},{"cell_type":"markdown","source":["### 5th Question -- Difficult"],"metadata":{"id":"K9708y5GZ0Tu"}},{"cell_type":"code","source":["# Default answer is wrong. The answer after Wikipedia is write.\n","question = \"Who is Alexander Tuzikov?\"\n","run_tests(question)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c7L0v_Vt1Tn2","executionInfo":{"status":"ok","timestamp":1753775963631,"user_tz":-180,"elapsed":63828,"user":{"displayName":"Dzmitry Paulenka","userId":"15727351917421840757"}},"outputId":"7079f181-63fe-443f-abe8-6c2acdb3f331"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["As of my last knowledge update in October 2023, there is no widely recognized public figure or notable individual named Alexander Tuzikov. It's possible that he may be a private individual or a person who has gained prominence after that date. If you have more context or specific details about him, I may be able to provide more relevant information.\n","\n","----------\n","\n"," Iteration: 1 \n","\n","Thought: I should look up \"Alexander Tuzikov\" on Wikipedia to find out more about him.  \n","Action: wikipedia: Alexander Tuzikov  \n","PAUSE\n","\n","\n"," -- running wikipedia Alexander Tuzikov   --\n","Next prompt: Observation: Ryabova (Deputy Minister of Communications and Informatization), <span class=\"searchmatch\">Alexander</span> <span class=\"searchmatch\">Tuzikov</span> [ru] (General Director of the United Institute of Informatics Problems\n","\n"," Iteration: 2 \n","\n","Answer: Alexander Tuzikov is the General Director of the United Institute of Informatics Problems.\n","\n","\n"]}]},{"cell_type":"code","source":["# Here the default answer is incorrect,\n","#   but after checking it on Wikipedia it turns out to be correct.\n","question = \"Кто такой Александр Васильевич Тузиков?\"\n","run_tests(question)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_37Ldd2efIxy","executionInfo":{"status":"ok","timestamp":1753776009804,"user_tz":-180,"elapsed":46167,"user":{"displayName":"Dzmitry Paulenka","userId":"15727351917421840757"}},"outputId":"76957e6b-0b25-401f-aa65-ef7aea62e85d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["На момент моего последнего обновления информации в октябре 2023 года, нет широко известных данных о человеке по имени Александр Васильевич Тузиков. Возможно, это частное лицо или человек, не имеющий значительной публичной известности. Если у вас есть конкретные детали или контекст, связанные с этой персоной, я постараюсь помочь вам лучше.\n","\n","----------\n","\n"," Iteration: 1 \n","\n","Thought: Я должен поискать информацию о \"Александре Васильевиче Тузикове\" в Википедии, чтобы узнать о нем больше.  \n","Action: wikipedia: Александр Васильевич Тузиков  \n","PAUSE\n","\n","\n"," -- running wikipedia Александр Васильевич Тузиков   --\n","Next prompt: Observation: <span class=\"searchmatch\">Александр</span> <span class=\"searchmatch\">Васильевич</span> <span class=\"searchmatch\">Тузиков</span> (род. 5 сентября 1958 года, Полоцк) — белорусский учёный, c 2008 по 2022 год -генеральный директор Объединенного института\n","\n"," Iteration: 2 \n","\n","Answer: Александр Васильевич Тузиков (род. 5 сентября 1958 года, Полоцк) — белорусский учёный, который с 2008 по 2022 год занимал должность генерального директора Объединенного института информатики.\n","\n","\n"]}]},{"cell_type":"code","source":["# Wrong answer\n","question = \"What are the scientific works of Alexandra Kosareva?\"\n","run_tests(question)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sOzPc2-0aUFc","executionInfo":{"status":"ok","timestamp":1753776080197,"user_tz":-180,"elapsed":70375,"user":{"displayName":"Dzmitry Paulenka","userId":"15727351917421840757"}},"outputId":"525cca2d-f35d-440d-9a07-098419a6f8fa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["As of my last update in October 2023, I do not have specific information on the scientific works of Alexandra Kosareva. To find her publications or contributions to scientific research, I recommend checking academic databases such as Google Scholar, PubMed, or institutional repositories. You can also look for her profile on research networking sites like ResearchGate or LinkedIn, where researchers often list their publications and projects. If you have a specific field or topic in mind, I can help guide you on how to search for relevant works.\n","\n","----------\n","\n"," Iteration: 1 \n","\n","Thought: I should search for scientific works by Alexandra Kosareva to provide a summary of her contributions.  \n","Action: arxiv_search: Alexandra Kosareva  \n","PAUSE\n","\n","\n"," -- running arxiv_search Alexandra Kosareva   --\n","Next prompt: Observation: {\"title\": \"All-optical attoclock for imaging tunnelling wavepackets\", \"summary\": \"Recent experiments on measuring time-delays during tunnelling of cold atoms\\nthrough an optically created potential barrier are reinvigorating the\\ncontroversial debate regarding possible time-delays during light-induced\\ntunnelling of an electron from an atom. Compelling theoretical and experimental\\narguments have been put forward to advocate opposite views, confirming or\\nrefuting the existence of finite tunnelling time delays. Yet, such a delay,\\nwhether present or not, is but a single quantity characterizing the tunnelling\\nwavepacket; the underlying dynamics are richer. Here we propose to augment\\nphoto-electron detection in laser-induced tunnelling with detection of light\\nemitted by the tunnelling electron -- the so-called Brunel radiation. Using a\\ncombination of single-color and two-color driving fields, we identify the\\nall-optical signatures of the re-shaping of the tunnelling wavepacket as it\\nemerges from the tunnelling barrier and moves away from the core. This\\nreshaping includes not only an effective time-delay but also time-reversal\\nasymmetry of the ionization process, which we describe theoretically and\\nobserve experimentally. We show how both delay and reshaping are mapped on the\\npolarization properties of the Brunel radiation, with different harmonics\\nbehaving as different hands of a clock moving at different speeds. The\\nall-optical detection paves the way to time-resolving optical tunnelling in\\ncondensed matter systems, e.g. tunnelling across bandgaps in solids, on the\\nattosecond time-scale.\"}\n","\n"," Iteration: 2 \n","\n","Answer: One of the scientific works by Alexandra Kosareva is titled \"All-optical attoclock for imaging tunnelling wavepackets.\" This work discusses recent experiments measuring time delays during the tunneling of cold atoms through an optically created potential barrier, contributing to the debate on time delays in light-induced tunneling of electrons. The paper proposes augmenting photo-electron detection with the detection of light emitted by the tunneling electron, known as Brunel radiation. It identifies all-optical signatures of the reshaping of the tunneling wavepacket as it emerges from the barrier, including effective time delays and time-reversal asymmetry in the ionization process. The study shows how these phenomena are related to the polarization properties of the Brunel radiation, paving the way for time-resolving optical tunneling in condensed matter systems on an attosecond time scale.\n","\n","\n"]}]},{"cell_type":"code","source":["# Wrong answer\n","question = \"What are the scientific works of Dzmitry Paulenka?\"\n","run_tests(question)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rLdKLjnAczeh","executionInfo":{"status":"ok","timestamp":1753776136075,"user_tz":-180,"elapsed":55922,"user":{"displayName":"Dzmitry Paulenka","userId":"15727351917421840757"}},"outputId":"d58dbde9-371d-4fb1-e20d-d587dd161ecd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["As of my last update in October 2023, I do not have specific information on the scientific works of Dzmitry Paulenka. To find his publications or contributions to scientific research, I recommend checking academic databases such as Google Scholar, ResearchGate, or institutional repositories. You can also look for his work in relevant journals or conference proceedings in his field of expertise. If you have access to a university library, they may also provide resources to help you find his research.\n","\n","----------\n","\n"," Iteration: 1 \n","\n","Thought: I should search for scientific works by Dzmitry Paulenka to provide a summary of his contributions.  \n","Action: arxiv_search: Dzmitry Paulenka  \n","PAUSE\n","\n","\n"," -- running arxiv_search Dzmitry Paulenka   --\n","Next prompt: Observation: {\"title\": \"The Decoration Theorem for Mandelbrot and Multibrot Sets\", \"summary\": \"We prove the decoration theorem for the Mandelbrot set (and Multibrot sets)\\nwhich says that when a \\\"little Mandelbrot set\\\" is removed from the Mandelbrot\\nset, then most of the resulting connected components have small diameters.\"}\n","\n"," Iteration: 2 \n","\n","Answer: One of the scientific works by Dzmitry Paulenka is titled \"The Decoration Theorem for Mandelbrot and Multibrot Sets.\" In this work, the author proves the decoration theorem for the Mandelbrot set and Multibrot sets. The theorem states that when a \"little Mandelbrot set\" is removed from the Mandelbrot set, most of the resulting connected components will have small diameters. This result contributes to the understanding of the structure and properties of these complex sets.\n","\n","\n"]}]},{"cell_type":"code","source":["# The default answer here is more detailed and complete\n","#   than what you would get from searching Wikipedia.\n","question = \"Что такое эффект зловещей долины?\"\n","run_tests(question)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G8HOveX8dywS","executionInfo":{"status":"ok","timestamp":1753776208113,"user_tz":-180,"elapsed":72043,"user":{"displayName":"Dzmitry Paulenka","userId":"15727351917421840757"}},"outputId":"78273646-6a05-457c-8417-e07719932af3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Эффект зловещей долины (или \"uncanny valley\" на английском) — это концепция, описывающая реакцию людей на роботов, анимацию или другие искусственные объекты, которые выглядят почти, но не совсем, как реальные люди. Когда внешний вид объекта становится очень близким к человеческому, но все же содержит некоторые неестественные черты, это может вызывать у людей чувство дискомфорта или отторжения.\n","\n","Термин был введен японским робототехником Масахиро Мори в 1970-х годах. Он заметил, что по мере того, как роботы становятся более человечными, положительная реакция людей на них возрастает, но только до определенного момента. Когда робот или анимация становятся почти, но не совсем человеческими, реакция начинает резко ухудшаться, создавая \"долину\" в графике, где уровень симпатии падает.\n","\n","Эффект зловещей долины имеет важные последствия для дизайна роботов, анимации и виртуальной реальности, поскольку создатели стремятся создать более естественные и привлекательные образы, избегая при этом этого дискомфорта.\n","\n","----------\n","\n"," Iteration: 1 \n","\n","Thought: Я должен поискать информацию о \"эффекте зловещей долины\" в Википедии, чтобы объяснить, что это такое.  \n","Action: wikipedia: Эффект зловещей долины  \n","PAUSE\n","\n","\n"," -- running wikipedia Эффект зловещей долины   --\n","Next prompt: Observation: <span class=\"searchmatch\">Эффе́кт</span> «<span class=\"searchmatch\">злове́щей</span> <span class=\"searchmatch\">доли́ны</span>» (яп. 不気味の谷 букими но тани, англ. uncanny valley) — явление, основанное на гипотезе, сформулированной японским учёным-робототехником\n","\n"," Iteration: 2 \n","\n","Answer: Эффект зловещей долины (яп. 不気味の谷, англ. uncanny valley) — это явление, основанное на гипотезе, сформулированной японским учёным-робототехником. Этот эффект описывает реакцию людей на роботов или анимацию, которые выглядят почти, но не совсем, как реальные люди. Когда внешний вид объекта становится всё более похожим на человеческий, но всё же содержит некоторые неестественные черты, это может вызывать у людей чувство дискомфорта или отторжения. Эффект зловещей долины подчеркивает важность достижения высокой степени реалистичности в дизайне роботов и анимации для создания положительного восприятия.\n","\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"OfMWhsFoh4YP"},"execution_count":null,"outputs":[]}]}