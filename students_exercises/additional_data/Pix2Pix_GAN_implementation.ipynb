{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"oU8cYIikhbJI"},"source":["# Pix2Pix GAN implementation\n","\n","[Original video](https://youtu.be/SuddDSqGRzg)\n","\n","[Paper walkthrough video](https://youtu.be/9SGs4Nm0VR4)\n","\n","[Pix2Pix paper](https://arxiv.org/abs/1611.07004)\n","\n","[Datasets](http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/) to train neural network\n","\n","Training of GPU should take around **1.5 hours**."]},{"cell_type":"markdown","source":["## Install `albumentations` image augmentation library\n","\n","**NOTE:** After installation restart the runtime."],"metadata":{"id":"8fIhh54-xigW"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4X9XJXqI9rXC","executionInfo":{"status":"ok","timestamp":1754591006091,"user_tz":-180,"elapsed":31,"user":{"displayName":"Dzmitry Paulenka","userId":"15727351917421840757"}},"outputId":"98f47e23-0798-4454-a90e-f4dae0e8565f"},"source":["%%script echo \"Skip this cell. Use installed albumentations library\"\n","\n","# Google CoLab has old version of albumentations library. Update it.\n","# After installation restart the runtime.\n","!pip install -U git+https://github.com/albu/albumentations --no-cache-dir"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Skip this cell. Use installed albumentations library\n"]}]},{"cell_type":"markdown","metadata":{"id":"fePeBMzypaOQ"},"source":["## Get dataset. Import libraries"]},{"cell_type":"code","source":["# Mount your Google Drive to this Colab\n","from google.colab import drive\n","\n","gdrive = '/content/gdrive'\n","drive.mount(gdrive)\n","\n","# Check connection\n","data_dir = f'{gdrive}/My Drive/Colab Notebooks/2025.07.25_execises/models'\n","\n","# Show files in a data directory\n","!ls -hal \"{data_dir}\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F2vz8eiXr7Xt","executionInfo":{"status":"ok","timestamp":1754587420503,"user_tz":-180,"elapsed":2606,"user":{"displayName":"Dzmitry Paulenka","userId":"15727351917421840757"}},"outputId":"f820a840-48ad-4a00-93da-d290cb2f0972"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","total 1.2G\n","-rw------- 1 root root  16M Jul 28 11:26 3d_image_classification.keras\n","-rw------- 1 root root  32M Jul 30 16:32 dcgan_discriminator_celeb.pth.tar\n","-rw------- 1 root root  32M Jul 30 16:32 dcgan_discriminator_checkpoint.pth.tar\n","-rw------- 1 root root  41M Jul 30 16:32 dcgan_generator_celeb.pth.tar\n","-rw------- 1 root root 145M Jul 30 16:32 dcgan_generator_checkpoint.pth.tar\n","-rw------- 1 root root 239M Sep  2  2018 maps.tar.gz\n","-rw------- 1 root root  32M Aug  7 15:41 pix2pix_discriminator.pth.tar\n","-rw------- 1 root root 623M Aug  7 15:41 pix2pix_generator.pth.tar\n","-rw------- 1 root root 1.2M Jul 29 08:44 simple_gan_discriminator_checkpoint.pth.tar\n","-rw------- 1 root root 2.6M Jul 29 08:44 simple_gan_generator_checkpoint.pth.tar\n"]}]},{"cell_type":"code","metadata":{"id":"FYyHUwd7VnPH","executionInfo":{"status":"ok","timestamp":1754591010382,"user_tz":-180,"elapsed":4286,"user":{"displayName":"Dzmitry Paulenka","userId":"15727351917421840757"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"414540b0-b0be-4a7f-b7a8-a9f9010a5619"},"source":["import os\n","\n","# Get pretrained models\n","CHECKPOINT_DISC = 'pix2pix_discriminator.pth.tar'\n","CHECKPOINT_GEN = 'pix2pix_generator.pth.tar'\n","\n","# Copy files from Google Drive\n","if os.path.exists(f\"{data_dir}/{CHECKPOINT_DISC}\") and \\\n","   os.path.exists(f\"{data_dir}/{CHECKPOINT_GEN}\"):\n","\n","    !cp -rf \"{data_dir}/{CHECKPOINT_DISC}\" \".\"\n","    !cp -rf \"{data_dir}/{CHECKPOINT_GEN}\"  \".\"\n","    !cp -rf \"{data_dir}/maps.tar.gz\"       \".\"\n","\n","    !ls -hal \"{CHECKPOINT_DISC}\"\n","    !ls -hal \"{CHECKPOINT_GEN}\"\n","    !ls -hal \"maps.tar.gz\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-rw------- 1 root root 32M Aug  7 17:23 pix2pix_discriminator.pth.tar\n","-rw------- 1 root root 623M Aug  7 17:23 pix2pix_generator.pth.tar\n","-rw------- 1 root root 239M Aug  7 17:23 maps.tar.gz\n","-rw------- 1 root root 32M Aug  7 18:23 pix2pix_discriminator.pth.tar\n","-rw------- 1 root root 623M Aug  7 18:23 pix2pix_generator.pth.tar\n","-rw------- 1 root root 239M Aug  7 18:23 maps.tar.gz\n"]}]},{"cell_type":"code","metadata":{"id":"lqGyZ-UlJe6w"},"source":["import os\n","import cv2  # OpenCV library\n","import numpy as np\n","import multiprocessing\n","import matplotlib.pyplot as plt\n","import albumentations as A  # image augmentation library\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","from PIL import Image\n","from torchvision.utils import save_image\n","from albumentations.pytorch import ToTensorV2\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm.notebook import tqdm  # library to show the progressbar"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GlygEa4riZbT","executionInfo":{"status":"ok","timestamp":1754640677087,"user_tz":-180,"elapsed":54184,"user":{"displayName":"Dzmitry Paulenka","userId":"15727351917421840757"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a4437df1-1535-4158-fe30-dad9d5e7d1e3"},"source":["import os\n","\n","# Get maps dataset\n","if not os.path.exists('maps.tar.gz'):\n","    !wget 'http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/maps.tar.gz'"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-08-08 08:10:23--  http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/maps.tar.gz\n","Resolving efrosgans.eecs.berkeley.edu (efrosgans.eecs.berkeley.edu)... 128.32.244.190\n","Connecting to efrosgans.eecs.berkeley.edu (efrosgans.eecs.berkeley.edu)|128.32.244.190|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 250242400 (239M) [application/x-gzip]\n","Saving to: ‘maps.tar.gz’\n","\n","maps.tar.gz         100%[===================>] 238.65M  4.55MB/s    in 54s     \n","\n","2025-08-08 08:11:17 (4.42 MB/s) - ‘maps.tar.gz’ saved [250242400/250242400]\n","\n"]}]},{"cell_type":"code","metadata":{"id":"DcQjMrT1jlkt"},"source":["# Extract data\n","import zipfile\n","import tarfile\n","\n","def extract(fname):\n","    \"\"\" Extract files from archive. \"\"\"\n","    if fname.endswith(\".tar.gz\") or fname.endswith('.tgz'):\n","        ref = tarfile.open(fname, mode='r:gz')\n","    elif fname.endswith('.tar'):\n","        ref = tarfile.open(fname, mode='r:')\n","    elif fname.endswith('.tar.bz2') or fname.endswith('.tbz'):\n","        ref = tarfile.open(fname, mode='r:bz2')\n","    elif fname.endswith('.zip'):\n","        ref = zipfile.ZipFile(fname, mode='r')\n","\n","    ref.extractall()\n","    ref.close()\n","\n","extract('maps.tar.gz')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tlJRIkjiiWaN"},"source":["## Discriminator model"]},{"cell_type":"code","metadata":{"id":"w19x0EMepZyK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754591015533,"user_tz":-180,"elapsed":2156,"user":{"displayName":"Dzmitry Paulenka","userId":"15727351917421840757"}},"outputId":"003af321-0837-4f93-c911-eb5de99bccdd"},"source":["class CNNBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=2):\n","        super().__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=stride,\n","                      padding_mode='reflect', bias=False),\n","\n","            # Do not normalize across the batches. Normalize only across the layer (instance).\n","            nn.InstanceNorm2d(out_channels, affine=True),  # LayerNorm <--> InstanceNorm\n","\n","            # nn.InstanceNorm2d has better results. No artifacts\n","            # nn.BatchNorm2d(out_channels),\n","\n","            nn.LeakyReLU(0.2),\n","        )\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n","        # Input 286x286. Output: 30x30\n","        super().__init__()\n","        self.initial = nn.Sequential(\n","            nn.Conv2d(in_channels*2, features[0], kernel_size=4, stride=2,\n","                      padding=1, padding_mode='reflect'),\n","            nn.LeakyReLU(0.2),\n","        )\n","\n","        layers = []\n","        in_channels = features[0]\n","        for feature in features[1:]:\n","            stride = 1 if feature == features[-1] else 2\n","            layers.append(CNNBlock(in_channels, feature, stride=stride))\n","            in_channels = feature\n","\n","        layers.append(nn.Conv2d(in_channels, 1, kernel_size=4, stride=1,\n","                                padding=1, padding_mode='reflect'))\n","\n","        self.model = nn.Sequential(*layers)\n","\n","    def forward(self, x, y):\n","        # x - satellite image, y - transformed real or generated fake image\n","        # x and y are concatenated along the channels\n","        x = torch.cat([x, y], dim=1)  # concatenate along channels\n","        x = self.initial(x)\n","        return self.model(x)\n","\n","\n","def test():  # test function\n","    x = torch.randn((8, 3, 286, 286))  # 286x286 pixels image size\n","    y = torch.randn((8, 3, 286, 286))\n","    model = Discriminator()\n","    predictions = model(x, y)\n","    print(predictions.shape)\n","    assert predictions.shape == (8, 1, 30, 30)\n","\n","    x = torch.randn((8, 3, 256, 256))  # 256x256 pixels image size\n","    y = torch.randn((8, 3, 256, 256))\n","    model = Discriminator()\n","    predictions = model(x, y)\n","    print(predictions.shape)\n","\n","    print('Test - OK')\n","\n","\n","test()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([8, 1, 30, 30])\n","torch.Size([8, 1, 26, 26])\n","Test - OK\n","torch.Size([8, 1, 30, 30])\n","torch.Size([8, 1, 26, 26])\n","Test - OK\n"]}]},{"cell_type":"markdown","metadata":{"id":"JA4kmSlZpki6"},"source":["## Generator model"]},{"cell_type":"code","metadata":{"id":"mMLgvqBQpZsJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754591018675,"user_tz":-180,"elapsed":3139,"user":{"displayName":"Dzmitry Paulenka","userId":"15727351917421840757"}},"outputId":"e4699087-d3ba-413a-e878-6d0a72bb893c"},"source":["class Block(nn.Module):\n","    def __init__(self, in_channels, out_channels, down=True, act='relu', use_dropout=False):\n","        super().__init__()\n","\n","        if down:\n","            layer = nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2,\n","                              padding=1, padding_mode='reflect', bias=False)\n","        else:  # cannot use padding_mode='reflect' on the ConvTranspose2d layer\n","            layer = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4,\n","                                       stride=2, padding=1, bias=False)\n","\n","        self.conv = nn.Sequential(\n","            layer,\n","\n","            # Do not normalize across the batches. Normalize only across the layer (instance).\n","            nn.InstanceNorm2d(out_channels, affine=True),  # LayerNorm <--> InstanceNorm\n","\n","            # nn.InstanceNorm2d has better results. No artifacts\n","            # nn.BatchNorm2d(out_channels),\n","\n","            nn.ReLU() if act == 'relu' else nn.LeakyReLU(0.2),\n","        )\n","        self.use_dropout = use_dropout\n","        self.dropout = nn.Dropout(0.5)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        return self.dropout(x) if self.use_dropout else x\n","\n","\n","class Generator(nn.Module):\n","    def __init__(self, in_channels=3, features=64):\n","        super().__init__()\n","        # Input: 256\n","        self.initial_down = nn.Sequential(\n","            nn.Conv2d(in_channels, features, kernel_size=4, stride=2, padding=1,\n","                      padding_mode='reflect'),\n","            nn.LeakyReLU(0.2),\n","        )  # 128\n","\n","        self.down1 = Block(features,   features*2, down=True, act='leaky', use_dropout=False)  # 64x64\n","        self.down2 = Block(features*2, features*4, down=True, act='leaky', use_dropout=False)  # 32x32\n","        self.down3 = Block(features*4, features*8, down=True, act='leaky', use_dropout=False)  # 16x16\n","        self.down4 = Block(features*8, features*8, down=True, act='leaky', use_dropout=False)  # 8x8\n","        self.down5 = Block(features*8, features*8, down=True, act='leaky', use_dropout=False)  # 4x4\n","        self.down6 = Block(features*8, features*8, down=True, act='leaky', use_dropout=False)  # 2x2\n","        self.bottleneck = nn.Sequential(\n","            nn.Conv2d(features*8, features*8, 4, 1, 1, padding_mode='reflect'),\n","            nn.ReLU(),\n","        )  # 1x1\n","        self.up1 = Block(features*8,   features*8, down=False, act='relu', use_dropout=True)   # 2x2\n","        self.up2 = Block(features*8*2, features*8, down=False, act='relu', use_dropout=True)   # 4x4\n","        self.up3 = Block(features*8*2, features*8, down=False, act='relu', use_dropout=True)   # 8x8\n","        self.up4 = Block(features*8*2, features*8, down=False, act='relu', use_dropout=False)  # 16x16\n","        self.up5 = Block(features*8*2, features*4, down=False, act='relu', use_dropout=False)  # 32x32\n","        self.up6 = Block(features*4*2, features*2, down=False, act='relu', use_dropout=False)  # 64x64\n","        self.up7 = Block(features*2*2, features,   down=False, act='relu', use_dropout=False)  # 128x128\n","        self.final_up = nn.Sequential(\n","            nn.ConvTranspose2d(features*2, in_channels, kernel_size=4, stride=2, padding=1),\n","            nn.Tanh(),  # use hyperbolic tangent\n","        )  # 256x256\n","\n","    def forward(self, x):  # U-Net shape-like structure\n","        d1 = self.initial_down(x)\n","        d2 = self.down1(d1)\n","        d3 = self.down2(d2)\n","        d4 = self.down3(d3)\n","        d5 = self.down4(d4)\n","        d6 = self.down5(d5)\n","        d7 = self.down6(d6)\n","        bottleneck = self.bottleneck(d7)\n","        up1 = self.up1(bottleneck)\n","        up2 = self.up2(torch.cat([up1, d7], dim=1))\n","        up3 = self.up3(torch.cat([up2, d6], dim=1))\n","        up4 = self.up4(torch.cat([up3, d5], dim=1))\n","        up5 = self.up5(torch.cat([up4, d4], dim=1))\n","        up6 = self.up6(torch.cat([up5, d3], dim=1))\n","        up7 = self.up7(torch.cat([up6, d2], dim=1))\n","        return self.final_up(torch.cat([up7, d1], dim=1))\n","\n","\n","def test():\n","    x = torch.randn((8, 3, 256, 256))\n","    model = Generator()\n","    predictions = model(x)\n","    print(predictions.shape)\n","    assert predictions.shape == (8, 3, 256, 256)\n","    print('Test - OK')\n","\n","test()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([8, 3, 256, 256])\n","Test - OK\n","torch.Size([8, 3, 256, 256])\n","Test - OK\n"]}]},{"cell_type":"markdown","metadata":{"id":"ALbH5oYX8Clw"},"source":["## Configuration"]},{"cell_type":"code","metadata":{"id":"9GmNKr4-8F3X"},"source":["DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","USE_CUDA = torch.cuda.is_available()  # check if CUDA is available\n","LEARNING_RATE = 2e-4\n","BATCH_SIZE = 160\n","NUM_WORKERS = multiprocessing.cpu_count()  # get number of CPU cores\n","IMAGE_SIZE = 256\n","CHANNELS_IMG = 3\n","L1_LAMBDA = 100\n","NUM_EPOCHS = 200  # 200 epochs\n","LOAD_MODEL = True  # True\n","SAVE_MODEL = True\n","CHECKPOINT_DISC = 'pix2pix_discriminator.pth.tar'\n","CHECKPOINT_GEN = 'pix2pix_generator.pth.tar'\n","EVALUATION = 'pix2pix_evaluation_images'\n","\n","\n","transform1 = A.Compose([\n","    A.Resize(width=IMAGE_SIZE, height=IMAGE_SIZE),  # resize image\n","])\n","\n","augment_both_images = A.Compose(\n","    [\n","        # Applies one of the eight possible D4 dihedral group transformations to a square-shaped input,\n","        # maintaining the square shape. These transformations correspond to the symmetries of a square,\n","        # including rotations and reflections.\n","        A.D4(p=1.0),\n","    ],\n","    additional_targets={'image0': 'image'})\n","\n","\n","transform2 = A.Compose([\n","    A.ColorJitter(p=0.1),  # NOTE: this color ColorJitter seems important\n","    A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0),\n","    ToTensorV2(),  # convert array to tensor\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WCX5X72Vpneo"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"id":"rziWZpCWpZlw"},"source":["class MapDataset(Dataset):\n","    def __init__(self, root, augment):\n","        super().__init__()\n","        self.root = root\n","        self.augment = augment\n","        self.list_files = os.listdir(self.root)\n","\n","    def __len__(self):\n","        return len(self.list_files)\n","\n","    def __getitem__(self, index):\n","        filepath = os.path.join(self.root, self.list_files[index])\n","        image = np.array(Image.open(filepath))\n","\n","        input_image = image[:, :600, :]\n","        target_image = image[:, 600:, :]\n","\n","        input_image = transform1(image=input_image)['image']\n","        target_image = transform1(image=target_image)['image']\n","\n","        if self.augment:\n","            augmentations = augment_both_images(image=input_image, image0=target_image)\n","            input_image, target_image = augmentations['image'], augmentations['image0']\n","\n","        input_image = transform2(image=input_image)['image']\n","        target_image = transform2(image=target_image)['image']\n","\n","        return input_image, target_image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Helper functions"],"metadata":{"id":"4XY9WNnCnDy-"}},{"cell_type":"code","metadata":{"id":"f1ZbH4_lD-Zb"},"source":["def save_checkpoint(model, optimizer, filename):\n","    print('=> Saving checkpoint')\n","    checkpoint = {\n","        'state_dict': model.state_dict(),\n","        'optimizer': optimizer.state_dict(),\n","    }\n","    torch.save(checkpoint, filename)\n","\n","\n","def load_checkpoint(checkpoint_file, model, optimizer, lr):\n","    print('=> Loading checkpoint')\n","    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n","    model.load_state_dict(checkpoint['state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","\n","    # Replace old learning rate from the saved model\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","\n","\n","def prepare(image):\n","    \"\"\" Prepare image for display. \"\"\"\n","    image = image * 0.5 + 0.5  # convert tensor from [-1,1] to [0,1]\n","    image = image.cpu().numpy()  # copy PyTorch tensor from GPU to NumPy CPU\n","    image = image.squeeze()  # convert (1, 3, 256, 256) to (3, 256, 256)\n","    # Transpose tensor channels from (channel, height, width) to (height, width, channel).\n","    image = np.transpose(image, (1, 2, 0))  # convert from (3, 256, 256) to (256, 256, 3)\n","    return image\n","\n","\n","def add_image(image, title, cmap=\"viridis\"):\n","    \"\"\" Add image to the grid. Default color map is 'viridis'. \"\"\"\n","    plt.imshow(image, cmap=cmap)  # add image to the grid\n","    plt.title(title)  # show image title\n","    plt.axis(\"off\")  # turn off axis numbers\n","\n","\n","def show_one_example(gen, image_with_map):\n","    \"\"\" Show one example on the screen. \"\"\"\n","    x, y = image_with_map  # x - image, y - ground truth map\n","    x, y = x.to(DEVICE), y.to(DEVICE)  # cast tensors to CPU or to GPU device\n","\n","    with torch.no_grad():  # do not calculate gradiences\n","        y_fake = gen(x)  # generate fake map\n","        x, y, y_fake = prepare(x), prepare(y), prepare(y_fake)\n","\n","        fig = plt.figure(figsize=(12, 4))  # create figure with size 12×4 inches\n","\n","        fig.add_subplot(1, 3, 1)  # add a new cell to the grid\n","        add_image(x, \"Original image\")\n","\n","        fig.add_subplot(1, 3, 2)  # add a new cell to the grid\n","        add_image(y, \"Ground truth map\")\n","\n","        fig.add_subplot(1, 3, 3)  # add a new cell to the grid\n","        add_image(y_fake, \"Generated map\")\n","\n","        plt.show()  # show the grid\n","\n","\n","\n","def show_examples(root, augment, num=25):\n","    \"\"\" Show image example on the screen. \"\"\"\n","    gen = Generator().to(DEVICE)\n","    gen.eval()  # switch generator from training to evaluation mode\n","    opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n","    load_checkpoint(CHECKPOINT_GEN, gen, opt_gen, LEARNING_RATE)\n","    dataset = MapDataset(root=root, augment=augment)\n","    loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=NUM_WORKERS)\n","    # Iterate through the DataLoader\n","    for i, image_with_map in enumerate(loader):\n","        if i >= num:\n","            break\n","        show_one_example(gen, image_with_map)\n","\n","\n","def save_example(gen, test_loader, epoch, folder):\n","    \"\"\" Save image example in the folder. \"\"\"\n","    x, y = next(iter(test_loader))\n","    x, y = x.to(DEVICE), y.to(DEVICE)\n","    os.makedirs(folder, exist_ok=True)\n","\n","    gen.eval()\n","    with torch.no_grad():\n","        y_fake = gen(x)\n","        y_fake = y_fake * 0.5 + 0.5  # convert tensor from [-1,1] to [0,1]\n","        save_image(y_fake, folder + f'/y_gen_{epoch}.jpg')\n","        if epoch == 0:\n","            save_image(x*0.5+0.5, folder + f'/_input_.jpg')\n","            save_image(y*0.5+0.5, folder + f'/_label_.jpg')\n","    gen.train()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"juKM0_FUpujZ"},"source":["## Train"]},{"cell_type":"code","metadata":{"id":"NmmoPbLrpZZy"},"source":["def train(disc, gen, loader, opt_disc, opt_gen, l1_loss, bce_loss, d_scaler, g_scaler):\n","    loop = tqdm(loader, leave=False)\n","\n","    for idx, (x, y) in enumerate(loop):\n","        x, y = x.to(DEVICE), y.to(DEVICE)\n","\n","        # Train Discriminator\n","        # Forward pass with autocast enabled for automatic mixed precision\n","        with torch.autocast(DEVICE, dtype=torch.float16):\n","            y_fake = gen(x)\n","            d_real = disc(x, y)\n","            d_fake = disc(x, y_fake.detach())\n","            d_real_loss = bce_loss(d_real, torch.ones_like(d_real))\n","            d_fake_loss = bce_loss(d_fake, torch.zeros_like(d_fake))\n","            d_loss = (d_real_loss + d_fake_loss) / 2\n","\n","        opt_disc.zero_grad()\n","        d_scaler.scale(d_loss).backward()\n","        d_scaler.step(opt_disc)\n","        d_scaler.update()\n","\n","        # Train Generator\n","        # Forward pass with autocast enabled for automatic mixed precision\n","        with torch.autocast(DEVICE, dtype=torch.float16):\n","            d_fake = disc(x, y_fake)\n","            g_fake_loss = bce_loss(d_fake, torch.ones_like(d_fake))\n","            l1 = l1_loss(y_fake, y) * L1_LAMBDA\n","            g_loss = g_fake_loss + l1\n","\n","        opt_gen.zero_grad()\n","        d_scaler.scale(g_loss).backward()\n","        d_scaler.step(opt_gen)\n","        d_scaler.update()\n","\n","\n","def main():\n","    disc = Discriminator().to(DEVICE)\n","    gen = Generator().to(DEVICE)\n","    opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n","    opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n","    bce_loss = nn.BCEWithLogitsLoss()\n","    l1_loss = nn.L1Loss()\n","\n","    if LOAD_MODEL and os.path.exists(CHECKPOINT_DISC) and os.path.exists(CHECKPOINT_GEN):\n","        load_checkpoint(CHECKPOINT_DISC, disc, opt_disc, LEARNING_RATE)\n","        load_checkpoint(CHECKPOINT_GEN, gen, opt_gen, LEARNING_RATE)\n","\n","    train_dataset = MapDataset(root='./maps/train', augment=True)\n","    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n","    test_dataset = MapDataset(root='./maps/val', augment=False)\n","    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=NUM_WORKERS)\n","\n","    # Initialize GradScaler for gradient scaling (essential for AMP training)\n","    d_scaler = torch.amp.GradScaler(enabled=USE_CUDA)\n","    g_scaler = torch.amp.GradScaler(enabled=USE_CUDA)\n","\n","    disc.train()\n","    gen.train()\n","\n","    for epoch in range(NUM_EPOCHS+1):\n","        train(disc, gen, train_loader, opt_disc, opt_gen, l1_loss, bce_loss, d_scaler, g_scaler)\n","\n","        if SAVE_MODEL and epoch % 10 == 0:  # save model every 10 epochs\n","            print(f'epoch: {epoch}')\n","            save_checkpoint(disc, opt_disc, CHECKPOINT_DISC)\n","            save_checkpoint(gen, opt_gen, CHECKPOINT_GEN)\n","\n","        if epoch % 5 == 0:  # show example every 5 epochs\n","            show_one_example(gen, next(iter(test_loader)))\n","\n","        save_example(gen, test_loader, epoch, folder=EVALUATION)\n","\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Show results\n","\n","### Show validation images without augmentations"],"metadata":{"id":"ZxX2m3VyXzF8"}},{"cell_type":"code","source":["show_examples('./maps/val', augment=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1LyGCF0hr-s7WBeOglYtsGYfG61JWtYWU"},"id":"aboEtd-dX19b","executionInfo":{"status":"aborted","timestamp":1754591111438,"user_tz":-180,"elapsed":59,"user":{"displayName":"Dzmitry Paulenka","userId":"15727351917421840757"}},"outputId":"5e76b97d-f039-4ee3-ee56-41a6c4366f16"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["### Show train images with augmentations"],"metadata":{"id":"R3CXb6f2Kggr"}},{"cell_type":"code","source":["show_examples('./maps/train', augment=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1VWblXG8_l50YP1XMb_8OlWXJzH-hEOq-"},"id":"PdMVJQC4Kkob","executionInfo":{"status":"aborted","timestamp":1754591111440,"user_tz":-180,"elapsed":54,"user":{"displayName":"Dzmitry Paulenka","userId":"15727351917421840757"}},"outputId":"e50bd6f3-dd45-41a5-b4d3-45f47f993d23"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["## Save results to Google Drive"],"metadata":{"id":"WKP0aANbrvxM"}},{"cell_type":"code","metadata":{"id":"fgVuhLzBhPzc","executionInfo":{"status":"aborted","timestamp":1754591111441,"user_tz":-180,"elapsed":53,"user":{"displayName":"Dzmitry Paulenka","userId":"15727351917421840757"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"904b4ee9-f5f8-4ef4-80fc-a91cc9ad58de"},"source":["pictures_dir = os.path.dirname(data_dir) + '/pictures'\n","print(pictures_dir)\n","\n","!zip -qr \"{EVALUATION}.zip\"  \"{EVALUATION}/\"\n","\n","!cp -rf \"{EVALUATION}.zip\"   \"{pictures_dir}\"\n","!cp -rf \"{CHECKPOINT_DISC}\"  \"{data_dir}\"\n","!cp -rf \"{CHECKPOINT_GEN}\"   \"{data_dir}\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/Colab Notebooks/2025.07.25_execises/pictures\n"]}]},{"cell_type":"code","metadata":{"id":"97MMUXHSntux"},"source":[],"execution_count":null,"outputs":[]}]}