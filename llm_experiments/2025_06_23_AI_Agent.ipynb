{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How to Build an AI Agent from Scratch in Raw Python\n",
        "\n",
        "Self-link: https://colab.research.google.com/drive/1g31aYu3ylEurdQjAJtnSxGFcx9aSZa_D\n",
        "\n",
        "Tutorial by Shrinivasan Sankar: https://www.ai-bites.net/lets-build-an-ai-agent-from-scratch-in-raw-python/?ref=ai-bites-newsletter\n",
        "\n",
        "GitHub scource code: https://github.com/ai-bites/langgraph-crash-course/blob/main/0_simple_python_agent.ipynb?ref=ai-bites.net\n",
        "\n",
        "Video: https://youtu.be/_TzW6F1NVsc"
      ],
      "metadata": {
        "id": "iIaNC_RO_NYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install necessary libraries"
      ],
      "metadata": {
        "id": "o-uFbIr_R3uG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai httpx dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IW3-zNndgBY2",
        "outputId": "db4485e9-564b-4fb3-a3f1-d117a4820e80"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.86.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (0.28.1)\n",
            "Collecting dotenv\n",
            "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx) (0.16.0)\n",
            "Collecting python-dotenv (from dotenv)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
            "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv, dotenv\n",
            "Successfully installed dotenv-0.9.9 python-dotenv-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import necessary packages"
      ],
      "metadata": {
        "id": "d-mEIl4bZSBG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OwUnMhTc-WEt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c87575c5-eaa7-4167-b45e-8a46bb1c5678"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
            "Python version: 3.11.13\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "import re\n",
        "import httpx\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "import json\n",
        "\n",
        "# Print the full version information string.\n",
        "import sys\n",
        "print(sys.version)\n",
        "\n",
        "# Show the Python version.\n",
        "import platform\n",
        "print(f\"Python version: {platform.python_version()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "\n",
        "MODEL = \"gpt-4o-mini\"  # Chat-GPT model name\n",
        "\n",
        "# Set your OPENAI_API_KEY in a \"my_openai_key.env\" file and load it.\n",
        "load_dotenv(\"my_openai_key_2.env\")  # OPENAI_API_KEY\n",
        "# !echo $OPENAI_API_KEY\n",
        "\n",
        "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'],)\n",
        "\n",
        "# Check to see if the LLM works.\n",
        "chat_completion = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Hello there!\"}],\n",
        ")\n",
        "\n",
        "# The anser should be: \"Hello! How can I assist you today?\"\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "wwhnWgzKCXxc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba2f4567-0c74-4178-d587-08f0e1569b8c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatbot\n",
        "Create a chatbot that takes raw messages, stacks them together, and uses the LLM to respond to the messages."
      ],
      "metadata": {
        "id": "y37HgspAwETP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatBot:\n",
        "    def __init__(self, system=\"\"):\n",
        "        self.system = system\n",
        "        self.messages = []\n",
        "        if self.system:\n",
        "            self.messages.append({\"role\": \"system\", \"content\": self.system})\n",
        "\n",
        "\n",
        "    def __call__(self, message):\n",
        "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
        "        result = self.run_llm()\n",
        "        self.messages.append({\"role\": \"assistant\", \"content\": result})\n",
        "        return result\n",
        "\n",
        "\n",
        "    def run_llm(self):\n",
        "        completion = client.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            temperature=0,\n",
        "            messages=self.messages,\n",
        "        )\n",
        "        return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "30bvGkKpcIgJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent\n",
        "Wrap the above chatbot in Agent class to create ReAct Agent."
      ],
      "metadata": {
        "id": "sOWyHh07w58Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# abot = Agent(prompt)\n",
        "action_re = re.compile('^Action: (\\w+): (.*)$')\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, system_prompt=\"\", max_turns=1, known_actions=None):\n",
        "        self.max_turns = max_turns\n",
        "        self.bot = ChatBot(system_prompt)\n",
        "        self.known_actions = known_actions\n",
        "\n",
        "\n",
        "    def run(self, question):\n",
        "        i = 0  # iteration\n",
        "        next_prompt = question\n",
        "\n",
        "        while i < self.max_turns:  # loop through iterations\n",
        "            i += 1\n",
        "            result = self.bot(next_prompt)\n",
        "            print(f\"\\n Iteration: {i} \\n\\n\"\n",
        "                  f\"{result}\\n\\n\")\n",
        "            actions = [action_re.match(a) for a in result.split('\\n') if action_re.match(a)]\n",
        "\n",
        "            if actions:  # there is an action to run\n",
        "                action, action_input = actions[0].groups()\n",
        "                if action not in self.known_actions:\n",
        "                    raise Exception(f\"Unknown action: {action}: {action_input}\")\n",
        "                print(f\" -- running {action} {action_input} --\")\n",
        "\n",
        "                # Make a request to Wikipedia, ArXiv or calculator using action functions.\n",
        "                # print(self.known_actions)\n",
        "                observation = self.known_actions[action](action_input)\n",
        "\n",
        "                next_prompt = f\"Observation: {observation}\"\n",
        "                print(\"Next prompt:\", next_prompt)\n",
        "            else:\n",
        "                return"
      ],
      "metadata": {
        "id": "RrUecbQWrNFY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actions\n",
        "Define actions as functions.\n",
        "\n",
        "The arxiv_search function will invoke arxiv API to fetch data for the agent.\n",
        "\n",
        "The Wikipedia function will use httpx to query Wikipedia and fetch knowledge about any topic unknown to the agent.\n",
        "\n",
        "The calculate function is a Python hack to evaluate any equations\n",
        "given to it."
      ],
      "metadata": {
        "id": "WDJ3TMvJyT4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wikipedia(q):\n",
        "    try:\n",
        "        return httpx.get(\"https://en.wikipedia.org/w/api.php\", params={\n",
        "            \"action\": \"query\",\n",
        "            \"list\": \"search\",\n",
        "            \"srsearch\": q,\n",
        "            \"format\": \"json\"\n",
        "        }).json()[\"query\"][\"search\"][0][\"snippet\"]\n",
        "\n",
        "    except IndexError:  # try Russian instead\n",
        "        return httpx.get(\"https://ru.wikipedia.org/w/api.php\", params={\n",
        "            \"action\": \"query\",\n",
        "            \"list\": \"search\",\n",
        "            \"srsearch\": q,\n",
        "            \"format\": \"json\"\n",
        "        }).json()[\"query\"][\"search\"][0][\"snippet\"]\n",
        "\n",
        "\n",
        "def arxiv_search(q):\n",
        "    ARXIV_NAMESPACE = '{http://www.w3.org/2005/Atom}'\n",
        "\n",
        "    url = f'http://export.arxiv.org/api/query?search_query=all:{q}&start=0&max_results=1'\n",
        "    res = requests.get(url)\n",
        "    et_root = ET.fromstring(res.content)\n",
        "\n",
        "    for entry in et_root.findall(f\"{ARXIV_NAMESPACE}entry\"):\n",
        "        title = entry.find(f\"{ARXIV_NAMESPACE}title\").text.strip()\n",
        "        summary = entry.find(f\"{ARXIV_NAMESPACE}summary\").text.strip()\n",
        "    return json.dumps({\"title\" : title, \"summary\" : summary})\n",
        "\n",
        "\n",
        "def calculate(what):\n",
        "    return eval(what)"
      ],
      "metadata": {
        "id": "FNKUHXkarYDr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt\n",
        "Prompting is quite crucial for ReAct agents. They generally work by few-shot prompting. Meaning, we need to provide a few examples of what is expected of the agent in the prompt itself. For this reason, we need to write a detailed prompt with a few examples. Below is the prompt for the task.\n",
        "\n",
        "Nothing fancy here other than pro-level prompting. We have listed all the actions, explicitly mentioned, “run in a loop of Thought, Action, PAUSE, Observation” so that the agent can invoke actions and think of the perceived observations."
      ],
      "metadata": {
        "id": "J-H5M5eAzBxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "You run in a loop of Thought, Action, PAUSE, Observation.\n",
        "At the end of the loop you output an Answer\n",
        "Use Thought to describe your thoughts about the question you have been asked.\n",
        "Use Action to run one of the actions available to you - then return PAUSE.\n",
        "Observation will be the result of running those actions.\n",
        "\n",
        "Your available actions are:\n",
        "\n",
        "calculate:\n",
        "e.g. calculate: 4 * 7 / 3\n",
        "Runs a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\n",
        "\n",
        "wikipedia:\n",
        "e.g. wikipedia:\n",
        "Returns a summary from searching Wikipedia\n",
        "\n",
        "arxiv_search:\n",
        "e.g. arxiv_search:\n",
        "Returns a summary of research papers\n",
        "\n",
        "Example session:\n",
        "\n",
        "Question: What is the capital of France?\n",
        "Thought: I should look up France on Wikipedia\n",
        "Action: wikipedia: France\n",
        "PAUSE\n",
        "\n",
        "You will be called again with this:\n",
        "\n",
        "Observation: France is a country. The capital is Paris.\n",
        "\n",
        "You then output:\n",
        "\n",
        "Answer: The capital of France is Paris\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "lb0hIIFLy4nU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the Agent\n",
        "It's time to test out our ReAct agent. Let's collate all the actions in a dictionary, create an instance of the agent, and invoke its run method to run the agent."
      ],
      "metadata": {
        "id": "rLatnn0CzouZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "known_actions = {\n",
        "    \"wikipedia\": wikipedia,\n",
        "    \"calculate\": calculate,\n",
        "    \"arxiv_search\": arxiv_search\n",
        "}\n",
        "\n",
        "agent = Agent(prompt, max_turns=3, known_actions=known_actions)\n",
        "\n",
        "\n",
        "def run_tests(question):\n",
        "    bot = ChatBot(question)  # run chatbot without agent\n",
        "    print(bot.run_llm())\n",
        "\n",
        "    print(f\"\\n----------\")\n",
        "    agent.run(question)  # run agent"
      ],
      "metadata": {
        "id": "yBxQn310zdh2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1st Question -- Easy"
      ],
      "metadata": {
        "id": "9Dff2bZEDl4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the capital of Indonesia?\"\n",
        "run_tests(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBPIib_GCJD4",
        "outputId": "29009937-8349-41b7-f81c-783f791ba831"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of Indonesia is Jakarta.\n",
            "\n",
            "----------\n",
            "\n",
            " Iteration: 1 \n",
            "\n",
            "Thought: I already looked up Indonesia on Wikipedia, but I need to confirm the capital again.  \n",
            "Action: wikipedia: Indonesia  \n",
            "PAUSE\n",
            "\n",
            "\n",
            " -- running wikipedia Indonesia   --\n",
            "Next prompt: Observation: <span class=\"searchmatch\">Indonesia</span>, officially the Republic of <span class=\"searchmatch\">Indonesia</span>, is a country in Southeast Asia and Oceania, between the Indian and Pacific oceans. Comprising over 17\n",
            "\n",
            " Iteration: 2 \n",
            "\n",
            "Answer: The capital of Indonesia is Jakarta.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2nd Question -- Easy"
      ],
      "metadata": {
        "id": "lOrGQ_pBDsdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what is the sum of 7 and 5?\"\n",
        "run_tests(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iEEHXIoDD4s",
        "outputId": "c71788f2-b0ca-48b2-faef-cdffda1ecff4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sum of 7 and 5 is 12.\n",
            "\n",
            "----------\n",
            "\n",
            " Iteration: 1 \n",
            "\n",
            "Thought: I can calculate the sum of 7 and 5 directly.  \n",
            "Action: calculate: 7 + 5  \n",
            "PAUSE\n",
            "\n",
            "\n",
            " -- running calculate 7 + 5   --\n",
            "Next prompt: Observation: 12\n",
            "\n",
            " Iteration: 2 \n",
            "\n",
            "Answer: The sum of 7 and 5 is 12.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3rd Question -- Difficult"
      ],
      "metadata": {
        "id": "fYY-dg4QD34Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"explain the contrastive learning paper\"\n",
        "run_tests(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWYR53unDPvz",
        "outputId": "ff0f76fc-3d01-40f2-fc81-6036b2228656"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contrastive learning is a self-supervised learning approach that aims to learn representations by contrasting positive and negative pairs of data points. The key idea is to bring similar data points (positive pairs) closer together in the representation space while pushing dissimilar points (negative pairs) apart. This method has gained popularity in various domains, including computer vision, natural language processing, and audio processing.\n",
            "\n",
            "While I don't have access to specific papers published after October 2021, I can provide a general overview of the typical structure and findings of contrastive learning papers based on the trends and methodologies that were prevalent up to that time.\n",
            "\n",
            "### Key Components of a Contrastive Learning Paper\n",
            "\n",
            "1. **Introduction**: \n",
            "   - The paper usually starts with an introduction to the problem being addressed and the motivation for using contrastive learning. It often highlights the limitations of supervised learning and the advantages of self-supervised methods.\n",
            "\n",
            "2. **Methodology**:\n",
            "   - **Data Augmentation**: The paper describes how data is augmented to create positive pairs. For example, in image datasets, this could involve applying transformations like cropping, flipping, or color jittering.\n",
            "   - **Contrastive Loss Function**: The core of the methodology is the loss function used to train the model. Common choices include the InfoNCE loss or triplet loss, which explicitly encourages the model to minimize the distance between positive pairs and maximize the distance between negative pairs.\n",
            "   - **Architecture**: The paper may detail the neural network architecture used for feature extraction, such as convolutional neural networks (CNNs) for images or transformers for text.\n",
            "\n",
            "3. **Experiments**:\n",
            "   - The authors typically conduct experiments to evaluate the effectiveness of their approach. This may include:\n",
            "     - Comparing the learned representations on downstream tasks (e.g., classification, retrieval).\n",
            "     - Evaluating the performance against baseline methods (e.g., supervised learning, other self-supervised methods).\n",
            "     - Analyzing the impact of different hyperparameters, such as the number of negative samples or the choice of augmentations.\n",
            "\n",
            "4. **Results**:\n",
            "   - The results section presents quantitative and qualitative findings, often using metrics like accuracy, precision, recall, or F1 score. Visualizations, such as t-SNE plots, may be included to illustrate the learned representations.\n",
            "\n",
            "5. **Discussion**:\n",
            "   - The authors discuss the implications of their findings, potential limitations of their approach, and directions for future work. They may also explore the theoretical underpinnings of contrastive learning and its relationship to other learning paradigms.\n",
            "\n",
            "6. **Conclusion**:\n",
            "   - The paper concludes by summarizing the contributions and the significance of the work in advancing the field of self-supervised learning.\n",
            "\n",
            "### Recent Trends in Contrastive Learning\n",
            "\n",
            "- **Multi-Modal Contrastive Learning**: Recent papers have explored contrastive learning across different modalities (e.g., images and text) to improve representation learning.\n",
            "- **Negative Sampling Strategies**: Innovations in how negative samples are selected have been a focus, with methods that dynamically adjust the selection based on the learned representations.\n",
            "- **Applications**: Contrastive learning has been applied to various tasks, including image classification, object detection, and even in reinforcement learning settings.\n",
            "\n",
            "If you have a specific contrastive learning paper in mind, please provide its title or key details, and I can help summarize or explain its contributions based on the general principles of contrastive learning.\n",
            "\n",
            "----------\n",
            "\n",
            " Iteration: 1 \n",
            "\n",
            "Thought: I need to search for the \"contrastive learning\" paper to find a summary or relevant information about it. Contrastive learning is a popular topic in machine learning, particularly in representation learning. I will use the arxiv_search action to find details.\n",
            "\n",
            "Action: arxiv_search: contrastive learning\n",
            "PAUSE\n",
            "\n",
            "\n",
            " -- running arxiv_search contrastive learning --\n",
            "Next prompt: Observation: {\"title\": \"Heterogeneous Contrastive Learning for Foundation Models and Beyond\", \"summary\": \"In the era of big data and Artificial Intelligence, an emerging paradigm is\\nto utilize contrastive self-supervised learning to model large-scale\\nheterogeneous data. Many existing foundation models benefit from the\\ngeneralization capability of contrastive self-supervised learning by learning\\ncompact and high-quality representations without relying on any label\\ninformation. Amidst the explosive advancements in foundation models across\\nmultiple domains, including natural language processing and computer vision, a\\nthorough survey on heterogeneous contrastive learning for the foundation model\\nis urgently needed. In response, this survey critically evaluates the current\\nlandscape of heterogeneous contrastive learning for foundation models,\\nhighlighting the open challenges and future trends of contrastive learning. In\\nparticular, we first present how the recent advanced contrastive learning-based\\nmethods deal with view heterogeneity and how contrastive learning is applied to\\ntrain and fine-tune the multi-view foundation models. Then, we move to\\ncontrastive learning methods for task heterogeneity, including pretraining\\ntasks and downstream tasks, and show how different tasks are combined with\\ncontrastive learning loss for different purposes. Finally, we conclude this\\nsurvey by discussing the open challenges and shedding light on the future\\ndirections of contrastive learning.\"}\n",
            "\n",
            " Iteration: 2 \n",
            "\n",
            "Answer: The paper titled \"Heterogeneous Contrastive Learning for Foundation Models and Beyond\" discusses the use of contrastive self-supervised learning to model large-scale heterogeneous data in the context of big data and artificial intelligence. It highlights how existing foundation models leverage the generalization capabilities of contrastive learning to learn compact and high-quality representations without relying on labeled data. The survey evaluates the current landscape of heterogeneous contrastive learning, addressing challenges and future trends. It covers how advanced contrastive learning methods handle view heterogeneity and their application in training multi-view foundation models, as well as methods for task heterogeneity, including pretraining and downstream tasks. The paper concludes by discussing open challenges and future directions in contrastive learning.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"explain the lightrag paper\"\n",
        "run_tests(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3we7PCc5iqA",
        "outputId": "19982708-b2d0-4739-e7f4-2e0fdb8ee833"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The LightRAG paper presents a novel approach to enhancing the performance of retrieval-augmented generation (RAG) models, which combine retrieval mechanisms with generative models for tasks like question answering and information retrieval. The key innovation in LightRAG is the introduction of a lightweight and efficient method for integrating retrieved documents into the generative process, aiming to improve both the quality of generated responses and the efficiency of the model.\n",
            "\n",
            "### Key Components of LightRAG:\n",
            "\n",
            "1. **Retrieval-Augmented Generation (RAG)**: RAG models typically retrieve relevant documents from a large corpus and use them to inform the generation of responses. This approach leverages external knowledge, which can significantly enhance the model's performance on tasks requiring factual accuracy.\n",
            "\n",
            "2. **Lightweight Integration**: LightRAG focuses on reducing the computational overhead associated with traditional RAG models. It introduces techniques that allow for more efficient processing of retrieved documents, enabling the model to handle larger corpora without a proportional increase in resource consumption.\n",
            "\n",
            "3. **Improved Response Quality**: By optimizing how retrieved information is integrated into the generative process, LightRAG aims to produce more coherent and contextually relevant responses. This is particularly important in applications like conversational agents, where the quality of interaction is crucial.\n",
            "\n",
            "4. **Evaluation and Results**: The paper typically includes empirical evaluations demonstrating the effectiveness of LightRAG compared to baseline models. These evaluations often highlight improvements in metrics such as accuracy, coherence, and user satisfaction.\n",
            "\n",
            "5. **Applications**: LightRAG can be applied to various natural language processing tasks, including but not limited to question answering, summarization, and dialogue systems. Its efficiency makes it suitable for real-time applications where response time is critical.\n",
            "\n",
            "### Conclusion:\n",
            "\n",
            "LightRAG represents a significant step forward in the field of retrieval-augmented generation by addressing the challenges of efficiency and response quality. Its lightweight approach allows for broader applicability in real-world scenarios, making it a valuable contribution to the ongoing development of advanced natural language processing systems.\n",
            "\n",
            "----------\n",
            "\n",
            " Iteration: 1 \n",
            "\n",
            "Thought: I need to search for the \"LightRAG\" paper to find a summary or relevant information about it. It seems to be a specific research paper related to Retrieval-Augmented Generation (RAG) systems. I will use the arxiv_search action to find details.\n",
            "\n",
            "Action: arxiv_search: LightRAG\n",
            "PAUSE\n",
            "\n",
            "\n",
            " -- running arxiv_search LightRAG --\n",
            "Next prompt: Observation: {\"title\": \"LightRAG: Simple and Fast Retrieval-Augmented Generation\", \"summary\": \"Retrieval-Augmented Generation (RAG) systems enhance large language models\\n(LLMs) by integrating external knowledge sources, enabling more accurate and\\ncontextually relevant responses tailored to user needs. However, existing RAG\\nsystems have significant limitations, including reliance on flat data\\nrepresentations and inadequate contextual awareness, which can lead to\\nfragmented answers that fail to capture complex inter-dependencies. To address\\nthese challenges, we propose LightRAG, which incorporates graph structures into\\ntext indexing and retrieval processes. This innovative framework employs a\\ndual-level retrieval system that enhances comprehensive information retrieval\\nfrom both low-level and high-level knowledge discovery. Additionally, the\\nintegration of graph structures with vector representations facilitates\\nefficient retrieval of related entities and their relationships, significantly\\nimproving response times while maintaining contextual relevance. This\\ncapability is further enhanced by an incremental update algorithm that ensures\\nthe timely integration of new data, allowing the system to remain effective and\\nresponsive in rapidly changing data environments. Extensive experimental\\nvalidation demonstrates considerable improvements in retrieval accuracy and\\nefficiency compared to existing approaches. We have made our LightRAG\\nopen-source and available at the link: https://github.com/HKUDS/LightRAG\"}\n",
            "\n",
            " Iteration: 2 \n",
            "\n",
            "Answer: The \"LightRAG\" paper presents a framework for Retrieval-Augmented Generation (RAG) systems that enhances large language models by integrating external knowledge sources. It addresses significant limitations of existing RAG systems, such as reliance on flat data representations and inadequate contextual awareness, which can lead to fragmented answers. LightRAG incorporates graph structures into text indexing and retrieval processes, employing a dual-level retrieval system for comprehensive information retrieval from both low-level and high-level knowledge discovery. This approach improves response times and contextual relevance, supported by an incremental update algorithm for timely data integration. Experimental results show significant improvements in retrieval accuracy and efficiency compared to existing methods. The framework is open-source and available at [LightRAG GitHub](https://github.com/HKUDS/LightRAG).\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4th Qestion -- Difficult"
      ],
      "metadata": {
        "id": "vYyyYxBUElZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is AI Bites?\"\n",
        "run_tests(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4EI_EeeE3Az",
        "outputId": "538eee64-ae97-4d36-dfd5-a098e1d20e67"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI Bites is a platform or initiative that typically focuses on providing concise, digestible information about artificial intelligence (AI) topics. It may include articles, videos, podcasts, or other formats that break down complex AI concepts into easily understandable segments. The goal is to educate and inform a broad audience about AI developments, applications, and implications in various fields. If you have a specific context or platform in mind, please provide more details!\n",
            "\n",
            "----------\n",
            "\n",
            " Iteration: 1 \n",
            "\n",
            "Thought: I need to look up \"AI Bites\" to find out what it refers to. It could be a project, a publication, or something else related to artificial intelligence. I will use the Wikipedia action to gather information.\n",
            "\n",
            "Action: wikipedia: AI Bites\n",
            "PAUSE\n",
            "\n",
            "\n",
            " -- running wikipedia AI Bites --\n",
            "Next prompt: Observation: &quot;Retrieval Augmented Generation(RAG) — A quick and comprehensive introduction&quot;. <span class=\"searchmatch\">ai</span>-<span class=\"searchmatch\">bites</span>.net. Kiela Douwe, Ho Alan (Oct 13, 2023). &quot;Where did Retrieval Augmented\n",
            "\n",
            " Iteration: 2 \n",
            "\n",
            "Answer: \"AI Bites\" appears to be a platform or publication that provides quick and comprehensive introductions to topics in artificial intelligence, such as Retrieval Augmented Generation (RAG). It features articles and resources aimed at making complex AI concepts more accessible.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5th Question -- Difficult"
      ],
      "metadata": {
        "id": "K9708y5GZ0Tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Default answer is wrong. The answer after Wikipedia is write.\n",
        "question = \"Who is Alexander Tuzikov?\"\n",
        "run_tests(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7L0v_Vt1Tn2",
        "outputId": "81ec18b8-d8eb-4c4d-8ddc-44fcd45e2221"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As of my last knowledge update in October 2023, there is no widely recognized public figure or notable individual named Alexander Tuzikov. It's possible that he may be a private individual or a person who has gained prominence after that date. If you have specific context or details about him, I may be able to provide more relevant information.\n",
            "\n",
            "----------\n",
            "\n",
            " Iteration: 1 \n",
            "\n",
            "Thought: I need to search for information about Alexander Tuzikov to understand who he is and what he is known for. I will use the Wikipedia action to find relevant details.\n",
            "\n",
            "Action: wikipedia: Alexander Tuzikov\n",
            "PAUSE\n",
            "\n",
            "\n",
            " -- running wikipedia Alexander Tuzikov --\n",
            "Next prompt: Observation: Ryabova (Deputy Minister of Communications and Informatization), <span class=\"searchmatch\">Alexander</span> <span class=\"searchmatch\">Tuzikov</span> [ru] (General Director of the United Institute of Informatics Problems\n",
            "\n",
            " Iteration: 2 \n",
            "\n",
            "Answer: Alexander Tuzikov is the General Director of the United Institute of Informatics Problems. He is mentioned in the context of communications and informatization, indicating his involvement in the field of information technology and possibly government-related initiatives.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Здесь ответ по умолчанию неверный, но после уточнения в Википедии -- верный.\n",
        "question = \"Кто такой Александр Васильевич Тузиков?\"\n",
        "run_tests(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_37Ldd2efIxy",
        "outputId": "bc4616d9-4401-43af-97f2-472380fe3bdc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Александр Васильевич Тузиков — это российский ученый, известный в области физики и материаловедения. Однако, на момент моего последнего обновления информации в октябре 2023 года, нет широко доступной информации о его достижениях или вкладе в науку. Возможно, он является менее известной фигурой или работает в узкой области, не получившей широкой огласки. Если у вас есть конкретные вопросы о его деятельности или исследованиях, пожалуйста, уточните, и я постараюсь помочь!\n",
            "\n",
            "----------\n",
            "\n",
            " Iteration: 1 \n",
            "\n",
            "Thought: Мне нужно найти информацию о Александре Васильевиче Тузикове, чтобы понять, кто он и чем известен. Я воспользуюсь действием Wikipedia, чтобы найти соответствующие детали.\n",
            "\n",
            "Action: wikipedia: Александр Васильевич Тузиков\n",
            "PAUSE\n",
            "\n",
            "\n",
            " -- running wikipedia Александр Васильевич Тузиков --\n",
            "Next prompt: Observation: <span class=\"searchmatch\">Александр</span> <span class=\"searchmatch\">Васильевич</span> <span class=\"searchmatch\">Тузиков</span> (род. 5 сентября 1958 года, Полоцк) — белорусский учёный, c 2008 по 2022 год -генеральный директор Объединенного института\n",
            "\n",
            " Iteration: 2 \n",
            "\n",
            "Answer: Александр Васильевич Тузиков (род. 5 сентября 1958 года, Полоцк) — белорусский учёный, который с 2008 по 2022 год занимал должность генерального директора Объединенного института информатики.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrong answer\n",
        "question = \"What are the scientific works of Alexandra Kosareva?\"\n",
        "run_tests(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOzPc2-0aUFc",
        "outputId": "0261a2c5-59d3-4e89-be84-f2ec30e2e40c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As of my last update in October 2023, I do not have specific information on the scientific works of Alexandra Kosareva. To find her publications or contributions to scientific research, I recommend checking academic databases such as Google Scholar, PubMed, or institutional repositories. You can also look for her profile on research networking sites like ResearchGate or LinkedIn, where researchers often list their publications and projects. If you have a specific field or topic in mind, I may be able to help you find more general information related to that area.\n",
            "\n",
            "----------\n",
            "\n",
            " Iteration: 1 \n",
            "\n",
            "Thought: I need to search for information about the scientific works of Alexandra Kosareva to understand her contributions to her field. I will use the arxiv_search action to find relevant research papers or publications.\n",
            "\n",
            "Action: arxiv_search: Alexandra Kosareva\n",
            "PAUSE\n",
            "\n",
            "\n",
            " -- running arxiv_search Alexandra Kosareva --\n",
            "Next prompt: Observation: {\"title\": \"All-optical attoclock for imaging tunnelling wavepackets\", \"summary\": \"Recent experiments on measuring time-delays during tunnelling of cold atoms\\nthrough an optically created potential barrier are reinvigorating the\\ncontroversial debate regarding possible time-delays during light-induced\\ntunnelling of an electron from an atom. Compelling theoretical and experimental\\narguments have been put forward to advocate opposite views, confirming or\\nrefuting the existence of finite tunnelling time delays. Yet, such a delay,\\nwhether present or not, is but a single quantity characterizing the tunnelling\\nwavepacket; the underlying dynamics are richer. Here we propose to augment\\nphoto-electron detection in laser-induced tunnelling with detection of light\\nemitted by the tunnelling electron -- the so-called Brunel radiation. Using a\\ncombination of single-color and two-color driving fields, we identify the\\nall-optical signatures of the re-shaping of the tunnelling wavepacket as it\\nemerges from the tunnelling barrier and moves away from the core. This\\nreshaping includes not only an effective time-delay but also time-reversal\\nasymmetry of the ionization process, which we describe theoretically and\\nobserve experimentally. We show how both delay and reshaping are mapped on the\\npolarization properties of the Brunel radiation, with different harmonics\\nbehaving as different hands of a clock moving at different speeds. The\\nall-optical detection paves the way to time-resolving optical tunnelling in\\ncondensed matter systems, e.g. tunnelling across bandgaps in solids, on the\\nattosecond time-scale.\"}\n",
            "\n",
            " Iteration: 2 \n",
            "\n",
            "Answer: Alexandra Kosareva has contributed to scientific research, including a paper titled \"All-optical attoclock for imaging tunnelling wavepackets.\" This work discusses measuring time delays during the tunneling of cold atoms through an optically created potential barrier and explores the dynamics of tunneling wavepackets. The study proposes augmenting photo-electron detection with the detection of light emitted by tunneling electrons, identifying all-optical signatures of reshaping of the tunneling wavepacket. The research aims to enhance time-resolving optical tunneling in condensed matter systems on the attosecond time scale.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrong answer\n",
        "question = \"What are the scientific works of Dzmitry Paulenka?\"\n",
        "run_tests(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLdKLjnAczeh",
        "outputId": "3467e70b-30f1-44e6-dfda-3cec212ea2e6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As of my last update in October 2023, I do not have specific information on the scientific works of Dzmitry Paulenka. To find his publications or contributions to scientific research, I recommend checking academic databases such as Google Scholar, ResearchGate, or institutional repositories. You can also look for his work in relevant journals or conference proceedings in his field of study. If you have access to a university library, they may also provide resources to help you find his research.\n",
            "\n",
            "----------\n",
            "\n",
            " Iteration: 1 \n",
            "\n",
            "Thought: I need to search for information about the scientific works of Dzmitry Paulenka to understand his contributions to his field. I will use the arxiv_search action to find relevant research papers or publications.\n",
            "\n",
            "Action: arxiv_search: Dzmitry Paulenka\n",
            "PAUSE\n",
            "\n",
            "\n",
            " -- running arxiv_search Dzmitry Paulenka --\n",
            "Next prompt: Observation: {\"title\": \"The Decoration Theorem for Mandelbrot and Multibrot Sets\", \"summary\": \"We prove the decoration theorem for the Mandelbrot set (and Multibrot sets)\\nwhich says that when a \\\"little Mandelbrot set\\\" is removed from the Mandelbrot\\nset, then most of the resulting connected components have small diameters.\"}\n",
            "\n",
            " Iteration: 2 \n",
            "\n",
            "Answer: Dzmitry Paulenka has contributed to scientific research, including a paper titled \"The Decoration Theorem for Mandelbrot and Multibrot Sets.\" This work proves the decoration theorem for the Mandelbrot set and Multibrot sets, stating that when a \"little Mandelbrot set\" is removed from the Mandelbrot set, most of the resulting connected components have small diameters.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Здесь ответ по умолчанию более подробный и полный, чем после поиска в Википедии.\n",
        "question = \"Что такое эффект зловещей долины?\"\n",
        "run_tests(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8HOveX8dywS",
        "outputId": "db6af7d2-0f58-47fe-c52b-9f972a781e90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эффект зловещей долины (или \"uncanny valley\" на английском) — это концепция, описывающая реакцию людей на роботов, анимацию или другие искусственные объекты, которые выглядят почти, но не совсем, как реальные люди. Когда внешний вид объекта становится всё более похожим на человеческий, наша реакция к нему становится положительной, но только до определённого момента. Если объект становится слишком похожим на человека, но всё же имеет некоторые отличия (например, неестественные движения, выражения лиц или детали), это может вызывать у людей чувство дискомфорта или даже отвращения.\n",
            "\n",
            "Термин был введён японским робототехником Масахиро Мори в 1970-х годах. Эффект зловещей долины имеет важные последствия в области дизайна роботов, анимации, видеоигр и виртуальной реальности, поскольку создатели стремятся создать более естественные и привлекательные образы, избегая при этом \"долины\", где возникает негативная реакция.\n",
            "\n",
            "----------\n",
            "\n",
            " Iteration: 1 \n",
            "\n",
            "Thought: Я должен поискать информацию о \"эффекте зловещей долины\" в Википедии, чтобы предоставить точное определение.  \n",
            "Action: wikipedia: Эффект зловещей долины  \n",
            "PAUSE\n",
            "\n",
            "\n",
            " -- running wikipedia Эффект зловещей долины   --\n",
            "Next prompt: Observation: <span class=\"searchmatch\">Эффе́кт</span> «<span class=\"searchmatch\">злове́щей</span> <span class=\"searchmatch\">доли́ны</span>» (яп. 不気味の谷 букими но тани, англ. uncanny valley) — явление, основанное на гипотезе, сформулированной японским учёным-робототехником\n",
            "\n",
            " Iteration: 2 \n",
            "\n",
            "Answer: Эффект зловещей долины (яп. 不気味の谷, англ. uncanny valley) — это явление, основанное на гипотезе, сформулированной японским учёным-робототехником, которое описывает реакцию людей на роботов или анимацию, которые выглядят почти, но не совсем как люди, вызывая чувство дискомфорта или отторжения.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OfMWhsFoh4YP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}