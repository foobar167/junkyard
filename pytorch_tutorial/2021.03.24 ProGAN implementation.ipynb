{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2021.03.24 ProGAN implementation.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMfNJzWIiCFeWRiXgBPVQ6h"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"OLbs3OM2Tfwp"},"source":["# ProGAN implementation\n","\n","[Original video](https://youtu.be/nkQHASviYac)\n","\n","[Source code](https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/GANs/ProGAN)\n","\n","[Paper walkthrough](https://youtu.be/lhs78if-E7E)\n","\n","[Paper 1](https://arxiv.org/abs/1710.10196), [paper 2](https://research.nvidia.com/sites/default/files/pubs/2017-10_Progressive-Growing-of/karras2018iclr-paper.pdf)\n","\n","[CelebA-HQ dataset](https://www.kaggle.com/lamsimon/celebahq)"]},{"cell_type":"markdown","metadata":{"id":"aQaQAkRXbbjm"},"source":["## Import libraries"]},{"cell_type":"code","metadata":{"id":"bajjy53Zb3tG"},"source":["import os\n","import torch\n","import random\n","import numpy as np\n","import torchvision\n","import torch.nn as nn\n","import multiprocessing\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","\n","from math import log2\n","from tqdm.notebook import tqdm\n","from scipy.stats import truncnorm\n","from torchvision.utils import save_image\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","\n","torch.backends.cudnn.benchmarks = True  # additional performance"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aM9Gd-50eCZm"},"source":["## Model\n","\n","![ProGAN fade](https://miro.medium.com/max/1580/1*-lY_AywUNxaWVmdo0qQ5sA.png)\n","\n","![ProGAN model](https://aisc.ai.science/static/post-assets/gan-collaborative-post/image20.png)"]},{"cell_type":"code","metadata":{"id":"8jFXztUdddA2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616693883801,"user_tz":-180,"elapsed":33033,"user":{"displayName":"Max Planck","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBGeFwU-LUM8Wyn9zmAOd8U2BBj-wFfVDmw-TU=s64","userId":"06869868537886587167"}},"outputId":"4a67e44e-7a81-481e-8c29-209dcbe9dcfe"},"source":["factors = [1, 1, 1, 1, 1/2, 1/4, 1/8, 1/16, 1/32]\n","\n","# Weighted Scaled Convolutional layer for equalized learning rate\n","class WSConv2d(nn.Module):\n","    \"\"\" Inspired and looked at:\n","        https://github.com/nvnbny/progressive_growing_of_gans/blob/master/modelUtils.py\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels,\n","                 kernel_size=3, stride=1, padding=1, gain=2):\n","        super().__init__()\n","\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n","        self.scale = (gain / (in_channels * kernel_size ** 2)) ** 0.5\n","        \n","        # do not scale the bias of {self.conv} layer\n","        self.bias = self.conv.bias\n","        self.conv.bias = None\n","\n","        # initialize conv layer, '_' for inplace normalization\n","        nn.init.normal_(self.conv.weight)\n","        nn.init.zeros_(self.bias)\n","\n","    def forward(self, x):\n","        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)\n","\n","\n","# normalization replacement for BatchNorm\n","class PixelNorm(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.epsilon = 1e-8\n","\n","    def forward(self, x):\n","        # mean across channels dim=1; keepdim=True element-wise division\n","        return x / torch.sqrt(torch.mean(x**2, dim=1, keepdim=True) + self.epsilon)\n","\n","\n","# block of convolutional layers to make code compact\n","class ConvBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, use_pixelnorm=True):\n","        super().__init__()\n","        self.use_pixelnorm = use_pixelnorm\n","        self.conv1 = WSConv2d(in_channels, out_channels)\n","        self.conv2 = WSConv2d(out_channels, out_channels)\n","        self.leaky = nn.LeakyReLU(0.2)\n","        self.pn = PixelNorm()\n","\n","    def forward(self, x):\n","        x = self.leaky(self.conv1(x))\n","        if self.use_pixelnorm: x = self.pn(x)\n","        x = self.leaky(self.conv2(x))\n","        if self.use_pixelnorm: x = self.pn(x)\n","        return x\n","\n","\n","class Generator(nn.Module):\n","    def __init__(self, z_dim, in_channels, img_channels=3):\n","        super().__init__()\n","        self.initial = nn.Sequential(\n","            PixelNorm(),\n","            nn.ConvTranspose2d(z_dim, in_channels, 4, 1, 0),  # 1x1 --> 4x4\n","            nn.LeakyReLU(0.2),\n","            WSConv2d(in_channels, in_channels),\n","            nn.LeakyReLU(0.2),\n","            PixelNorm(),\n","        )\n","        self.initial_rgb = WSConv2d(in_channels, img_channels,\n","                                    kernel_size=1, stride=1, padding=0)\n","        self.prog_blocks = nn.ModuleList([])\n","        self.rgb_layers = nn.ModuleList([self.initial_rgb])\n","\n","        # factors[i] --> factors[i+1]\n","        for i in range(len(factors) - 1):\n","            conv_in_c = int(in_channels * factors[i])\n","            conv_out_c = int(in_channels * factors[i+1])\n","            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c))\n","            self.rgb_layers.append(WSConv2d(conv_out_c, img_channels,\n","                                            kernel_size=1, stride=1, padding=0))\n","\n","    def fade_in(self, alpha, upscaled, generated):\n","        # alpha should be scalar within [0, 1], and upscale.shape == generated.shape\n","        return torch.tanh(alpha * generated + (1 - alpha) * upscaled)  # (-1, 1)\n","\n","    def forward(self, x, alpha, steps):\n","        out = self.initial(x)  # z_dim 1x1 --> out 4x4\n","\n","        # steps=0 (4x4), steps=1 (8x8), steps=2 (16x16), etc.\n","        if steps == 0:\n","            return self.initial_rgb(out)\n","\n","        for step in range(steps):\n","            upscaled = F.interpolate(out, scale_factor=2, mode='nearest')\n","            out = self.prog_blocks[step](upscaled)\n","\n","        # to RGB conversion\n","        final_upscaled = self.rgb_layers[steps-1](upscaled)\n","        final_out = self.rgb_layers[steps](out)\n","\n","        return self.fade_in(alpha, final_upscaled, final_out)\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, in_channels, img_channels=3):\n","        super().__init__()\n","        self.prog_blocks = nn.ModuleList([])\n","        self.rgb_layers = nn.ModuleList([])\n","        self.leaky = nn.LeakyReLU(0.2)\n","\n","        for i in range(len(factors)-1, 0, -1):  # move backward\n","            conv_in_c = int(in_channels * factors[i])\n","            conv_out_c = int(in_channels * factors[i-1])\n","            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c, use_pixelnorm=False))\n","            self.rgb_layers.append(WSConv2d(img_channels, conv_in_c,\n","                                            kernel_size=1, stride=1, padding=0))\n","        \n","        # mirror the initial rgb of the Generator; for final 4x4 image resolution\n","        initial_rgb = WSConv2d(img_channels, in_channels,\n","                                    kernel_size=1, stride=1, padding=0)\n","        self.rgb_layers.append(initial_rgb)\n","        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n","        \n","        # block for 4x4 resolution\n","        self.final_block = nn.Sequential(\n","            WSConv2d(in_channels+1, in_channels),\n","            nn.LeakyReLU(0.2),\n","            WSConv2d(in_channels, in_channels, kernel_size=4, stride=1, padding=0),\n","            nn.LeakyReLU(0.2),\n","            # we use WSConv2d instead of fully-connected layer, this is the same\n","            WSConv2d(in_channels, 1, kernel_size=1, stride=1, padding=0),\n","            nn.Flatten(),\n","        )\n","\n","    def fade_in(self, alpha, downscaled, out):\n","        return alpha * out + (1 - alpha) * downscaled\n","\n","    def minibatch_std(self, x):\n","        # (N,C,H,W) --> (N) std --> mean scalar --> (N,1,H,W) of repeated scalar value\n","        batch_statistics = torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n","        # add it like an additional channel\n","        return torch.cat([x, batch_statistics], dim=1)  # 512 --> 513 \n","\n","    def forward(self, x, alpha, steps):\n","        # steps=0 (4x4), steps=1 (8x8), steps=2 (16x16), etc.\n","        cur_step = len(self.prog_blocks) - steps\n","        out = self.leaky(self.rgb_layers[cur_step](x))\n","\n","        if steps == 0:\n","            out = self.minibatch_std(out)\n","            return self.final_block(out)\n","\n","        # make current step with from RGB conversion\n","        downscaled = self.leaky(self.rgb_layers[cur_step + 1](self.avg_pool(x)))\n","        out = self.avg_pool(self.prog_blocks[cur_step](out))\n","        out = self.fade_in(alpha, downscaled, out)\n","\n","        # make further steps\n","        for step in range(cur_step+1, len(self.prog_blocks)):\n","            out = self.avg_pool(self.prog_blocks[step](out))\n","\n","        out = self.minibatch_std(out)\n","        return self.final_block(out)\n","\n","\n","def test():\n","    Z_DIM = 100\n","    IN_CHANNELS = 256\n","    IMG_CHANNELS = 3\n","    BATCH_SIZE = 5\n","    gen = Generator(Z_DIM, IN_CHANNELS, IMG_CHANNELS)\n","    disc = Discriminator(IN_CHANNELS, IMG_CHANNELS)\n","\n","    for img_size in [4, 8, 16, 32, 64, 128, 256, 512, 1024]:\n","        num_steps = int(log2(img_size / 4))\n","        x = torch.randn((BATCH_SIZE, Z_DIM, 1, 1))\n","        z = gen(x, alpha=0.5, steps=num_steps)\n","        assert z.shape == (BATCH_SIZE, IMG_CHANNELS, img_size, img_size)\n","        out = disc(z, alpha=0.5, steps=num_steps)\n","        assert out.shape == (BATCH_SIZE, 1)\n","        print(f'Success! At img size: {img_size}')\n","\n","test()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Success! At img size: 4\n","Success! At img size: 8\n","Success! At img size: 16\n","Success! At img size: 32\n","Success! At img size: 64\n","Success! At img size: 128\n","Success! At img size: 256\n","Success! At img size: 512\n","Success! At img size: 1024\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"862BHN95UZ1f"},"source":["## Configuration parameters"]},{"cell_type":"code","metadata":{"id":"-UKKf2_FrJIa"},"source":["START_TRAIN_AT_IMG_SIZE = 4\n","DATASET = 'celeba_hq'\n","CHECKPOINT_DISC = 'disc-celeb.pth.tar'\n","CHECKPOINT_GEN = 'gen-celeb.pth.tar'\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","SAVE_MODEL = True\n","LOAD_MODEL = True\n","LEARNING_RATE = 1e-3\n","BATCH_SIZES = [512, 256, 128, 128, 64, 64, 32, 16, 8]\n","CHANNELS_IMG = 3\n","Z_DIM = 256  # should be 512 in original paper\n","IN_CHANNELS = 256  # should be 512 in original paper\n","LAMBDA_GP = 10\n","PROGRESSIVE_EPOCHS = [10] * len(BATCH_SIZES)\n","FIXED_NOISE = torch.randn(8, Z_DIM, 1, 1).to(DEVICE)\n","NUM_WORKERS = multiprocessing.cpu_count()\n","\n","# !mv disc.pth.tar disc-celeb.pth.tar\n","# !mv gen.pth.tar gen-celeb.pth.tar"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-db8-0bAuz01"},"source":["## Utils\n","  * for **plot_to_tensorboard** function review [Pytorch TensorBoard](https://colab.research.google.com/drive/1uftzYqL8gwmp2wvvujBHvRxfGLOxnO54)\n","  * for **gradient_penalty** function review [WGAN-GP implementation](https://colab.research.google.com/drive/193niPgYt2Qm8Ok5Dy2_z2xocwN-bygXd)\n","  * for **save_checkpoint** and **load_checkpoint** functions review [How to save and load models in Pytorch](https://colab.research.google.com/drive/1h5G53mEm_ez7zJj1qju-RF8FrjBlNCX2)\n","  * for **seed_everything** function (not used here) review [Quick Tips](https://colab.research.google.com/drive/1ZNBzRnUG2cJvYemO2Gz5JqOKIbuaBa4Y)"]},{"cell_type":"code","metadata":{"id":"Z0T6i4hku3E_"},"source":["# Print losses occasionally and print to tensorboard\n","def plot_to_tensorboard(writer, loss_disc, loss_gen, real, fake, tensorboard_step):\n","    writer.add_scalar('Loss discriminator', loss_disc, global_step=tensorboard_step)\n","\n","    with torch.no_grad():\n","        # take out (up to) 8 examples to plot\n","        img_grid_real = torchvision.utils.make_grid(real[:8], normalize=True)\n","        img_grid_fake = torchvision.utils.make_grid(fake[:8], normalize=True)\n","        writer.add_image('Real', img_grid_real, global_step=tensorboard_step)\n","        writer.add_image('Fake', img_grid_fake, global_step=tensorboard_step)\n","\n","\n","def gradient_penalty(disc, real, fake, alpha, train_step, device):\n","    BATCH_SIZE, C, H, W = real.shape\n","    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n","    interpolated_images = real * beta + fake.detach() * (1 - beta)\n","    interpolated_images.requires_grad_(True)\n","\n","    # Calculate discriminator scores\n","    mixed_scores = disc(interpolated_images, alpha, train_step)\n","\n","    # Take the gradient of the scores with respect to the images\n","    gradient = torch.autograd.grad(inputs=interpolated_images,\n","                                   outputs=mixed_scores,\n","                                   grad_outputs=torch.ones_like(mixed_scores),\n","                                   create_graph=True,\n","                                   retain_graph=True,)[0]\n","    gradient = gradient.view(gradient.shape[0], -1)\n","    gradient_norm = gradient.norm(2, dim=1)\n","    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n","    return gradient_penalty\n","\n","\n","def save_checkpoint(model, optimizer, filename):\n","    print('=> Saving checkpoint')\n","    checkpoint = {\n","        'state_dict': model.state_dict(),\n","        'optimizer': optimizer.state_dict(),\n","    }\n","    torch.save(checkpoint, filename)\n","\n","\n","def load_checkpoint(checkpoint_file, model, optimizer, lr):\n","    print('=> Loading checkpoint')\n","    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n","    model.load_state_dict(checkpoint['state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","\n","    # If we don't do this then it will just have learning rate of old checkpoint\n","    # and it will lead to many hours of debugging \\:\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","\n","\n","def seed_everything(seed=42):\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","\n","def generate_examples(gen, step, truncation=0.7, n=100):\n","    \"\"\" Tried using truncation trick here but not sure it actually helped anything,\n","        you can remove it if you like and just sample from torch.randn\n","    \"\"\"\n","    dir = 'saved_examples'\n","    os.makedirs(dir, exist_ok=True)\n","\n","    gen.eval()\n","    alpha = 1.0\n","    for i in range(n):\n","        with torch.no_grad():\n","            noise = torch.tensor(truncnorm.rvs(-truncation,\n","                                               truncation,\n","                                               size=(1, Z_DIM, 1, 1)),\n","                                 device=DEVICE,\n","                                 dtype=torch.float32)\n","            img = gen(noise, alpha, step)\n","            save_image(img * 0.5 + 0.5, os.path.join(dir, f'img_{i}.jpg'))\n","    gen.train()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N60YSRDTxRkQ"},"source":["## Prepare Train"]},{"cell_type":"code","metadata":{"id":"ou1jFTVKsR2v"},"source":["def get_loader(step):\n","    image_size = 4 * 2 ** step  # 4->0, 8->1, 16->2, 32->3, 64->4\n","    print(f'Current image size: {image_size}')\n","    transform = transforms.Compose([\n","            transforms.Resize((image_size, image_size)),\n","            transforms.ToTensor(),\n","            transforms.RandomHorizontalFlip(p=0.5),\n","            transforms.Normalize([0.5 for _ in range(CHANNELS_IMG)],\n","                                 [0.5 for _ in range(CHANNELS_IMG)]),\n","    ])\n","    batch_size = BATCH_SIZES[step]\n","    dataset = datasets.ImageFolder(root=DATASET, transform=transform)\n","    loader = DataLoader(dataset,\n","                        batch_size=batch_size,\n","                        shuffle=True,\n","                        num_workers=NUM_WORKERS,\n","                        pin_memory=True,)\n","    return loader, dataset\n","\n","\n","def train(disc, gen, loader, dataset, step, alpha, opt_disc, opt_gen,\n","          tensorboard_step, writer, scaler_gen, scaler_disc, epoch, num_epochs):\n","\n","    loop = tqdm(loader, leave=False)\n","    loop.set_description(f'Epoch [{epoch}/{num_epochs}]')\n","\n","    for batch_idx, (real, _) in enumerate(loop):\n","        real = real.to(DEVICE)\n","        cur_batch_size = real.shape[0]\n","\n","        # Train Discriminator: min -(E[D(real)] - E[D(fake)])\n","        noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(DEVICE)\n","        with torch.cuda.amp.autocast():\n","            fake = gen(noise, alpha, step)\n","            disc_real = disc(real, alpha, step)\n","            disc_fake = disc(fake.detach(), alpha, step)\n","            gp = gradient_penalty(disc, real, fake, alpha, step, DEVICE)\n","\n","            loss_disc = (\n","                -(torch.mean(disc_real) - torch.mean(disc_fake))\n","                + LAMBDA_GP * gp\n","\n","                # keep the discriminator output from drifting too far away from zero\n","                + 0.001 * torch.mean(disc_real ** 2)\n","            )\n","\n","        opt_disc.zero_grad()\n","        scaler_disc.scale(loss_disc).backward()\n","        scaler_disc.step(opt_disc)\n","        scaler_disc.update()\n","\n","        # Train Generator: max E[D(fake)] or min -(E[D(fake)])\n","        with torch.cuda.amp.autocast():\n","            gen_fake = disc(fake, alpha, step)\n","            loss_gen = -torch.mean(gen_fake)\n","\n","        opt_gen.zero_grad()\n","        scaler_gen.scale(loss_gen).backward()\n","        scaler_gen.step(opt_gen)\n","        scaler_gen.update()\n","\n","        # update alpha and ensure it less than 1\n","        # half epochs alpha < 1, half epochs alpha == 1\n","        alpha += cur_batch_size / (len(dataset) * PROGRESSIVE_EPOCHS[step] * 0.5)\n","        alpha = min(alpha, 1)\n","\n","        if batch_idx % 500 == 0:\n","            with torch.no_grad():\n","                fixed_fakes = gen(FIXED_NOISE, alpha, step) * 0.5 + 0.5\n","            plot_to_tensorboard(writer,\n","                                loss_disc.item(),\n","                                loss_gen.item(),\n","                                real.detach(),\n","                                fixed_fakes.detach(),\n","                                tensorboard_step,)\n","            tensorboard_step += 1\n","\n","        loop.set_postfix(gp=gp.item(), loss_disc=loss_disc.item(),)\n","\n","    return tensorboard_step, alpha\n","\n","\n","def main():\n","    # initialize gen and disc, note: discriminator should be called critic,\n","    # according to WGAN paper (since it no longer outputs between [0, 1])\n","    # but really who cares..\n","    disc = Discriminator(IN_CHANNELS, CHANNELS_IMG).to(DEVICE)\n","    gen = Generator(Z_DIM, IN_CHANNELS, CHANNELS_IMG).to(DEVICE)\n","\n","    # initialize optimizers and scalers for FP16 training\n","    opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99))\n","    opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99))\n","    scaler_disc = torch.cuda.amp.GradScaler()\n","    scaler_gen = torch.cuda.amp.GradScaler()\n","\n","    # for tensorboard plotting\n","    writer = SummaryWriter(LOG_DIR)\n","\n","    if (LOAD_MODEL\n","        and os.path.exists(CHECKPOINT_DISC)\n","        and os.path.exists(CHECKPOINT_GEN)):\n","        load_checkpoint(CHECKPOINT_DISC, disc, opt_disc, LEARNING_RATE)\n","        load_checkpoint(CHECKPOINT_GEN, gen, opt_gen, LEARNING_RATE)\n","\n","    gen.train()\n","    disc.train()\n","\n","    tensorboard_step = 0\n","    # start at step that corresponds to img size that we set in configuration\n","    step = int(log2(START_TRAIN_AT_IMG_SIZE / 4))\n","\n","    for num_epochs in PROGRESSIVE_EPOCHS[step:]:\n","        alpha = 1e-5  # start with very low alpha\n","        loader, dataset = get_loader(step)\n","\n","        for epoch in range(1, num_epochs+1):\n","            tensorboard_step, alpha = train(\n","                disc, gen, loader, dataset, step, alpha, opt_disc, opt_gen,\n","                tensorboard_step, writer, scaler_gen, scaler_disc,\n","                epoch, num_epochs)\n","\n","            if SAVE_MODEL:\n","                save_checkpoint(disc, opt_disc, CHECKPOINT_DISC)\n","                save_checkpoint(gen, opt_gen, CHECKPOINT_GEN)\n","\n","        generate_examples(gen, step)\n","        step += 1  # progress to the next img size"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EPeEDM1PsDmB"},"source":["## Download datasets: \"CelebA-HQ\" and \"Cats vs Dogs\"\n","\n","CelebA-HQ: 2.55 G\n","\n","Cats vs Dogs: 800 M"]},{"cell_type":"code","metadata":{"id":"zbO5I5f6Te79","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":"OK"}},"base_uri":"https://localhost:8080/","height":112},"executionInfo":{"status":"ok","timestamp":1616693984296,"user_tz":-180,"elapsed":94722,"user":{"displayName":"Max Planck","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBGeFwU-LUM8Wyn9zmAOd8U2BBj-wFfVDmw-TU=s64","userId":"06869868537886587167"}},"outputId":"f66b68e2-b6e1-4919-eb6f-f808bcdfa116"},"source":["# Get dataset from Kaggle\n","\n","# Colab's file access feature\n","from google.colab import files\n","\n","# Upload `kaggle.json` file\n","uploaded = files.upload()\n","\n","# Retrieve uploaded file and print results\n","for fn in uploaded.keys():\n","  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n","      name=fn, length=len(uploaded[fn])))\n","\n","\n","# Then copy kaggle.json into the folder where the API expects to find it.\n","!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json\n","!ls ~/.kaggle\n","\n","# Download the dataset\n","# !kaggle datasets list -s celeb\n","!kaggle datasets download -d lamsimon/celebahq\n","# !kaggle competitions download -c dogs-vs-cats"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-f89628c5-1648-4c0e-bff6-53525a99387f\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-f89628c5-1648-4c0e-bff6-53525a99387f\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["kaggle.json\n","Downloading celebahq.zip to /content\n","100% 2.54G/2.55G [01:15<00:00, 63.5MB/s]\n","100% 2.55G/2.55G [01:15<00:00, 36.2MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"s08MgNv5bazm"},"source":["# Extract data\n","import zipfile\n","import tarfile\n","\n","def extract(fname):\n","    if fname.endswith('.tar.gz') or fname.endswith('.tgz'):\n","        ref = tarfile.open(fname, mode='r:gz')\n","    elif fname.endswith('.tar'):\n","        ref = tarfile.open(fname, mode='r:')\n","    elif fname.endswith('.tar.bz2') or fname.endswith('.tbz'):\n","        ref = tarfile.open(fname, mode='r:bz2')\n","    elif fname.endswith('.zip'):\n","        ref = zipfile.ZipFile(fname, mode='r')\n","\n","    ref.extractall()\n","    ref.close()\n","\n","extract('celebahq.zip')\n","# extract('train.zip')\n","\n","# remove archives\n","!rm celebahq.zip\n","# !rm test1.zip train.zip sampleSubmission.csv\n","\n","# move train into subfolder for datasets.ImageFolder\n","# !mkdir -p cats-dogs/images\n","# !mv -T train cats-dogs/images"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"btvpgawzfvlz"},"source":["Copy saved models from Google if necessary."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NeMPe_vce1To","executionInfo":{"status":"ok","timestamp":1616687474848,"user_tz":-180,"elapsed":43641,"user":{"displayName":"Max Planck","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBGeFwU-LUM8Wyn9zmAOd8U2BBj-wFfVDmw-TU=s64","userId":"06869868537886587167"}},"outputId":"75ee0816-05c4-4dcf-8a04-fa7954082c93"},"source":["# from google.colab import drive\n","# drive.mount('/content/gdrive')\n","\n","# copy_from = '/content/gdrive/MyDrive/Colab Notebooks/PyTorch tutorial'\n","\n","# # !ls -hal '$copy_from'\n","\n","# !cp '$copy_from'/'gen-celeb.pth.tar' .\n","# !cp '$copy_from'/'disc-celeb.pth.tar' .\n","\n","# !cp '$copy_from'/'gen-cats-dogs.pth.tar' .\n","# !cp '$copy_from'/'disc-cats-dogs.pth.tar' ."],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LKpmD4x9r72x"},"source":["# Run TensorBoard"]},{"cell_type":"code","metadata":{"id":"Ti6sBTM6TuQw"},"source":["# Run TensorBoard\n","\n","# Delete previous logs dir\n","LOG_DIR = 'logs/gan1'\n","if os.path.exists(LOG_DIR):\n","    !rm -rf $LOG_DIR\n","\n","# To fix the error, because PyTorch and TensorFlow are installed both:\n","# AttributeError: module 'tensorflow._api.v2.io.gfile' has no attribute 'get_filesystem'\n","import tensorflow as tf\n","import tensorboard as tb\n","tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n","\n","# Load the TensorBoard notebook extension\n","%load_ext tensorboard\n","\n","# Start TensorBoard before training to monitor it in progress\n","%tensorboard --logdir $LOG_DIR\n","\n","# Reload TensorBoard\n","%reload_ext tensorboard"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"50QkfgOcAGem"},"source":["## Train"]},{"cell_type":"code","metadata":{"id":"XQFnSkIyxnPO"},"source":["START_TRAIN_AT_IMG_SIZE = 32  # 4\n","\n","main()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jVYnTr0tGv1h"},"source":["## Train cats and dogs"]},{"cell_type":"code","metadata":{"id":"yOq7pGtXAIL9"},"source":["START_TRAIN_AT_IMG_SIZE = 64  # 4\n","DATASET = 'cats-dogs'\n","CHECKPOINT_GEN = 'gen-cats-dogs.pth.tar'\n","CHECKPOINT_DISC = 'disc-cats-dogs.pth.tar'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z4jHJ-2cHKbz"},"source":["# Run TensorBoard\n","\n","# Delete previous logs dir\n","LOG_DIR = 'logs/cats-dogs'\n","if os.path.exists(LOG_DIR):\n","    !rm -rf $LOG_DIR\n","\n","# To fix the error, because PyTorch and TensorFlow are installed both:\n","# AttributeError: module 'tensorflow._api.v2.io.gfile' has no attribute 'get_filesystem'\n","import tensorflow as tf\n","import tensorboard as tb\n","tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n","\n","# Load the TensorBoard notebook extension\n","%load_ext tensorboard\n","\n","# Start TensorBoard before training to monitor it in progress\n","%tensorboard --logdir $LOG_DIR\n","\n","# Reload TensorBoard\n","%reload_ext tensorboard"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ghUx46UQHfzR"},"source":["torch.cuda.empty_cache()\n","\n","main()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A_wJZxdhf3pw"},"source":["Copy saved models to Google drive if necessary."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jHbQ0EPAHifn","executionInfo":{"status":"ok","timestamp":1616710652928,"user_tz":-180,"elapsed":5091,"user":{"displayName":"Max Planck","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBGeFwU-LUM8Wyn9zmAOd8U2BBj-wFfVDmw-TU=s64","userId":"06869868537886587167"}},"outputId":"d0154fb6-26c9-4c6d-8878-822cfaedbf8c"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","copy_to = '/content/gdrive/MyDrive/Colab Notebooks/PyTorch tutorial'\n","\n","# !ls -hal '$copy_to'\n","\n","!cp 'gen-celeb.pth.tar' '$copy_to'\n","!cp 'disc-celeb.pth.tar' '$copy_to'\n","\n","!cp 'gen-cats-dogs.pth.tar' '$copy_to'\n","!cp 'disc-cats-dogs.pth.tar' '$copy_to'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xCRo1APOIrGp"},"source":[""],"execution_count":null,"outputs":[]}]}