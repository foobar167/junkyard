{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2021.04.05 Transformers implementation.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPpg84vr0YmRZPqT4ZQn6+b"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"csHsr9iwEdpM"},"source":["# Transformers implementation\n","\n","[Original video](https://youtu.be/U0s0f995w14)\n","\n","Resources:\n","  * Paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n","  * [Transformers from scratch](http://peterbloem.nl/blog/transformers) with [GitHub code](https://github.com/pbloem/former) and [video lecture](https://www.youtube.com/playlist?list=PLIXJ-Sacf8u60G1TwcznBmK6rEL3gmZmV)"]},{"cell_type":"code","metadata":{"id":"TNSGfAn2EWCx","executionInfo":{"status":"ok","timestamp":1617713103128,"user_tz":-180,"elapsed":631,"user":{"displayName":"Max Planck","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBGeFwU-LUM8Wyn9zmAOd8U2BBj-wFfVDmw-TU=s64","userId":"06869868537886587167"}}},"source":["import torch\n","import torch.nn as nn"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZTpu_BZEqr4j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617713103866,"user_tz":-180,"elapsed":1358,"user":{"displayName":"Max Planck","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBGeFwU-LUM8Wyn9zmAOd8U2BBj-wFfVDmw-TU=s64","userId":"06869868537886587167"}},"outputId":"37af7719-045f-461d-89bb-d6830eba94ea"},"source":["class SelfAttention(nn.Module):\n","    def __init__(self, embed_size, heads):\n","        super().__init__()\n","        self.embed_size = embed_size\n","        self.heads = heads\n","        self.head_dim = embed_size // heads\n","\n","        assert(self.head_dim * heads == embed_size), 'Embed size have to be divisible by heads'\n","\n","        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n","\n","    def forward(self, value, key, query, mask):\n","        N = query.shape[0]\n","        value_len, key_len, query_len = value.shape[1], key.shape[1], query.shape[1]\n","\n","        # Split embedding into {self.heads} pieces\n","        value = value.reshape(N, value_len, self.heads, self.head_dim)\n","        key = key.reshape(N, key_len, self.heads, self.head_dim)\n","        query = query.reshape(N, query_len, self.heads, self.head_dim)\n","\n","        value = self.values(value)\n","        key = self.keys(key)\n","        query = self.queries(query)\n","\n","        # query shape (N, query_len, heads, head_dim)\n","        # key shape (N, key_len, heads, head_dim)\n","        # energy shape (N, heads, query_len, key_len)\n","        energy = torch.einsum('nqhd, nkhd -> nhqk', [query, key])\n","\n","        if mask is not None:\n","            energy = energy.masked_fill(mask == 0, float('-1e20'))  # set to -âˆž\n","        \n","        # normalize across key_len\n","        attention = torch.softmax(energy / (self.embed_size ** 0.5), dim=3)\n","\n","        # attention shape (N, heads, query_len, key_len)\n","        # value shape (N, value_len, heads, head_dim)\n","        # out shape (N, query_len, heads, head_dim), where key_len == value_len\n","        out = torch.einsum('nhql, nlhd -> nqhd', [attention, value]).reshape(\n","            N, query_len, self.heads * self.head_dim\n","        )  # concat / flatten heads and head_dim\n","\n","        return self.fc_out(out)\n","\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, embed_size, heads, dropout, forward_expansion):\n","        super().__init__()\n","        self.attention = SelfAttention(embed_size, heads)\n","        self.norm1 = nn.LayerNorm(embed_size)\n","        self.norm2 = nn.LayerNorm(embed_size)\n","        self.feed_forward = nn.Sequential(\n","            nn.Linear(embed_size, forward_expansion * embed_size),\n","            nn.ReLU(),\n","            nn.Linear(forward_expansion * embed_size, embed_size),\n","        )\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, value, key, query, mask):\n","        attention = self.attention(value, key, query, mask)\n","        x = self.dropout(self.norm1(attention + query))\n","        forward = self.feed_forward(x)\n","        out = self.dropout(self.norm2(forward + x))\n","        return out\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(self, src_vocab_size, embed_size, num_layers, heads,\n","                 forward_expansion, dropout, device, max_length):\n","        super().__init__()\n","        self.embed_size = embed_size\n","        self.device = device\n","        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n","        self.position_embedding = nn.Embedding(max_length, embed_size)\n","        self.layers = nn.ModuleList([\n","            TransformerBlock(embed_size, heads, dropout, forward_expansion)\n","            for _ in range(num_layers)\n","        ])\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask):\n","        N, seq_length = x.shape\n","        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n","        out = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n","\n","        for layer in self.layers:\n","            out = layer(out, out, out, mask)\n","        \n","        return out\n","\n","\n","class DecoderBlock(nn.Module):\n","    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n","        super().__init__()\n","        self.attention = SelfAttention(embed_size, heads)\n","        self.norm = nn.LayerNorm(embed_size)\n","        self.transformer_block = TransformerBlock(\n","            embed_size, heads, dropout, forward_expansion)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, value, key, src_mask, trg_mask):\n","        attention = self.attention(x, x, x, trg_mask)\n","        query = self.dropout(self.norm(attention + x))\n","        out = self.transformer_block(value, key, query, src_mask)\n","        return out\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(self, trg_vocab_size, embed_size, num_layers, heads,\n","                 forward_expansion, dropout, device, max_length):\n","        super().__init__()\n","        self.device = device\n","        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n","        self.position_embedding = nn.Embedding(max_length, embed_size)\n","        self.layers = nn.ModuleList([\n","            DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n","            for _ in range(num_layers)\n","        ])\n","        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, enc_out, src_mask, trg_mask):\n","        N, seq_length = x.shape\n","        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n","        x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n","\n","        for layer in self.layers:\n","            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n","        \n","        return self.fc_out(x)\n","\n","\n","class Transformer(nn.Module):\n","    def __init__(self, src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx,\n","                 embed_size=256, num_layers=6, forward_expansion=4,\n","                 heads=8, dropout=0, device='cpu', max_length=100):\n","        super().__init__()\n","        self.encoder = Encoder(src_vocab_size, embed_size, num_layers, heads,\n","                               forward_expansion, dropout, device, max_length)\n","        self.decoder = Decoder(trg_vocab_size, embed_size, num_layers, heads,\n","                               forward_expansion, dropout, device, max_length)\n","        self.src_pad_idx = src_pad_idx\n","        self.trg_pad_idx = trg_pad_idx\n","        self.device = device\n","\n","    def make_src_mask(self, src):\n","        # (N, 1, 1, src_length)\n","        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n","        return src_mask.to(self.device)\n","\n","    def make_trg_mask(self, trg):\n","        N, trg_len = trg.shape\n","        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n","            N, 1, trg_len, trg_len).to(self.device)\n","        return trg_mask\n","\n","    def forward(self, src, trg):\n","        src_mask = self.make_src_mask(src)\n","        trg_mask = self.make_trg_mask(trg)\n","        enc_source = self.encoder(src, src_mask)\n","        out = self.decoder(trg, enc_source, src_mask, trg_mask)\n","        return out\n","\n","\n","def test():\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0],\n","                      [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(device)\n","    trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0],\n","                        [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n","    src_pad_idx, trg_pad_idx = 0, 0\n","    src_vocab_size, trg_vocab_size = 10, 10\n","    model = Transformer(src_vocab_size, trg_vocab_size,\n","                        src_pad_idx, trg_pad_idx).to(device)\n","    out = model(x, trg[:, :-1])\n","    print(out.shape)\n","\n","\n","test()"],"execution_count":14,"outputs":[{"output_type":"stream","text":["torch.Size([2, 7, 10])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8CFixOPsMy-B","executionInfo":{"status":"ok","timestamp":1617713104086,"user_tz":-180,"elapsed":1572,"user":{"displayName":"Max Planck","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBGeFwU-LUM8Wyn9zmAOd8U2BBj-wFfVDmw-TU=s64","userId":"06869868537886587167"}}},"source":[""],"execution_count":14,"outputs":[]}]}