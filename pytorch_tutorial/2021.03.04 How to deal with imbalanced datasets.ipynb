{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2021.03.04 How to deal with imbalanced datasets.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN+4TtzsM415IkfhFKGb6rz"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"S49jkiuSL09x"},"source":["# How to deal with imbalanced datasets\r\n","\r\n","[Original video](https://youtu.be/4JFVhJyTZ44)\r\n","\r\n","For how to create CSV file review [2021.03.02-3 Build custom dataset](https://colab.research.google.com/drive/1v4_SaGOXDprZNP7YcTmi21o5v56YSGDD) CoLab.\r\n","\r\n","Methods for dealing with imbalanced datasets:\r\n","  1. Oversampling (more augmentations, etc.)\r\n","  2. Class weighting (give higher priority for the network)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_D-epLrfLubH","executionInfo":{"status":"ok","timestamp":1614887592893,"user_tz":-180,"elapsed":2890,"user":{"displayName":"Max Planck","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBGeFwU-LUM8Wyn9zmAOd8U2BBj-wFfVDmw-TU=s64","userId":"06869868537886587167"}},"outputId":"7cee2b02-0061-426e-b9c7-2037d44b7eca"},"source":["# Get Aladdin Persson GitHub repository\r\n","!git clone https://github.com/aladdinpersson/Machine-Learning-Collection.git"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Cloning into 'Machine-Learning-Collection'...\n","remote: Enumerating objects: 247, done.\u001b[K\n","remote: Counting objects: 100% (247/247), done.\u001b[K\n","remote: Compressing objects: 100% (216/216), done.\u001b[K\n","remote: Total 508 (delta 52), reused 137 (delta 20), pack-reused 261\u001b[K\n","Receiving objects: 100% (508/508), 19.44 MiB | 23.06 MiB/s, done.\n","Resolving deltas: 100% (103/103), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r4dlQtL-Tv3T","executionInfo":{"status":"ok","timestamp":1614887595880,"user_tz":-180,"elapsed":5865,"user":{"displayName":"Max Planck","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBGeFwU-LUM8Wyn9zmAOd8U2BBj-wFfVDmw-TU=s64","userId":"06869868537886587167"}}},"source":["import os\r\n","import torch\r\n","import pandas as pd\r\n","import torch.nn as nn\r\n","import torchvision.datasets as datasets\r\n","import torchvision.transforms as transforms\r\n","\r\n","from PIL import Image\r\n","from torch.utils.data import WeightedRandomSampler, DataLoader, Dataset"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"soIqgGk6MGvt","executionInfo":{"status":"ok","timestamp":1614887595884,"user_tz":-180,"elapsed":5862,"user":{"displayName":"Max Planck","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBGeFwU-LUM8Wyn9zmAOd8U2BBj-wFfVDmw-TU=s64","userId":"06869868537886587167"}},"outputId":"f6e901a6-2017-4689-af11-6cb33e4a88d6"},"source":["images_dir = 'Machine-Learning-Collection/ML/Pytorch/Basics/custom_dataset/cats_dogs_resized'\r\n","csv_file = 'Machine-Learning-Collection/ML/Pytorch/Basics/custom_dataset/cats_dogs.csv'\r\n","\r\n","df = pd.read_csv(csv_file)\r\n","num_samples = list(df.value_counts(subset='Label', normalize=True))\r\n","weights = [1/x for x in num_samples]\r\n","\r\n","num_classes = len(num_samples)\r\n","\r\n","print(f'samples: {num_samples}')\r\n","print(f'weights: {weights}')\r\n","print(f'classes: {num_classes}')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["samples: [0.8, 0.2]\n","weights: [1.25, 5.0]\n","classes: 2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fzeVUAakX_hI"},"source":["## Class weighting"]},{"cell_type":"code","metadata":{"id":"gh1a8XwmMlXs","executionInfo":{"status":"ok","timestamp":1614887595887,"user_tz":-180,"elapsed":5857,"user":{"displayName":"Max Planck","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBGeFwU-LUM8Wyn9zmAOd8U2BBj-wFfVDmw-TU=s64","userId":"06869868537886587167"}}},"source":["# Multiply loss on weight of the class\r\n","loss_fn = nn.CrossEntropyLoss(weight=torch.tensor(weights))"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cq7qbz3KYDX7"},"source":["## Oversampling"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M3Y4NqOSYFbB","executionInfo":{"status":"ok","timestamp":1614887598413,"user_tz":-180,"elapsed":8378,"user":{"displayName":"Max Planck","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBGeFwU-LUM8Wyn9zmAOd8U2BBj-wFfVDmw-TU=s64","userId":"06869868537886587167"}},"outputId":"aade40a9-507f-4d86-a86e-d0e608ad7d5e"},"source":["class CatsAndDogsDataset(Dataset):\r\n","    def __init__(self, csv_file, root_dir, transform=None):\r\n","        self.annotations = pd.read_csv(csv_file)\r\n","        self.root_dir = root_dir\r\n","        self.transform = transform\r\n","\r\n","    def __len__(self):\r\n","        return len(self.annotations)\r\n","\r\n","    def __getitem__(self, index):\r\n","        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\r\n","        image = Image.open(img_path)\r\n","        y_label = torch.tensor(int(self.annotations.iloc[index, 1]))\r\n","        \r\n","        if self.transform:\r\n","            image = self.transform(image)\r\n","        \r\n","        return (image, y_label)\r\n","\r\n","\r\n","def get_loader(root_dir, batch_size):\r\n","    my_transforms = transforms.Compose([\r\n","        transforms.Resize((224, 224)),\r\n","        transforms.ToTensor(),\r\n","    ])\r\n","\r\n","    dataset = CatsAndDogsDataset(csv_file=csv_file,\r\n","                                 root_dir=images_dir,\r\n","                                 transform=my_transforms)\r\n","    \r\n","    annotations = pd.read_csv(csv_file)\r\n","    num_samples = list(df.value_counts(subset='Label', normalize=True))\r\n","    class_weights = [1/x for x in num_samples]\r\n","    sample_weights = [0] * len(dataset)\r\n","\r\n","    for idx, (data, label) in enumerate(dataset):\r\n","        sample_weights[idx] = class_weights[label]\r\n","\r\n","    # if replacement=False, then example is used only once\r\n","    # if replacement=True, use example several times for oversampling\r\n","    sampler = WeightedRandomSampler(sample_weights,\r\n","                                    num_samples=len(sample_weights),\r\n","                                    replacement=True)\r\n","    loader = DataLoader(dataset=dataset, batch_size=batch_size, sampler=sampler)\r\n","    \r\n","    return loader\r\n","\r\n","\r\n","def main():\r\n","    loader = get_loader(root_dir=images_dir, batch_size=8)\r\n","    \r\n","    class_frequency = [0] * num_classes\r\n","    for epoch in range(100):\r\n","        # Get one batch\r\n","        for data, labels in loader:\r\n","            # print(data.shape, labels)\r\n","            for l in labels:\r\n","                class_frequency[l] += 1\r\n","    print(f'class frequency: {class_frequency}')\r\n","\r\n","if __name__ == '__main__':\r\n","    main()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["class frequency: [503, 497]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QI1PUFEYestG","executionInfo":{"status":"ok","timestamp":1614887598415,"user_tz":-180,"elapsed":8372,"user":{"displayName":"Max Planck","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBGeFwU-LUM8Wyn9zmAOd8U2BBj-wFfVDmw-TU=s64","userId":"06869868537886587167"}}},"source":[""],"execution_count":5,"outputs":[]}]}