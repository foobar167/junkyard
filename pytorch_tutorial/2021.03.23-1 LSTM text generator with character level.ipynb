{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2021.03.23-1 LSTM text generator with character level.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOp4y4aum6dHwdZ4lGsHnwx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"_ltmXB8q-bBl"},"source":["# LSTM text generator\n","\n","[Original video](https://youtu.be/WujVlF_6h5A)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"67lFuDBgH7TO","executionInfo":{"status":"ok","timestamp":1616494287843,"user_tz":-180,"elapsed":6606,"user":{"displayName":"Max Planck","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBGeFwU-LUM8Wyn9zmAOd8U2BBj-wFfVDmw-TU=s64","userId":"06869868537886587167"}},"outputId":"5bbb264c-68f5-4bdf-a2e7-0c66081715a3"},"source":["!pip install Unidecode\n","\n","!wget https://raw.githubusercontent.com/aladdinpersson/Machine-Learning-Collection/master/ML/Projects/text_generation_babynames/data/example_names.txt\n","!wget https://raw.githubusercontent.com/aladdinpersson/Machine-Learning-Collection/master/ML/Projects/text_generation_babynames/data/names.txt\n","\n","!cat example_names.txt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting Unidecode\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n","\r\u001b[K     |█▍                              | 10kB 16.7MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 20.2MB/s eta 0:00:01\r\u001b[K     |████                            | 30kB 15.9MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 40kB 14.5MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 11.9MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71kB 10.4MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 81kB 11.4MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 92kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 122kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 133kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 153kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 163kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 174kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 184kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 194kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 204kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 215kB 11.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 225kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 235kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 11.0MB/s \n","\u001b[?25hInstalling collected packages: Unidecode\n","Successfully installed Unidecode-1.2.0\n","--2021-03-23 10:11:27--  https://raw.githubusercontent.com/aladdinpersson/Machine-Learning-Collection/master/ML/Projects/text_generation_babynames/data/example_names.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 118 [text/plain]\n","Saving to: ‘example_names.txt’\n","\n","example_names.txt   100%[===================>]     118  --.-KB/s    in 0s      \n","\n","2021-03-23 10:11:27 (5.32 MB/s) - ‘example_names.txt’ saved [118/118]\n","\n","--2021-03-23 10:11:27--  https://raw.githubusercontent.com/aladdinpersson/Machine-Learning-Collection/master/ML/Projects/text_generation_babynames/data/names.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 7303572 (7.0M) [text/plain]\n","Saving to: ‘names.txt’\n","\n","names.txt           100%[===================>]   6.96M  --.-KB/s    in 0.1s    \n","\n","2021-03-23 10:11:28 (51.2 MB/s) - ‘names.txt’ saved [7303572/7303572]\n","\n","Niela\n","Elia\n","Leneth\n","Ley\n","Ira\n","Bernandel\n","Gelico\n","Marti\n","Ednie\n","Ozel\n","Marin\n","Elithon\n","Mirce\n","Elie\n","Elvar\n","Domarine\n","Artha\n","Audrey\n","Davyd"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PZrrW5xc-Rez"},"source":["import os\n","import sys\n","import torch\n","import string\n","import random\n","import unidecode\n","import torch.nn as nn\n","\n","from torch.utils.tensorboard import SummaryWriter"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mB6QLbuF_WxD"},"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# get characters from string.printable\n","all_characters = string.printable\n","n_characters = len(all_characters)\n","\n","file = unidecode.unidecode(open('names.txt').read())  # unidecode to ASCII format"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HghjQxPUjwWg"},"source":["# Run TensorBoard\n","\n","# Delete previous logs dir\n","log_dir = 'runs/names0'\n","if os.path.exists(log_dir):\n","    !rm -rf $log_dir\n","\n","# To fix the error, because PyTorch and TensorFlow are installed both:\n","# AttributeError: module 'tensorflow._api.v2.io.gfile' has no attribute 'get_filesystem'\n","import tensorflow as tf\n","import tensorboard as tb\n","tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n","\n","# Load the TensorBoard notebook extension\n","%load_ext tensorboard\n","\n","# Start TensorBoard before training to monitor it in progress\n","%tensorboard --logdir $log_dir\n","\n","# Reload TensorBoard\n","%reload_ext tensorboard"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"seR2_NABCYCF"},"source":["class RNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, output_size):\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","\n","        self.embed = nn.Embedding(input_size, hidden_size)\n","        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x, hidden, cell):\n","        out = self.embed(x)\n","        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n","        out = self.fc(out.reshape(out.shape[0], -1))\n","        return out, (hidden, cell)\n","\n","    def init_hidden(self, batch_size):\n","        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n","        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n","        return hidden, cell\n","\n","\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.chunk_len = 250\n","        self.num_epochs = 2000\n","        self.batch_size = 32\n","        self.print_every = 100\n","        self.hidden_size = 256\n","        self.num_layers = 2\n","        self.lr = 0.003\n","\n","    def char_tensor(self, string):  # char to vector of size {n_characters}\n","        tensor = torch.zeros(len(string)).long()\n","        for i in range(len(string)):\n","            tensor[i] = all_characters.index(string[i])\n","        return tensor\n","\n","    def get_random_batch(self):  # get {self.chunk_len} chars in a batch\n","        text_input = torch.zeros(self.batch_size, self.chunk_len)\n","        text_target = torch.zeros(self.batch_size, self.chunk_len)\n","\n","        for i in range(self.batch_size):\n","            start_idx = random.randint(0, len(file) - self.chunk_len)\n","            end_idx = start_idx + self.chunk_len + 1\n","            text_str = file[start_idx:end_idx]\n","\n","            text_input[i, :] = self.char_tensor(text_str[:-1])\n","            text_target[i, :] = self.char_tensor(text_str[1:])\n","\n","        return text_input.long(), text_target.long()\n","\n","    # generate some names\n","    def generate(self, initial_str='Ab', prediction_len=100, temperature=0.85):\n","        hidden, cell = self.rnn.init_hidden(1)\n","        initial_input = self.char_tensor(initial_str)\n","        predicted = initial_str\n","\n","        for i in range(len(initial_str)-1):\n","            _, (hidden, cell) = self.rnn(initial_input[i].view(1).to(device),\n","                                         hidden, cell)\n","        last_char = initial_input[-1]\n","\n","        for i in range(prediction_len):\n","            output, (hidden, cell) = self.rnn(last_char.view(1).to(device),\n","                                         hidden, cell)\n","            output_dist = output.data.view(-1).div(temperature).exp()\n","            top_char = torch.multinomial(output_dist, 1)[0]\n","            predicted_char = all_characters[top_char]\n","            predicted += predicted_char\n","            last_char = self.char_tensor(predicted_char)\n","\n","        return predicted\n","\n","    def train(self):  # train RNN\n","        self.rnn = RNN(input_size=n_characters,\n","                       hidden_size=self.hidden_size,\n","                       num_layers=self.num_layers,\n","                       output_size=n_characters).to(device)\n","        optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n","        criterion = nn.CrossEntropyLoss()\n","        writer = SummaryWriter(log_dir)  # for TensorBoard\n","        print('=> Starting training')\n","\n","        for epoch in range(1, self.num_epochs+1):\n","            input, target = self.get_random_batch()\n","            hidden, cell = self.rnn.init_hidden(self.batch_size)\n","            input = input.to(device)\n","            target = target.to(device)\n","            loss = 0\n","\n","            for i in range(self.chunk_len):\n","                output, (hidden, cell) = self.rnn(input[:, i], hidden, cell)\n","                loss += criterion(output, target[:, i])\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            loss = loss.item() / self.chunk_len\n","\n","            if epoch % self.print_every == 0:\n","                print(f'loss: {loss}')\n","                print(self.generate())\n","\n","            writer.add_scalar('Training loss', loss, global_step=epoch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z7PSzFAQCZal","executionInfo":{"status":"ok","timestamp":1616501309615,"user_tz":-180,"elapsed":267267,"user":{"displayName":"Max Planck","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBGeFwU-LUM8Wyn9zmAOd8U2BBj-wFfVDmw-TU=s64","userId":"06869868537886587167"}},"outputId":"76eb0835-6abf-418d-9a75-4399748d2901"},"source":["gennames = Generator()\n","gennames.train()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["=> Starting training\n","loss: 1.9460123291015625\n","Abdra\n","Brastin\n","Decina\n","Doma\n","Tiell\n","Elis\n","Edwend\n","Leara\n","Alia\n","Olonte\n","Reanna\n","Denala\n","Mimmy\n","Avenney\n","Bronnith\n","Res\n","loss: 1.6196993408203124\n","Ablece\n","Derine\n","Catheris\n","Sherice\n","Walen\n","Denabette\n","Bernardine\n","Takalie\n","Deannol\n","Nicholas\n","Jeona\n","Wolla\n","Shanna\n","\n","loss: 1.45254443359375\n","Abralian\n","Brendan\n","Grewvin\n","Dony\n","Derry\n","Candyn\n","Jean\n","Joshan\n","Junilyn\n","Julio\n","Tony\n","Tommy\n","Matmy\n","Rita\n","Terrence\n","Em\n","loss: 1.332224609375\n","Abel\n","Angela\n","Aranna\n","Austina\n","Elma\n","Cemarline\n","Denise\n","Dyanna\n","Amarie\n","Alexandra\n","Eleza\n","Florah\n","Francince\n","Gabrie\n","loss: 1.189349853515625\n","Abriella\n","Ann\n","Oliver\n","Angelica\n","Abriana\n","Auzie\n","Anahi\n","Baby\n","Elisa\n","Carlie\n","Carolyn\n","Joel\n","Kristina\n","Lakea\n","Lucy\n","Ro\n","loss: 1.1640946044921876\n","Abra\n","Caroline\n","Christen\n","Crystal\n","Darin\n","Danielle\n","Dornes\n","Ellino\n","Erin\n","Hannah\n","Jessie\n","Josephine\n","Kathy\n","Leonar\n","\n","loss: 1.170986083984375\n","Abeth\n","Alexandra\n","Aurelia\n","Beatrice\n","Brea\n","Brittney\n","Carli\n","Carolyne\n","Cherie\n","Christin\n","Christina\n","Charlotte\n","Cori\n","loss: 1.1580269775390626\n","Abraley\n","Adelia\n","Alisha\n","Alan\n","Aleen\n","Avry\n","Allison\n","Ariza\n","Ashley\n","Aniyah\n","Carrie\n","Channella\n","Cheryl\n","Cathyne\n","Char\n","loss: 1.0960843505859375\n","Abrahary\n","Aniyah\n","Ariel\n","Atonya\n","Autumn\n","Ariana\n","Alexis\n","Barbara\n","Brandi\n","Brenda\n","Cassandra\n","Camille\n","Cherie\n","Clair\n","loss: 1.109453125\n","Abney\n","Aalyanna\n","Annette\n","Biana\n","Briana\n","Catrina\n","Cari\n","Charlene\n","Claudia\n","Della\n","Elizabeth\n","Melida\n","Philip\n","Helen\n","\n","loss: 1.084455078125\n","Abrielle\n","Aimee\n","Ariya\n","Aiquito\n","Alyssa\n","Ariel\n","Annabelle\n","Beatrice\n","Brenda\n","Brenna\n","Caley\n","Cariel\n","Chlarissa\n","Char\n","loss: 1.079702880859375\n","Abriana\n","Alice\n","Ashton\n","Aniyah\n","Carl\n","Carmen\n","Cathren\n","Cristy\n","Elisabeth\n","Erica\n","Felisha\n","Geneva\n","Gina\n","Gracie\n","Gret\n","loss: 1.0671309814453125\n","Abed\n","Aiden\n","Adrean\n","Zadeem\n","Alex\n","Agustin\n","Alvin\n","Bryn\n","Brigen\n","Chandler\n","Charlee\n","Caseb\n","Clark\n","Conrad\n","Danny\n","Dall\n","loss: 1.028200927734375\n","Abiana\n","Anessa\n","Antwania\n","Aria\n","Brianne\n","Debra\n","Dee\n","Delilah\n","Estefany\n","Jackie\n","Janene\n","Mantie\n","Mandy\n","Madalyn\n","Morg\n","loss: 1.0251671142578125\n","Abrielle\n","Amiya\n","Anissa\n","Anney\n","Ariyah\n","Alexa\n","Alejandree\n","Alejandra\n","Ambar\n","Anne\n","Ansley\n","Arlene\n","Aubrey\n","Bernadet\n","loss: 1.0393392333984375\n","Abilea\n","Alina\n","Aurora\n","Betty\n","Belinda\n","Bristol\n","Bessie\n","Clara\n","Corrine\n","Clariss\n","Connor\n","Deanne\n","Denise\n","Destineig\n","\n","loss: 0.9974932250976563\n","Abraham\n","Andersus\n","Benig\n","Brayan\n","Brayan\n","Brodie\n","Carlie\n","Cleif\n","Dane\n","Damison\n","Fernando\n","Griffin\n","Fred\n","Gene\n","Heder\n","loss: 0.9928871459960937\n","Abdie\n","Asia\n","Aubrienne\n","April\n","Ashlynn\n","Avyna\n","Ava\n","Bernadette\n","Betty\n","Bobbie\n","Brianne\n","Brianna\n","Blanche\n","Camille\n","C\n","loss: 1.0363822021484375\n","Abley\n","Alexis\n","Annalisa\n","Alyssa\n","Ann\n","Anette\n","Antoinette\n","Alli\n","Allycia\n","Arielle\n","Arlene\n","Arahis\n","Ann\n","Ariyah\n","Ashle\n","loss: 1.0143198852539062\n","Abebeah\n","Ayana\n","Alison\n","Amalia\n","Arlette\n","Aura\n","Artery\n","Armida\n","Bonnie\n","Briana\n","Brianna\n","Brianne\n","Brissa\n","Brenda\n","Cas\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VcSsLzyJElYd"},"source":[""],"execution_count":null,"outputs":[]}]}