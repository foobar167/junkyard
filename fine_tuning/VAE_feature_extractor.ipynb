{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55aa3bf9-a9cc-4edf-ba09-74922eb12479",
   "metadata": {},
   "source": [
    "# VAE Feature extractor\n",
    "\n",
    "A [Variational Autoencoder (VAE)](https://en.wikipedia.org/wiki/Variational_autoencoder) is a deep learning model that functions as a generative model, capable of creating new data samples similar to its training data. It consists of an encoder that learns a probabilistic latent representation of the input data and a decoder that reconstructs data from these latent variables. Unlike traditional autoencoders, VAEs use [variational inference](https://towardsdatascience.com/variational-inference-the-basics-f70ac511bcea/) to generate continuous, probabilistic representations, allowing them to produce novel, realistic data variations and perform tasks like data imputation and anomaly detection. \n",
    "\n",
    "**ReadMe.md**\n",
    "\n",
    "This feature extractor is based on Variational AutoEncoder (VAE),\n",
    "trained on 60k real and 60k generated CT-slices.\n",
    "\n",
    "To launch the script, execute the following command:\n",
    "\n",
    "    python feature_extractor.py IMAGE_FOLDER_PATH SAVING_RESULTS_PATH [DEVICE_IDX] [BATCH SIZE]\n",
    "\n",
    "`DEVICE_IDX` is index of GPU to use.\n",
    "\n",
    "Devault values:\n",
    "\n",
    "    DEVICE_IDX = 0\n",
    "    BATCH SIZE = 64\n",
    "\n",
    "Example:\n",
    "\n",
    "    python feature_extractor.py media/data results\n",
    "    python feature_extractor.py media/data results 2 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2087cb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "\n",
    "num_workers = math.ceil(multiprocessing.cpu_count() * 2/3)  # use 2/3 of all CPU cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b17fa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put here the path to the folder with the images whose features you want to compute\n",
    "# IMG_FOLDER = \"./img_6K_CT_224_Gray_RG/test\"\n",
    "IMG_FOLDER = \"./img_6K_CT_224_Gray_RG/test\"\n",
    "\n",
    "# Path to the checkpoint with the trained model\n",
    "checkpoint = \"./outputs_VAE_real_generated/vae_epoch_600.pth\"\n",
    "\n",
    "SAVE_PATH = './outputs'\n",
    "batch_size = 64\n",
    "device_idx = 3\n",
    "device = f\"cuda:{device_idx}\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "latent_size = 2048\n",
    "img_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b60ed1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1,32, kernel_size=3, stride=1,padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(2),\n",
    "            nn.Conv2d(32,64, kernel_size=3, stride=1,padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(2),\n",
    "            nn.Conv2d(64,128,kernel_size=3, stride=1,padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(2),\n",
    "            nn.Conv2d(128,256,kernel_size=3, stride=1,padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(2),\n",
    "            nn.Conv2d(256,512,kernel_size=3, stride=1,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(2),\n",
    "            nn.Conv2d(512,1024,kernel_size=3, stride=1,padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.encoder_linear = nn.Linear(1024*7*7, latent_size*2)\n",
    "        self.decoder_linear = nn.Linear(latent_size, 1024*7*7)\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.unflatten=nn.Unflatten(1,(1024,7,7))\n",
    "        self.relu=nn.ReLU()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(1024,512,kernel_size=3, stride=1,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor = 2),\n",
    "            nn.Conv2d(512,256,kernel_size=3, stride=1,padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor = 2),\n",
    "            nn.Conv2d(256,128,kernel_size=3, stride=1,padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor = 2),\n",
    "            nn.Conv2d(128,64,kernel_size=3, stride=1,padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor = 2),\n",
    "            nn.Conv2d(64,32,kernel_size=3, stride=1,padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor = 2),\n",
    "            nn.Conv2d(32,1,kernel_size=3, stride=1,padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x=self.flatten(x)\n",
    "        x=self.encoder_linear(x).view(x.shape[0],2,-1)\n",
    "        mu = x[:, 0, :]\n",
    "        logsigma = x[:, 1, :]\n",
    "        \n",
    "        return mu, logsigma#, idx\n",
    "    \n",
    "    def gaussian_sampler(self, mu, logsigma):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logsigma) # standard deviation\n",
    "            eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "            sample = mu + (eps * std) # sampling as if coming from the input space\n",
    "            return sample\n",
    "        else:\n",
    "            # на инференсе возвращаем не случайный вектор из нормального распределения, а центральный -- mu. \n",
    "            # на инференсе выход автоэнкодера должен быть детерминирован.\n",
    "            return mu\n",
    "    \n",
    "    def decode(self, z):\n",
    "        z=self.decoder_linear(z)\n",
    "        z=self.relu(z)\n",
    "        z=self.unflatten(z)\n",
    "        z = self.decoder(z)\n",
    "        reconstruction = torch.sigmoid(z)\n",
    "        return reconstruction\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logsigma=self.encode(x)\n",
    "        reconstruction = self.decode(self.gaussian_sampler(mu, logsigma))\n",
    "\n",
    "        return reconstruction, mu, logsigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d952bc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model():\n",
    "    model = VAE().to(device)\n",
    "    model.load_state_dict(torch.load(checkpoint, map_location=device))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daec1568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(img_size, batch_size):\n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.Resize(img_size,interpolation=transforms.InterpolationMode.LANCZOS),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    image_dataset = datasets.ImageFolder(IMG_FOLDER, data_transforms)\n",
    "    filenames = [path.split('\\\\')[-1] for path, label in image_dataset.imgs]\n",
    "    if not os.path.isdir(SAVE_PATH):\n",
    "        os.mkdir(SAVE_PATH)\n",
    "    with open(os.path.join(SAVE_PATH,'filenames.txt'), 'w') as f:\n",
    "        for e in filenames:\n",
    "            f.write(str(e)+'\\n')\n",
    "    batches = DataLoader(image_dataset, batch_size=batch_size, shuffle=False, num_workers = num_workers)\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c35aa081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_features(model, dataloader):\n",
    "    output_all = None\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch, _ in tqdm(dataloader):\n",
    "            # print(batch.shape)\n",
    "            batch = batch.to(device)\n",
    "            output,_ = model.encode(batch)\n",
    "            output = output.cpu().detach().numpy()\n",
    "            if output_all is None:\n",
    "                output_all = output\n",
    "            else:\n",
    "                output_all = np.concatenate((output_all, output))\n",
    "    return output_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23a9b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features(features):\n",
    "    os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "    np.savetxt(os.path.join(SAVE_PATH, 'features.txt'), features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c516041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(IMG_FOLDER = IMG_FOLDER, SAVE_PATH = SAVE_PATH, batch_size = batch_size):\n",
    "    model = prepare_model()\n",
    "    model = model.to(device)\n",
    "    print('Model prepared') \n",
    "    dataloader = prepare_dataloader(img_size, batch_size = batch_size)\n",
    "    print('Dataloader prepared')\n",
    "    print('Starting getting features...')\n",
    "    features = gen_features(model, dataloader)\n",
    "    print('Getting features done, saving...')\n",
    "    save_features(features)\n",
    "    print('Done')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ebbe39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prepared\n",
      "Dataloader prepared\n",
      "Starting getting features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:02<00:00, 15.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting features done, saving...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "features = get_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8c10d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2048)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac775725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
