{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49b1eae2-c2ae-43ab-9fc0-83d2f1ae6481",
   "metadata": {},
   "source": [
    "# Image Captioning Using ViT and GPT2 Transformers\n",
    "\n",
    "**NOTE1: This pipeline does not work.**\n",
    "\n",
    "**NOTE2: In this pipeline I get text subtitles which are not user friendly and I don't know why.**\n",
    "\n",
    "   * [Self-link](https://github.com/foobar167/junkyard/blob/master/fine_tuning/Image_Captioning_with_ViT_and_GPT2.ipynb)\n",
    "   * Completion time for this notebook is about **4:00 hours**. GPU: `GeForce GTX TITAN X`.\n",
    "\n",
    "Image captioning using Transformers is a technique that generates natural language descriptions of an image by combining vision Transformers (ViT) for image understanding and language Transformers for text generation.\n",
    "\n",
    "![Image Captioning Task Scheme](./data/Image_Captioning_scheme.png)\n",
    "\n",
    "This approach leverages the self-attention mechanism in Transformers to capture both global image context and linguistic structure, resulting in more accurate and coherent captions compared to older CNN-RNN methods. Examples include models like Vision-Encoder with Language-Decoder (VELD), BLIP, and Flamingo. It’s a powerful example of multimodal AIbridging vision and language through Transformer architectures.\n",
    "\n",
    "The [Vision Encoder Decoder Model](https://huggingface.co/docs/transformers/model_doc/vision-encoder-decoder) can be used to initialize an image-to-text model with any pre-trained Transformer-based vision model as the encoder (e.g. ViT, BEiT, DeiT, Swin) and any pre-trained language model as the decoder (e.g. RoBERTa, GPT2, BERT, DistilBERT).\n",
    "\n",
    "![Vision Encoder Decoder Architecture](./data/vision-encoder-decoder.png)\n",
    "\n",
    "The **Vision Transformer**, or ViT, is a model for image classification that employs a Transformer-like architecture over patches of the image. An image is split into fixed-size patches, each of them are then linearly embedded, position embeddings are added, and the resulting sequence of vectors is fed to a standard Transformer encoder. In order to perform classification, the standard approach of adding an extra learnable “classification token” to the sequence is used.\n",
    "\n",
    "The ViT was first introduced in paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/pdf/2010.11929.pdf). The ViT was created to perform similar tasks of an NLP based transformer. Instead of token for words, fixed sized patches are used in image that are subset of images.\n",
    "\n",
    "![ViT architecture](./data/ViT_architecture.png)\n",
    "\n",
    "The ViT model we are using has been pre-trained on the public ImageNet-21k dataset. The goal is to demonstrate **fine-tunning ViTs**, for generating image captions without the necessity of retraining from scratch.\n",
    "\n",
    "![ImageNet-21k dataset](./data/ImageNet-21k_dataset.png)\n",
    "\n",
    "The **GPT-2**, or **Generative Pre-trained Transformer 2** model pretrained on a very large corpus of English data in a self-supervised fashion. Inputs are sequences of continuous text of a certain length and the targets are the same sequence, shifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the predictions for the token i only uses the inputs from 1 to i but not the future tokens. This way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a prompt. Unlike models that use both encoder and decoder (like T5 or BERT), **GPT-2 uses only the decoder part of the original Transformer**, modified to process text autoregressively  meaning it predicts the next word based on previous words.\n",
    "\n",
    "[DistilGPT2](https://huggingface.co/distilbert/distilgpt2) (short for Distilled-GPT2) is an English-language model pre-trained with the supervision of the smallest version of Generative Pre-trained Transformer 2 (GPT-2). Like GPT-2, DistilGPT2 can be used to generate text.\n",
    "\n",
    "Links:   \n",
    "   * [Vision Encoder Decoder (ViT + GPT2) model that fine-tuned on flickr8k-dataset for image-to-text task](https://huggingface.co/atasoglu/vit-gpt2-flickr8k) - minimal implementation details. [My example is here](Image_Captioning_with_Min_Detailes.ipynb)\n",
    "   * [The Illustrated Image Captioning using transformers](https://ankur3107.github.io/blogs/the-illustrated-image-captioning-using-transformers/) - moderate implementation details\n",
    "   * [Image Caption Model from scratch  ViT+GPT](https://medium.com/@mr.sk12112002/image-caption-model-from-scratch-vit-gpt-94afaae30fb7) - much more detailed implementation of code blocks\n",
    "   * [Image Captioning : ViT + GPT2](https://www.kaggle.com/code/burhanuddinlatsaheb/image-captioning-vit-gpt2) (Kaggle) - additional info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad50b323-4b06-4d6f-9996-5b7e2428d4dd",
   "metadata": {},
   "source": [
    "# Helper Functions and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e5376e-f1c6-498c-aa24-bfbb5f1db3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set only one visible GPU. Turn off Huggingface trainer DataParallel\n",
    "GPU = 0  # GPU number. After change, restart the kernel\n",
    "\n",
    "%set_env CUDA_VISIBLE_DEVICES={GPU}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8a0d6e-022a-4e30-a12d-487dd35b0f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():  # make sure GPU is available\n",
    "    num = torch.cuda.device_count()\n",
    "    print(f\"GPU count: {num}\")\n",
    "    for i in range(num):\n",
    "        print(f\"GPU {i} name: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af69cb52-e243-494b-9f9c-eba9f9ac9751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc  # gabbage collector\n",
    "import nltk  # Natural Language Toolkit\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "import requests  # download images using URL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch libraries\n",
    "import torch\n",
    "import torchvision.transforms as t\n",
    "\n",
    "# Hugging Face libraries\n",
    "import datasets\n",
    "import evaluate\n",
    "import transformers as hf\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from itertools import islice\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\" Set random seed for reproducibility \"\"\"\n",
    "    random.seed(seed)\n",
    "    hf.set_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "\n",
    "\n",
    "def show_trainable_layers(model):\n",
    "    \"\"\" Show trainable layers of the model \"\"\"\n",
    "    train_params = 0  # number of trainable parameters\n",
    "    layers = []  # names of trainable layers\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:  # if trainable layer\n",
    "            train_params += param.numel()\n",
    "            layers.append(name)\n",
    "    print(f\"\\n\" f\"Number of trainable parameters = {train_params:,}\")\n",
    "    print(\"Trainable layers:\\n\")\n",
    "    print(*layers, sep=\"\\n\")\n",
    "\n",
    "\n",
    "set_seed()\n",
    "\n",
    "CPU = multiprocessing.cpu_count()\n",
    "NUM_WORKERS = math.ceil(CPU*2/3)\n",
    "print(f\"Number of CPU cores: {CPU}\")\n",
    "print(f\"Number of worker cores: {NUM_WORKERS}\")\n",
    "\n",
    "DEVICE = f\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "if \"cuda\" in DEVICE:\n",
    "    # Get the index of the current CUDA device\n",
    "    current_device_index = torch.cuda.current_device()\n",
    "    # Get the name of the current CUDA device\n",
    "    device_name = torch.cuda.get_device_name(current_device_index)\n",
    "    print(f\"Current CUDA device name: {device_name}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU\")\n",
    "\n",
    "# Vision Transformer (ViT) model pre-trained on ImageNet-21k\n",
    "# (14 million images, 21,843 classes) at resolution 224x224.\n",
    "ENCODER = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "# DistilGPT2 (short for Distilled-GPT2) is an English-language model pre-trained with\n",
    "# the supervision of the smallest version of Generative Pre-trained Transformer 2 (GPT-2).\n",
    "DECODER = \"distilgpt2\"\n",
    "\n",
    "# Checking different options:\n",
    "\n",
    "# * Option 1. Freeze all encoder layers\n",
    "# * Option 2. Freeze all encoder layers except the last one\n",
    "# * Option 3. Unfreeze all layers of the model\n",
    "# * Option 4. Unfreeze all layers of the model. Make one caption from 5 different captions\n",
    "# * Option 5. The same as option 3, but reduce MAX_TOKENS from 50 to 20\n",
    "# * Option 6. The same as option 4, but increase MAX_TOKENS from 50 to 132\n",
    "#    * 6.1. Static augmentation (augment 1 time at the beginning)\n",
    "#    * 6.2. Without augmentation\n",
    "#    * 6.3. Augmentation on-the-fly (dynamic augmentation)\n",
    "#    * 6.4. ROUGE-L F1 score instead of default cross-entropy loss\n",
    "# * Option 7. A model fine-tuned on the COCO dataset is used.\n",
    "#             Unfreeze only last layer of encoder and last layer of decoder.\n",
    "#             The task with image captions has already been solved here!\n",
    "# Option 8. The same as 6.4, but with texts augmentation.\n",
    "\n",
    "OPTION = 7  # can be from 1 to 8\n",
    "TEMP = \"temp\"  # path to temporal directory with data\n",
    "RESULTS = f\"{TEMP}/results-{OPTION}\"  # directory with checkpoints\n",
    "TRAIN_CSV, VALID_CSV = f\"{TEMP}/captions.csv\", f\"{TEMP}/captions_val.csv\"\n",
    "DS_NAME = f\"{TEMP}/flickr8k_huggingface_dataset\"\n",
    "\n",
    "if OPTION == 7:\n",
    "    # Huggingface link: https://huggingface.co/nlpconnect/vit-gpt2-image-captioning\n",
    "    # Fine-tuned on the COCO dataset model\n",
    "    # model_name = \"nlpconnect/vit-gpt2-image-captioning\"  # this model requires PyTorch >= 2.6\n",
    "    model_name = \"ashok2216/vit-gpt2-image-captioning_COCO_FineTuned\"  # old PyTorch version is OK\n",
    "\n",
    "os.makedirs(TEMP, exist_ok=True)  # create temporal directory\n",
    "print(f\"Checking option: {OPTION}\")\n",
    "print(f\"Save model checkpoints to '{RESULTS}'\")\n",
    "\n",
    "NUM_VAL = 500  # number of validation images in evaluation dataset\n",
    "MIN_CAPTION_LENGTH = 2  # minimum caption length in chars\n",
    "\n",
    "# In Flickr8k dataset the longest caption has length 199\n",
    "# MAX_CAPTION_LENGTH = 199  # not in use\n",
    "\n",
    "# Article: What are tokens and how to count them\n",
    "# URL: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "if OPTION == 5:\n",
    "    MAX_TOKENS = 20  # 1 token ≈ 4 chars (approximately)\n",
    "if OPTION in (6, 7, 8, ):\n",
    "    MAX_TOKENS = 132  # the longest caption has length 527 chars ≈ 132 tokens\n",
    "else:\n",
    "    MAX_TOKENS = 50  # 50 tokens ≈ 200 chars (approximately)\n",
    "\n",
    "EPOCHS = 5  # 3\n",
    "\n",
    "if OPTION in (1, 2, ):\n",
    "    BATCH_SIZE = 100\n",
    "elif OPTION in (3, 4, 5, ):\n",
    "    BATCH_SIZE = 40\n",
    "elif OPTION in (6, 8, ):\n",
    "    BATCH_SIZE = 25  # MAX_TOKENS require more memory\n",
    "elif OPTION in (7, ):\n",
    "    BATCH_SIZE = 15\n",
    "else:\n",
    "    BATCH_SIZE = 25\n",
    "\n",
    "# To remove tokenizers directory\n",
    "# cd ~/nltk_data/tokenizers  # navigate to the tokenizers directory\n",
    "# rm -rf punkt.zip punkt  # remove the punkt.zip file and directory\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except (LookupError, OSError):\n",
    "    nltk.download(\"punkt\", quiet=True)\n",
    "    nltk.download(\"punkt_tab\", quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df08b0c-b73a-4769-a858-9cc9ae4d1573",
   "metadata": {},
   "source": [
    "# A Quick Guide to Tokenization\n",
    "\n",
    "[Demystifying Tokenization: Preparing Data for Large Language Models (LLMs)](https://www.linkedin.com/pulse/demystifying-tokenization-preparing-data-large-models-rany-2nebc).\n",
    "\n",
    "[How tokenizers work in AI models: A beginner-friendly guide](https://nebius.com/blog/posts/how-tokenizers-work-in-ai-models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040b5fcd-7bb5-4f4b-b719-e529ac0fca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, try \"openai-community/gpt2\"\n",
    "tokenizer = hf.GPT2Tokenizer.from_pretrained(DECODER)\n",
    "\n",
    "text = \"Hello World! This is a test message.\"\n",
    "\n",
    "tokenized_text = tokenizer(text)\n",
    "encoded_text = tokenized_text[\"input_ids\"]\n",
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "\n",
    "print(tokenized_text)\n",
    "print(f\"Encoded text: {encoded_text}\\n\")\n",
    "print(f\"Decoded tokens back into text:\\n\\t{decoded_text}\\n\")\n",
    "\n",
    "list_texts = [\"Hi, how are you?\", \"I'm good\", \"Yes\"]\n",
    "encoded_texts = tokenizer(list_texts)[\"input_ids\"]\n",
    "print(f\"Encoded several texts: {encoded_texts}\\n\")\n",
    "\n",
    "# This line of code sets the tokenizer's pad token to be the same as its EOS token.\n",
    "# This means that the same token will be used both to pad shorter sequences\n",
    "# and to signify the end of a sequence.\n",
    "# GPT2 only has bos/eos tokens but not decoder_start/pad tokens\n",
    "tokenizer.pad_token = tokenizer.eos_token  # pad with EOS token\n",
    "\n",
    "encoded_texts_longest = tokenizer(list_texts, padding=True)[\"input_ids\"]\n",
    "print(f\"Using padding: {encoded_texts_longest}\\n\")\n",
    "\n",
    "encoded_texts_truncation = tokenizer(\n",
    "    list_texts, max_length=3, truncation=True)[\"input_ids\"]\n",
    "print(f\"Using truncation: {encoded_texts_truncation}\\n\")\n",
    "\n",
    "tokenizer.truncation_side = \"left\"  # truncate left side\n",
    "encoded_texts_truncation_left = tokenizer(\n",
    "    list_texts, max_length=3, truncation=True)[\"input_ids\"]\n",
    "print(f\"Using left-side truncation: {encoded_texts_truncation_left}\\n\")\n",
    "\n",
    "tokenizer.truncation_side = \"right\"  # truncate right side\n",
    "truncation_padding = tokenizer(\n",
    "    list_texts, max_length=3, truncation=True, padding=True)[\"input_ids\"]\n",
    "print(f\"Using both truncation and padding: {truncation_padding}\\n\")\n",
    "\n",
    "decoded_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in encoded_texts]\n",
    "print(f\"Decoded texts: {decoded_texts}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47343528-71cd-4464-8c08-dc094325563e",
   "metadata": {},
   "source": [
    "# Loading and Processing the Dataset\n",
    "\n",
    "[Flickr 8k Dataset](https://www.kaggle.com/datasets/adityajn105/flickr8k) is a new benchmark collection for sentence-based image description and search, connsisting of 8,091 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. The images were chosen from six different Flickr groups, and tend not to contain any well-known people or locations, but were manually selected to depict a variety of scenes and situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5415257e-e791-48bd-ac78-8a2152996fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and unzip Flickr8k dataset\n",
    "if os.path.exists(f\"{TEMP}/flickr8k\"):\n",
    "    pass  # already downloaded\n",
    "else:\n",
    "    URL = \"https://github.com/awsaf49/flickr-dataset/releases/download/v1.0/flickr8k.zip\"\n",
    "    !wget -P \"{TEMP}\" -N \"{URL}\"\n",
    "    !unzip -q \"{TEMP}/flickr8k.zip\" -d \"{TEMP}/flickr8k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ae18be-b001-46d9-bbb2-9c7d681967aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{TEMP}/flickr8k/captions.txt\")\n",
    "display(df.head())  # show first rows\n",
    "print()\n",
    "for row in islice(df.itertuples(), 12):  # print first 12 rows\n",
    "    print(row.image, row.caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1903c554-28af-4260-81cb-ccaba24990cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(captions, num, start=0):\n",
    "    \"\"\"\n",
    "        Show first `num` key-value pairs in captions.\n",
    "    \"\"\"\n",
    "    print(f\"Show {num} examples from {start} to {start + num}:\")\n",
    "    # islice(iterable, start, stop, step)\n",
    "    iterator = iter(captions.items())\n",
    "    key_value = islice(iterator, start, start + num)\n",
    "    for key, value in key_value:\n",
    "        print(f\"{key}\\t\", *value, sep=\"\\n\\t\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def histogram(chars, bins=10):\n",
    "    \"\"\" Create a histogram from a list of integers. \"\"\"\n",
    "    plt.hist(chars, bins=bins)  # create the histogram\n",
    "    # Add labels and a title for clarity\n",
    "    plt.xlabel(\"Number of characters in a caption\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Histogram of Captions Length\")\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "image_path = f\"{TEMP}/flickr8k/Images\"\n",
    "captions, captions_val = {}, {}  # training and validation captions\n",
    "\n",
    "# for row in islice(df.itertuples(), 500):  # test first 500 rows for 100 images\n",
    "for row in df.itertuples():\n",
    "    image_name = os.path.join(image_path, row.image)\n",
    "    caption = row.caption  # copy string\n",
    "\n",
    "    if len(caption) < MIN_CAPTION_LENGTH:\n",
    "        print(f'Skip caption \"{caption}\" for \"{image_name}\"')\n",
    "        continue  # skip this caption\n",
    "\n",
    "    # I don't know why, but from GPT-2 all letters are in lowercase\n",
    "    # caption = caption[0].upper() + caption[1:]  # capitalize the 1st letter\n",
    "    caption = caption[0].lower() + caption[1:]  # lowercase the 1st letter\n",
    "\n",
    "    if caption.endswith((\" .\", \" !\", \" ?\", ' \"', )):\n",
    "        pass  # do nothing\n",
    "    elif caption.endswith((\" ,\", \" '\", )):\n",
    "        caption = caption[:-2]  # remove the final comma or apostrophe from a caption\n",
    "        caption = caption + \" .\"  # add the final period to a caption\n",
    "    else:\n",
    "        caption = caption + \" .\"  # add the final period to a caption\n",
    "    \n",
    "    if OPTION in (4, 6, 7, 8, ):  # make 1 caption from 5 different captions for each image\n",
    "        if image_name in captions:\n",
    "            s = captions[image_name][0]  # get previous caption\n",
    "            s += \" \" + caption  # extend previous caption\n",
    "            captions[image_name] = [s]  # save extended caption\n",
    "        else:\n",
    "            captions[image_name] = [caption]\n",
    "    else:  # there are 5 different captions for each image\n",
    "        if image_name in captions:\n",
    "            captions[image_name].append(caption)\n",
    "        else:\n",
    "            captions[image_name] = [caption]\n",
    "\n",
    "shortest, longest = None, None  # shortest and longest captions\n",
    "shortest_len, longest_len = float(\"inf\"), 0\n",
    "shortest_name, longest_name = None, None\n",
    "chars = []  # list of captions length\n",
    "\n",
    "for key, value in captions.items():  # iterate over key-value pairs\n",
    "    for caption in value:\n",
    "        caption_len = len(caption)\n",
    "        chars.append(caption_len)\n",
    "        if caption_len < shortest_len:\n",
    "            shortest, shortest_len, shortest_name = caption, caption_len, key\n",
    "        if caption_len > longest_len:\n",
    "            longest, longest_len, longest_name = caption, caption_len, key\n",
    "\n",
    "bins = longest_len - shortest_len + 1\n",
    "histogram(chars, bins=bins)  # draw histogram of captions length\n",
    "\n",
    "print(f\"\\n\" f\"There are {len(captions)} images.\\n\")\n",
    "print(f\"Shortest caption has length {shortest_len} for image '{shortest_name}':\"\n",
    "      f\"\\n\\t\" f\"{shortest}\\n\")\n",
    "print(f\"Longest caption has length {longest_len} for '{longest_name}':\"\n",
    "      f\"\\n\\t\" f\"{longest}\\n\")\n",
    "\n",
    "for i in range(min(NUM_VAL, len(captions))):\n",
    "    key, value = captions.popitem()  # removes and returns the last key-value pair\n",
    "    captions_val[key] = value  # validation captions\n",
    "\n",
    "print(f\"There are {len(captions)} training images.\")\n",
    "print(f\"There are {len(captions_val)} validation images.\\n\")\n",
    "\n",
    "show(captions, 2)\n",
    "show(captions_val, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7830a3-d636-44c4-b654-6837246ffbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debug purposes. Making small dataset\n",
    "captions = dict(islice(captions.items(), 300))\n",
    "captions_val = dict(islice(captions_val.items(), 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef4731c-44ad-4197-9aef-7b7872b7ef77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(captions, csv_file):\n",
    "    \"\"\"\n",
    "        Convert dictionary back to CSV file and then to Huggingface dataset.\n",
    "        It is OK to have multiple captions per image becuase of data augmentation.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for key, value in captions.items():\n",
    "        for caption in value:\n",
    "            rows.append({\"image\": key, \"caption\": caption})\n",
    "\n",
    "    # There will be no memory overflow if you save the DataFrame to a CSV file\n",
    "    #   and then create a dataset from the CSV.\n",
    "    pd.DataFrame(rows).to_csv(csv_file, index=False)  # save dictionary to CSV file\n",
    "    ds = datasets.Dataset.from_csv(csv_file)  # convert CSV file to Huggingface Dataset\n",
    "\n",
    "    # # BUG! If you run these two lines of code,\n",
    "    # #      a memory overflow will occur in Dataset.map function later\n",
    "    # df = pd.DataFrame(rows)  # from dictionary to Pandas DataFrame\n",
    "    # ds = datasets.Dataset.from_pandas(df)  # from Pandas Dataframe to Huggingface Dataset\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "train_ds = get_dataset(captions, TRAIN_CSV)\n",
    "valid_ds = get_dataset(captions_val, VALID_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f069b29f-1655-45e7-9b67-f176568f1373",
   "metadata": {},
   "source": [
    "# Initialize Feature Extractor and Tokenizer\n",
    "\n",
    "We need image **feature extractor** to convert images to a tensor.\n",
    "\n",
    "We need text **tokenizer** to convert text to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31b9380-fbe7-4faa-bb9c-fcd78ead8223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature extractor\n",
    "feature_extractor = hf.AutoImageProcessor.from_pretrained(ENCODER, use_fast=True)\n",
    "# feature_extractor = hf.AutoFeatureExtractor.from_pretrained(ENCODER, use_fast=True)\n",
    "\n",
    "# Instantiate a tokenizer\n",
    "tokenizer = hf.GPT2TokenizerFast.from_pretrained(DECODER)\n",
    "# tokenizer = hf.AutoTokenizer.from_pretrained(DECODER)\n",
    "\n",
    "# GPT2 only has bos/eos tokens but not decoder_start/pad tokens\n",
    "tokenizer.pad_token = tokenizer.eos_token  # pad with EOS token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff2256a-de0f-40da-b95c-dff569b5acbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a344d1ea-feec-48d9-9c38-435e6ffa522e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491821f5-e3eb-435c-b23e-a9fc65edb5e8",
   "metadata": {},
   "source": [
    "Feature extraction works as tokenizer in NLP. It takes raw string and turn them in tokens. Feature extraction takes a raw image and perform some pre-processing to convert it into tensor. Some parameters of feature extraction are `do_normalize`, which is a normalization to certain mean and standard deviation and convert image to size of 224*224."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc7efb4-c53c-45fd-b3f9-3bfde7db6ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = next(iter(train_ds))\n",
    "print(first[\"caption\"])\n",
    "img = Image.open(first[\"image\"])  # use Pillow's Image object\n",
    "display(img)\n",
    "\n",
    "image = feature_extractor(img).pixel_values[0]\n",
    "print(image.shape)  # torch size [3, 224, 224] or [channels, height, width]\n",
    "\n",
    "image = image.permute(1, 2, 0)  # [3,224,224] to [224,224,3]\n",
    "print(image.shape)\n",
    "\n",
    "# This image has range [-0.937, 1.0]. Normalize it to the range [0.0, 1.0]\n",
    "image = (image - image.min()) / (image.max() - image.min())\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ab634e-b3dd-4384-bab7-79efe6746464",
   "metadata": {},
   "source": [
    "# Creating Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40e8177-0b61-42ee-bfc3-5ab42f952265",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = t.Normalize(\n",
    "    mean=feature_extractor.image_mean,\n",
    "    std=feature_extractor.image_std\n",
    ")\n",
    "\n",
    "size = list(feature_extractor.size.values())  # image size\n",
    "interpolation = t.InterpolationMode.BICUBIC  # for most functions default is BILINEAR\n",
    "\n",
    "# It is generally NOT recommended to use data augmentation on the validation set\n",
    "process = t.Compose([\n",
    "    t.Resize(size, interpolation=interpolation),\n",
    "    t.ToTensor(),  # convert to pytorch tensor\n",
    "    normalize,  # normalize pixel values\n",
    "])\n",
    "\n",
    "\n",
    "def random_color():\n",
    "    \"\"\" Generate a random fill color (e.g., RGB list) \"\"\"\n",
    "    return torch.randint(low=0, high=256, size=(3,), dtype=torch.uint8).tolist()\n",
    "\n",
    "\n",
    "class random_rotation_with_random_fill:\n",
    "    \"\"\" Fill empty areas with RANDOM RGB color \"\"\"\n",
    "    def __call__(self, img):\n",
    "        fill = random_color()\n",
    "        return t.RandomRotation(degrees=8,  # (-degrees, +degrees)\n",
    "                                interpolation=interpolation,\n",
    "                                fill=fill,\n",
    "                               )(img)\n",
    "\n",
    "\n",
    "class random_affine_with_random_fill:\n",
    "    \"\"\"\n",
    "      Define the RandomAffine transform\n",
    "        degrees - range of degrees for rotation (-8 to +8)\n",
    "        translate - maximum absolute fraction of the image width/height\n",
    "                    for translation (e.g., 0.05 means up to 5% shift)\n",
    "        scale - scaling factor interval (e.g., 0.9 to 1.1 means scaling between 90% and 110%)\n",
    "        shear - range of degrees for shear transformation (e.g., -5 to +5 along x-axis)\n",
    "        fill - fill empty areas with RANDOM RGB color\n",
    "    \"\"\"\n",
    "    def __call__(self, img):\n",
    "        fill = random_color()\n",
    "        return t.RandomAffine(degrees=8,\n",
    "                              translate=(0.05, 0.05),\n",
    "                              scale=(0.9, 1.1),\n",
    "                              shear=5,\n",
    "                              # No support for BICUBIC interpolation. Use BILINEAR\n",
    "                              interpolation=t.InterpolationMode.BILINEAR,\n",
    "                              fill=fill,\n",
    "                             )(img)\n",
    "\n",
    "\n",
    "class random_perspective_with_random_fill:\n",
    "    \"\"\"\n",
    "      Define the RandomPerspective transform\n",
    "        distortion_scale: controls the degree of distortion (0 to 1)\n",
    "        p is the probability of applying the transform (0 to 1)\n",
    "    \"\"\"\n",
    "    def __call__(self, img):\n",
    "        fill = random_color()\n",
    "        return t.RandomPerspective(distortion_scale=0.1,  # small distortion\n",
    "                                   p=0.5,  # probability == 1/2\n",
    "                                   interpolation=interpolation,\n",
    "                                   fill=fill,\n",
    "                                  )(img)\n",
    "\n",
    "\n",
    "transform = t.Compose([\n",
    "    lambda image: image.convert(\"RGB\"),  # ensure the image is in RGB format\n",
    "    random_affine_with_random_fill(),\n",
    "    random_perspective_with_random_fill(),\n",
    "    random_rotation_with_random_fill(),\n",
    "    t.RandomHorizontalFlip(),  # flip horizontally with probability 0.5\n",
    "    \n",
    "    # Define the ColorJitter transform with small adjustments\n",
    "    #   brightness: factor from [1-0.1, 1+0.1] = [0.9, 1.1]\n",
    "    #   contrast: factor from [1-0.1, 1+0.1] = [0.9, 1.1]\n",
    "    #   saturation: factor from [1-0.1, 1+0.1] = [0.9, 1.1]\n",
    "    #   hue: factor from [-0.05, 0.05]\n",
    "    t.ColorJitter(brightness=0.1,\n",
    "                  contrast=0.1,\n",
    "                  saturation=0.1,\n",
    "                  hue=0.05,\n",
    "                 ),\n",
    "])\n",
    "\n",
    "augment = t.Compose([\n",
    "    transform,  # image transformations\n",
    "    process,    # resize, convert to tensor and normalize\n",
    "])\n",
    "\n",
    "\n",
    "# Create a function that applies the transforms to a batch of examples\n",
    "# The function should take a dictionary of examples and return a dictionary\n",
    "# with the transformed data.\n",
    "def dynamic_augment(batch):\n",
    "    # ViT expects `pixel_values` instead of `input_ids`\n",
    "    batch[\"pixel_values\"] = [augment(Image.open(path)) for path in batch[\"image\"]]\n",
    "    return batch\n",
    "\n",
    "\n",
    "def image_preprocess(batch, process):\n",
    "    # ViT expects `pixel_values` instead of `input_ids`\n",
    "    batch[\"pixel_values\"] = [process(Image.open(path)) for path in batch[\"image\"]]\n",
    "\n",
    "    # We are padding tokens here instead of using a datacollator\n",
    "    tokenized = tokenizer(batch[\"caption\"], padding=\"max_length\",\n",
    "                          max_length=MAX_TOKENS,\n",
    "                          truncation=True)[\"input_ids\"]\n",
    "\n",
    "    # Important: make sure that PAD tokens are ignored by the loss function\n",
    "    labels = []\n",
    "    for token_sequence in tokenized:\n",
    "        new_label_sequence = []\n",
    "        for label_id in token_sequence:\n",
    "            if label_id == tokenizer.pad_token_id:\n",
    "                new_label_sequence.append(-100)\n",
    "            else:\n",
    "                new_label_sequence.append(label_id)\n",
    "        labels.append(new_label_sequence)\n",
    "\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "# It is better to use dynamic augmentation on-the-fly. Use dynamice augmentation later\n",
    "# image_preprocess_train = partial(image_preprocess, process=augment)  # use static augment\n",
    "\n",
    "image_preprocess_train = partial(image_preprocess, process=process)  # without static augment\n",
    "image_preprocess_valid = partial(image_preprocess, process=process)\n",
    "\n",
    "# Get current number of threads. PyTorch uses the number of\n",
    "# physical CPU cores as the default number of threads\n",
    "num_threads = torch.get_num_threads()  # save default number of threads\n",
    "print(f\"Current number of PyTorch threads: {num_threads}\")\n",
    "\n",
    "# BUG! Using num_proc>1 in Dataset.map hangs\n",
    "#      unless this parameter is set:\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "train_ds = train_ds.map(image_preprocess_train, batched=True, num_proc=NUM_WORKERS)\n",
    "# Don't delete \"image\" and \"caption\" columns for dynamic augmentation\n",
    "# train_ds = train_ds.remove_columns([\"image\", \"caption\"])\n",
    "\n",
    "valid_ds = valid_ds.map(image_preprocess_valid, batched=True, num_proc=NUM_WORKERS)\n",
    "# Don't delete \"image\" and \"caption\" columns for dynamic augmentation\n",
    "# valid_ds = valid_ds.remove_columns([\"image\", \"caption\"])\n",
    "\n",
    "# Restore default number of threads\n",
    "torch.set_num_threads(num_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a84f57-aa14-471f-b801-e45d7fd1d5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for convenience from training and validation parts\n",
    "ds = datasets.DatasetDict({\"train\": train_ds, \"valid\": valid_ds,})\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c52ab76-d5f6-4b4c-9a57-aa685c782574",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(DS_NAME):  # delete previous dataset\n",
    "    shutil.rmtree(DS_NAME, ignore_errors=False, onerror=None)\n",
    "\n",
    "# dataset size == 23 GB and you can save it locally\n",
    "ds.save_to_disk(DS_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03422080-1d22-4c36-b0c0-bd07d0268dca",
   "metadata": {},
   "source": [
    "**Always look at you data!**\n",
    "\n",
    "Wrong image augmentation with [`RandomResizedCrop`](https://docs.pytorch.org/vision/0.9/transforms.html#torchvision.transforms.RandomResizedCrop) from [this example](https://mehdirezvandehy.github.io/Image-Captioning-by-Fine-tunning-ViT-model/Image_Caption_ViT.html#Image-Processing):\n",
    "\n",
    "![Wrong image augmentation example with RandomResizedCrop](./data/wrong_image_augmentation.png)\n",
    "\n",
    "I down't use `RandomResizedCrop`, because I don't understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a5293e-a6f5-4f01-9e4e-bbdeacb9a33b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds = datasets.load_from_disk(DS_NAME)  # load dataset from disk\n",
    "print(ds)\n",
    "\n",
    "# Apply dynamic augmentation \"on the fly\"\n",
    "# set_transform modifies a dataset in-place\n",
    "# with_transform returns a new dataset \n",
    "ds[\"train\"].set_transform(dynamic_augment)  # check augmentation in-place\n",
    "# ds[\"train\"].with_transform(dynamic_augment)\n",
    "# ds.set_format('torch')\n",
    "\n",
    "# Access and display the first few images\n",
    "num_samples_to_display = 15  # 3 * 5 = 15 images\n",
    "rows = 1 + (num_samples_to_display - 1) // 5\n",
    "cols = 5\n",
    "\n",
    "plt.figure(figsize=(15, 9))\n",
    "for i in range(num_samples_to_display):\n",
    "    image = ds[\"train\"][i][\"pixel_values\"]\n",
    "    image = np.array(image)  # convert list to NumPy\n",
    "    image = image.transpose(1, 2, 0)  # [3,224,224] to [224,224,3]\n",
    "    image = (image - image.min()) / (image.max() - image.min())  # [-1,1] to [0,1]\n",
    "\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a0cf53-8735-40de-8281-13119008b460",
   "metadata": {},
   "source": [
    "# Obtaining a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfed0cf-ecef-4686-b88f-8fd12d9799a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to free GPU memory. This doesn't always work\n",
    "if \"model\" in locals():\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "try:  # trying to load a model from a checkpoint\n",
    "    model = hf.VisionEncoderDecoderModel.from_pretrained(RESULTS).to(DEVICE)\n",
    "except Exception as e:\n",
    "    name = type(e).__name__  # error name\n",
    "    print(\"Downloading model from the Hugging Face repository\")\n",
    "\n",
    "    if OPTION == 7:  # fine-tuned on the COCO dataset model\n",
    "        model = hf.VisionEncoderDecoderModel.from_pretrained(\n",
    "            model_name,\n",
    "            # use_safetensors=True,  # doesn't work\n",
    "        )\n",
    "    else:\n",
    "        # Many weights are initialized randomly, namely the cross attention weights\n",
    "        model = hf.VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "            ENCODER, DECODER)\n",
    "\n",
    "    # Not necessary. Huggingface automatically detect and utilize all GPUs in DataParallel\n",
    "    # To disable the following warning use `.to(DEVICE)` for your model and example images:\n",
    "    #   There were missing keys in the checkpoint model loaded: ['decoder.lm_head.weight'].\n",
    "    model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aff61f5-4b07-43e8-844e-da225fa46224",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show some technical information about encoder and decoder neural networks\n",
    "display(type(model.encoder))\n",
    "display(type(model.decoder))\n",
    "print(f\"\\n{'-'*32}\\n\")  # print delimiter\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c312290a-d737-483a-9505-c64542a385f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count total number of parameters in the model\n",
    "total_params = 0\n",
    "for param in model.parameters():\n",
    "    total_params += param.numel()\n",
    "print(f\"The model has {total_params:,} parameters\")\n",
    "\n",
    "# Get the number of hidden layers in encoder (ViT) Transforver\n",
    "num_hidden_layers = model.config.encoder.num_hidden_layers\n",
    "print(f\"The encoder Transformer has {num_hidden_layers} hidden layers\")\n",
    "\n",
    "# Get the number of hidden layers in decoder (Distilled-GPT2) Transforver\n",
    "num_hidden_layers = model.config.decoder.num_hidden_layers\n",
    "print(f\"The decoder Transformer has {num_hidden_layers} hidden layers\\n\\n\")\n",
    "\n",
    "# Print all layers of the model\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4af37be-b512-453f-869e-d7a070359fdc",
   "metadata": {},
   "source": [
    "# Preparing the Model for Training\n",
    "\n",
    "Typically, **none of the decoder Transformer layers should be frozen** especially in generative tasks like text generation, image captioning, or machine translation.\n",
    "\n",
    "There is no fixed number of encoder Transformer layers to freeze. It depends on the task, dataset size, and training goals. However, here are common practices:\n",
    "   * **On small datasets**: Freeze early/middle layers (e.g., first 612 out of 1224), especially if using a pre-trained encoder (like BERT or ViT). Only fine-tune the top layers and task-specific heads. This prevents overfitting and saves computation while preserving general feature extraction.\n",
    "   * **On large datasets**: Typically freeze none. Fine-tune all encoder layers. This allows the model to fully adapt learned representations to the new task, often yielding the best performance.\n",
    "   * **In multimodal models** (e.g., **image captioning**): Often, the vision encoder (like ViT) is frozen or partially frozen, while the language encoder/decoder is trained. This is because *visual features are already well-learned from large-scale pretraining*, and only language alignment needs adjustment.\n",
    "\n",
    "Let's consider the following options:\n",
    "\n",
    "* Option 1. Freeze all encoder layers\n",
    "* Option 2. Freeze all encoder layers except the last one\n",
    "* Option 3. Unfreeze all layers of the model\n",
    "* Option 4. Unfreeze all layers of the model. Make one caption from 5 different captions\n",
    "* Option 5. The same as option 3, but reduce MAX_TOKENS from 50 to 20\n",
    "* Option 6. The same as option 4, but increase MAX_TOKENS from 50 to 132\n",
    "   * 6.1. Static augmentation (augment 1 time at the beginning)\n",
    "   * 6.2. Without augmentation\n",
    "   * 6.3. Augmentation on-the-fly (dynamic augmentation)\n",
    "   * 6.4. ROUGE-L F1 score instead of default cross-entropy loss\n",
    "   * 6.5. A model fine-tuned on the COCO dataset is used\n",
    "* Option 7. A model fine-tuned on the COCO dataset is used. Unfreeze only last layer of encoder and last layer of decoder. **The task with image captions has already been solved here!**\n",
    "* Option 8. The same as 6.4, but with texts augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9220b6a-39e3-4d16-a545-4ee3fcbf6dca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoder = model.encoder  # access the encoder part of the model\n",
    "\n",
    "if OPTION == 1:\n",
    "    print(\"Freeze all encoder layers\")\n",
    "    for param in encoder.parameters():\n",
    "        param.requires_grad = False  # freeze\n",
    "\n",
    "elif OPTION == 2:\n",
    "    print(\"Freeze all encoder layers except the last one\")\n",
    "    for param in encoder.parameters():  # freeze all encoder parameters\n",
    "        param.requires_grad = False  # freeze\n",
    "\n",
    "    # Unfreeze the parameters of the last encoder layer\n",
    "    if hasattr(encoder, \"encoder\") and hasattr(encoder.encoder, \"layer\"):\n",
    "        last_encoder_layer = encoder.encoder.layer[-1]\n",
    "        for param in last_encoder_layer.parameters():\n",
    "            param.requires_grad = True  # unfreeze\n",
    "    else:\n",
    "        # You might need to manually inspect the model's structure\n",
    "        # if the above path is incorrect, e.g., print(model.encoder) or\n",
    "        # print(model.encoder.encoder) to understand the structure.\n",
    "        print(f\"Could not find `encoder.encoder.layer`. \"\n",
    "              f\"Please, adjust the path to the last layer \"\n",
    "              f\"based on your model's architecture.\")\n",
    "\n",
    "elif OPTION in (3, 4, 5, 6, 8,):\n",
    "    print(\"Unfreeze all layers of the model\")\n",
    "    if OPTION == 4:\n",
    "        print(\"Make one caption from 5 different captions\")\n",
    "    if OPTION == 5:\n",
    "        print(f\"Reduce MAX_TOKENS from 50 to {MAX_TOKENS}\")\n",
    "    if OPTION in (6, 7, ):\n",
    "        print(f\"Increase MAX_TOKENS from 50 to {MAX_TOKENS}\")\n",
    "    if OPTION == 7:\n",
    "        print(\"A model fine-tuned on the COCO dataset is used\")\n",
    "    if OPTION == 8:\n",
    "        print(\"Use text augmentations\")\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True  # unfreeze\n",
    "\n",
    "elif OPTION == 7:\n",
    "    print(\"A model fine-tuned on the COCO dataset is used\")\n",
    "    print(\"Freeze all model layers except the last one\",\n",
    "          \"for encoder and the last one for decoder\")\n",
    "    for param in model.parameters():  # freeze all model parameters\n",
    "        param.requires_grad = False  # freeze\n",
    "\n",
    "    # Unfreeze the parameters of the last encoder layer\n",
    "    if hasattr(encoder, \"encoder\") and hasattr(encoder.encoder, \"layer\"):\n",
    "        last_encoder_layer = encoder.encoder.layer[-1]\n",
    "        for param in last_encoder_layer.parameters():\n",
    "            param.requires_grad = True  # unfreeze\n",
    "\n",
    "    # Unfreeze the parameters of the last decoder layer\n",
    "    decoder = model.decoder\n",
    "    if hasattr(decoder, \"transformer\") and hasattr(decoder.transformer, \"h\"):\n",
    "        print(\"OK\")\n",
    "        last_decoder_layer = decoder.transformer.h[-1]\n",
    "        for param in last_decoder_layer.parameters():\n",
    "            param.requires_grad = True  # unfreeze\n",
    "\n",
    "else:  # OPTION not set\n",
    "    print(f\"Warning: The variable OPTION should be equal to 1, 2, or 3, \"\n",
    "          f\"but it is equal to {OPTION}.\\n\")\n",
    "\n",
    "show_trainable_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0f1c6c-84d2-47ec-b30c-38b64585862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set token IDs in our model to be the same as GPT-2\n",
    "#   [BOS] Beginning Of Sequence\n",
    "#   [EOS] End Of Sequence\n",
    "#   [PAD] Padding token\n",
    "#   [SEP] (Separator) token often has the ID 102\n",
    "#   [CLS] (Classification) token is always the first token\n",
    "#         in a sequence, signaling the start of the input\n",
    "#   [UNK] unknown token that doesn't exist the the vocabulary set\n",
    "\n",
    "# Make sure the model's decoder config reflects the tokenizer\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.bos_token_id = tokenizer.bos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "# model.config.sep_token_id = tokenizer.sep_token_id\n",
    "\n",
    "# Suppress warning:\n",
    "#   Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3e6dd5-f858-494e-8b9b-59b8f4f2f363",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"rouge\")  # define metric\n",
    "\n",
    "ignore_pad_token_for_loss = True\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    if ignore_pad_token_for_loss:\n",
    "        # Replace -100 in the labels as we can't decode them\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds,\n",
    "                                                     decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds,\n",
    "                            references=decoded_labels,\n",
    "                            use_stemmer=True)\n",
    "    result = {key: round(value * 100, 4) for key, value in result.items()}\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "    ]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result\n",
    "\n",
    "\n",
    "training_args = hf.Seq2SeqTrainingArguments(\n",
    "    output_dir=RESULTS,          # the output directory\n",
    "    overwrite_output_dir=False,  # do not overwrite the output dir\n",
    "    num_train_epochs=EPOCHS,     # number of training epochs\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,  # batch size for training\n",
    "    per_device_eval_batch_size=BATCH_SIZE,   # batch size for evaluation\n",
    "    log_level=\"info\",\n",
    "    logging_steps=50,\n",
    "\n",
    "    load_best_model_at_end=True,  # save the best checkpoint at the end\n",
    "    save_total_limit=1,           # keep only the best model checkpoint\n",
    "    \n",
    "    # # Use default metrics: cross-entropy loss\n",
    "    # metric_for_best_model=\"eval_loss\", # use cross-entropy loss\n",
    "    # greater_is_better=False,           # optional. Lower loss is better\n",
    "\n",
    "    predict_with_generate=True,  # crucial for generative metrics like ROUGE\n",
    "    \n",
    "    # Use ROUGE-L F1 score on the validation set,\n",
    "    # instead of minimizing \"eval_loss\"\n",
    "    metric_for_best_model=\"eval_rougeL\",  # ROUGE-L F1 score\n",
    "    greater_is_better=True,\n",
    "\n",
    "    # # IMPORTANT: Set to False to get logits for compute_metrics\n",
    "    # Delete it if works --> prediction_loss_only=False,\n",
    "    \n",
    "    # Important for dynamic augmentation\n",
    "    # Keep the 'image' column until transformations are applied\n",
    "    remove_unused_columns=False,  # important for set_transform function\n",
    "    \n",
    "    # Disable automatic Weights & Biases logging\n",
    "    report_to=\"none\",  # or report_to=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486583f4-174c-4e2e-8121-714b4821178e",
   "metadata": {},
   "source": [
    "# Training and Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d615d34-5ea7-4cee-b868-8c3f46e6b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = hf.Seq2SeqTrainer(  # set trainer\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,  # compute ROUGE metrics\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"valid\"],\n",
    "    data_collator=hf.default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e89e70-6ca8-4656-b951-c1d69daacdce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()  # evaluate model before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587a6c1d-ec35-4ba1-b03d-99f69a63693a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "\n",
    "# # Error! Old PyTorch == v2.5. Must install PyTorch >= v2.6\n",
    "# #        to resume from the last checkpoint\n",
    "# trainer.train(resume_from_checkpoint=True)\n",
    "# trainer.train(resume_from_checkpoint=f\"{RESULTS}/checkpoint-1140\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defd257b-59e5-486f-a866-e4311a95fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()  # evaluate model after training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5d4e42-ce51-4093-acea-967c7739dec9",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "## Option 1. Freeze all encoder layers\n",
    "\n",
    "The best model:\n",
    "\n",
    "    path: \"temp/results-1/checkpoint-1900\"\n",
    "    val loss:  3.864\n",
    "    epoch:     11\n",
    "    Rouge1:    22.35\n",
    "    Rouge2:    0.7817\n",
    "    Rougel:    18.53\n",
    "    Rougelsum: 18.54\n",
    "\n",
    "System resources:\n",
    "\n",
    "    GPU:            GeForce GTX TITAN X\n",
    "    time per epoch: 55 min\n",
    "    batch size:     100\n",
    "    GPU memory:     8.8 GiB\n",
    "    System memory:  11.1 GiB\n",
    "\n",
    "Evaluation without training:\n",
    "\n",
    "    {'eval_loss': 9.401498794555664,\n",
    "     'eval_model_preparation_time': 0.0037,\n",
    "     'eval_rouge1': 10.9609,\n",
    "     'eval_rouge2': 0.6706,\n",
    "     'eval_rougeL': 9.1676,\n",
    "     'eval_rougeLsum': 9.1623,\n",
    "     'eval_gen_len': 20.0,\n",
    "     'eval_runtime': 240.1668,\n",
    "     'eval_samples_per_second': 10.409,\n",
    "     'eval_steps_per_second': 0.104}\n",
    "\n",
    "\n",
    "| Epoch | Train Loss | Valid Loss | Model Prep Time | Rouge1 | Rouge2 | Rougel | Rougelsum | Gen Len |\n",
    "|-------|------------|------------|-----------------|--------|--------|--------|-----------|---------|\n",
    "| 1     | 4.070      | 4.360      | 0.0037          | 21.80  | 0.3243 | 19.48  | 19.50     | 20.0    |\n",
    "| 2     | 3.827      | 4.192      | 0.0037          | 22.80  | 0.5268 | 19.54  | 19.54     | 20.0    |\n",
    "| 3     | 3.719      | 4.152      | 0.0037          | 22.97  | 0.6284 | 19.46  | 19.45     | 20.0    |\n",
    "| 4     | 3.619      | 4.052      | 0.0039          | 22.94  | 0.5304 | 19.60  | 19.59     | 20.0    |\n",
    "| 5     | 3.511      | 3.990      | 0.0039          | 21.63  | 0.7762 | 18.19  | 18.20     | 20.0    |\n",
    "| 6     | 3.443      | 3.978      | 0.0039          | 21.58  | 0.6607 | 18.09  | 18.09     | 20.0    |\n",
    "| 7     | 3.372      | 3.949      | 0.0033          | 22.23  | 0.7098 | 18.71  | 18.71     | 20.0    |\n",
    "| 8     | 3.317      | 3.903      | 0.0033          | 20.54  | 0.7372 | 17.39  | 17.40     | 20.0    |\n",
    "| 9     | 3.240      | 3.886      | 0.0033          | 22.46  | 0.8211 | 18.72  | 18.73     | 20.0    |\n",
    "| 10    | 3.168      | 3.865      | 0.0033          | 22.45  | 0.7149 | 18.52  | 18.51     | 20.0    |\n",
    "|best 11| 3.074      | **3.864**  | 0.0033          | 22.35  | 0.7817 | 18.53  | 18.54     | 20.0    |\n",
    "| 12    | 3.031      | 3.867      | 0.0033          | 20.61  | 0.7626 | 17.35  | 17.35     | 20.0    |\n",
    "| 13    | 2.947      | 3.878      | 0.0033          | 22.08  | 0.7810 | 18.57  | 18.58     | 20.0    |\n",
    "| 14    | 2.890      | 3.878      | 0.0033          | 21.64  | 0.7248 | 18.03  | 18.03     | 20.0    |\n",
    "| 15    | 2.833      | 3.886      | 0.0033          | 22.36  | 0.7474 | 18.43  | 18.43     | 20.0    |\n",
    "| 16    | 2.781      | 3.927      | 0.0033          | 21.02  | 0.6517 | 17.10  | 17.10     | 20.0    |\n",
    "| 17    | 2.727      | 3.929      | 0.0033          | 22.17  | 0.7558 | 18.15  | 18.14     | 20.0    |\n",
    "| 18    | 2.690      | 3.966      | 0.0033          | 21.68  | 0.7067 | 17.92  | 17.93     | 20.0    |\n",
    "| 19    | 2.642      | 3.987      | 0.0033          | 22.53  | 0.7910 | 18.33  | 18.33     | 20.0    |\n",
    "| 20    | 2.614      | 4.012      | 0.0033          | 22.38  | 0.6558 | 18.07  | 18.08     | 20.0    |\n",
    "| 21    | 2.561      | 4.026      | 0.0033          | 21.37  | 0.7144 | 17.54  | 17.53     | 20.0    |\n",
    "| 22    | 2.517      | 4.060      | 0.0033          | 22.39  | 0.7016 | 18.31  | 18.31     | 20.0    |\n",
    "| 23    | 2.508      | 4.067      | 0.0033          | 21.55  | 0.6468 | 17.47  | 17.47     | 20.0    |\n",
    "| 24    | 2.466      | 4.085      | 0.0033          | 22.00  | 0.7118 | 17.82  | 17.83     | 20.0    |\n",
    "| 25    | 2.441      | 4.113      | 0.0033          | 22.57  | 0.6768 | 18.26  | 18.26     | 20.0    |\n",
    "| 26    | 2.434      | 4.121      | 0.0033          | 22.44  | 0.7640 | 18.16  | 18.16     | 20.0    |\n",
    "| 27    | 2.385      | 4.130      | 0.0033          | 21.36  | 0.6297 | 17.53  | 17.53     | 20.0    |\n",
    "| 28    | 2.364      | 4.156      | 0.0033          | 21.94  | 0.7998 | 17.80  | 17.80     | 20.0    |\n",
    "| 29    | 2.370      | 4.176      | 0.0033          | 21.82  | 0.6829 | 17.72  | 17.71     | 20.0    |\n",
    "| 30    | 2.352      | 4.181      | 0.0033          | 21.65  | 0.6921 | 17.63  | 17.63     | 20.0    |\n",
    "| 31    | 2.347      | 4.196      | 0.0033          | 21.75  | 0.6942 | 17.60  | 17.60     | 20.0    |\n",
    "| 32    | 2.320      | 4.198      | 0.0033          | 21.89  | 0.6908 | 17.67  | 17.67     | 20.0    |\n",
    "\n",
    "Without `Rouge1` metrics and with `RandomResizedCrop` augmentaton:\n",
    "\n",
    "    Epoch \tTraining Loss \tValidation Loss \tModel Preparation Time\n",
    "    1 \t4.077100 \t3.982260 \t\t0.009800\n",
    "    2 \t3.838600 \t3.834445 \t\t0.009800\n",
    "    3 \t3.735400 \t3.797139 \t\t0.009800\n",
    "\n",
    "\n",
    "## Option 2. Freeze all encoder layers except the last one\n",
    "\n",
    "The best model:\n",
    "\n",
    "    path: \"temp/results-2/checkpoint-2660\"\n",
    "    val loss:  3.875\n",
    "    epoch:     7\n",
    "    Rouge1:    21.89\n",
    "    Rouge2:    0.8528\n",
    "    Rougel:    18.61\n",
    "    Rougelsum: 18.61\n",
    "\n",
    "System resources:\n",
    "\n",
    "    GPU:            GeForce GTX 1080 Ti\n",
    "    time per epoch: 50 min\n",
    "    batch size:     100\n",
    "    GPU memory:     9.8 GiB\n",
    "    System memory:  11.1 GiB\n",
    "\n",
    "Evaluation without training:\n",
    "\n",
    "    {'eval_loss': 5.6199164390563965,\n",
    "     'eval_model_preparation_time': 0.004,\n",
    "     'eval_rouge1': 16.6046,\n",
    "     'eval_rouge2': 1.1628,\n",
    "     'eval_rougeL': 13.1714,\n",
    "     'eval_rougeLsum': 13.1793,\n",
    "     'eval_gen_len': 20.0,\n",
    "     'eval_runtime': 214.0398,\n",
    "     'eval_samples_per_second': 11.68,\n",
    "     'eval_steps_per_second': 0.117}\n",
    "\n",
    "\n",
    "| Epoch | Train Loss | Valid Loss | Model Prep Time | Rouge1 | Rouge2 | Rougel | Rougelsum | Gen Len |\n",
    "|-------|------------|------------|-----------------|--------|--------|--------|-----------|---------|\n",
    "| 1     | 3.985      | 4.270      | 0.004           | 21.88  | 0.4221 | 19.12  | 19.12     | 20.0    |\n",
    "| 2     | 3.679      | 4.071      | 0.004           | 20.66  | 0.5120 | 17.56  | 17.56     | 20.0    |\n",
    "| 3     | 3.493      | 3.976      | 0.004           | 20.59  | 0.6132 | 17.37  | 17.36     | 20.0    |\n",
    "| 4     | 3.368      | 3.910      | 0.004           | 21.55  | 0.7771 | 18.02  | 18.03     | 20.0    |\n",
    "| 5     | 3.239      | 3.886      | 0.004           | 22.31  | 0.9343 | 18.58  | 18.59     | 20.0    |\n",
    "| 6     | 3.166      | 3.878      | 0.004           | 20.68  | 0.7047 | 17.52  | 17.52     | 20.0    |\n",
    "|best 7 | 3.057      | **3.875**  | 0.004           | 21.89  | 0.8528 | 18.61  | 18.61     | 20.0    |\n",
    "| 8     | 2.982      | 3.881      | 0.004           | 20.72  | 0.6773 | 17.29  | 17.30     | 20.0    |\n",
    "| 9     | 2.893      | 3.900      | 0.004           | 21.51  | 0.8518 | 17.89  | 17.90     | 20.0    |\n",
    "| 10    | 2.815      | 3.936      | 0.004           | 21.76  | 0.6937 | 17.78  | 17.78     | 20.0    |\n",
    "| 11    | 2.744      | 3.962      | 0.004           | 21.31  | 0.6021 | 17.74  | 17.75     | 20.0    |\n",
    "| 12    | 2.684      | 4.008      | 0.004           | 21.39  | 0.7786 | 17.71  | 17.70     | 20.0    |\n",
    "| 13    | 2.621      | 4.044      | 0.004           | 20.99  | 0.7117 | 17.11  | 17.12     | 20.0    |\n",
    "| 14    | 2.572      | 4.112      | 0.004           | 21.71  | 0.7766 | 17.70  | 17.71     | 20.0    |\n",
    "| 15    | 2.494      | 4.152      | 0.004           | 21.87  | 0.8325 | 17.71  | 17.71     | 20.0    |\n",
    "| 16    | 2.431      | 4.188      | 0.004           | 22.19  | 0.8707 | 18.20  | 18.20     | 20.0    |\n",
    "\n",
    "\n",
    "## Option 3. Unfreeze all layers of the model\n",
    "\n",
    "The best model:\n",
    "\n",
    "    path: \"temp/results-3/checkpoint-2847\"\n",
    "    val loss:  3.853\n",
    "    epoch:     3\n",
    "    Rouge1:    24.76\n",
    "    Rouge2:    0.9882\n",
    "    Rougel:    20.44\n",
    "    Rougelsum: 20.44\n",
    "\n",
    "System resources:\n",
    "\n",
    "    GPU:            GeForce GTX TITAN X\n",
    "    time per epoch: 74 min\n",
    "    batch size:     40\n",
    "    GPU memory:     10.5 GiB\n",
    "    System memory:  8.7 GiB\n",
    "\n",
    "Evaluation without training:\n",
    "\n",
    "    {'eval_loss': 9.400985717773438,\n",
    "     'eval_model_preparation_time': 0.0045,\n",
    "     'eval_rouge1': 10.9609,\n",
    "     'eval_rouge2': 0.6706,\n",
    "     'eval_rougeL': 9.1676,\n",
    "     'eval_rougeLsum': 9.1623,\n",
    "     'eval_gen_len': 20.0,\n",
    "     'eval_runtime': 256.4259,\n",
    "     'eval_samples_per_second': 9.749,\n",
    "     'eval_steps_per_second': 0.246}\n",
    "\n",
    "\n",
    "| Epoch | Train Loss | Valid Loss | Model Prep Time | Rouge1 | Rouge2 | Rougel | Rougelsum | Gen Len |\n",
    "|-------|------------|------------|-----------------|--------|--------|--------|-----------|---------|\n",
    "| 1     | 3.705      | 4.102      | 0.0045          | 23.06  | 0.9606 | 19.58  | 19.57     | 20.0    |\n",
    "| 2     | 3.411      | 3.912      | 0.0045          | 23.58  | 0.7694 | 19.47  | 19.47     | 20.0    |\n",
    "|best 3 | 3.145      | **3.853**  | 0.0045          | 24.76  | 0.9882 | 20.44  | 20.44     | 20.0    |\n",
    "| 4     | 2.885      | 3.901      | 0.0045          | 24.08  | 0.8212 | 19.81  | 19.81     | 20.0    |\n",
    "| 5     | 2.689      | 4.006      | 0.0045          | 24.42  | 0.8536 | 19.86  | 19.86     | 20.0    |\n",
    "| 6     | 2.450      | 4.205      | 0.0045          | 22.55  | 0.7631 | 18.29  | 18.30     | 20.0    |\n",
    "| 7     | 2.232      | 4.420      | 0.0045          | 22.58  | 0.6779 | 18.28  | 18.28     | 20.0    |\n",
    "| 8     | 1.991      | 4.790      | 0.0045          | 22.60  | 0.6153 | 18.03  | 18.03     | 20.0    |\n",
    "| 9     | 1.807      | 5.051      | 0.0045          | 22.97  | 0.6608 | 18.34  | 18.34     | 20.0    |\n",
    "| 10    | 1.644      | 5.442      | 0.0045          | 22.11  | 0.7197 | 17.75  | 17.74     | 20.0    |\n",
    "\n",
    "\n",
    "## Option 4. Unfreeze all layers of the model. Make one caption from 5 different captions\n",
    "\n",
    "The best model:\n",
    "\n",
    "    path: \"temp/results-4/checkpoint-950\"\n",
    "    val loss:  3.429\n",
    "    epoch:     5\n",
    "    Rouge1:    28.49\n",
    "    Rouge2:    1.7744\n",
    "    Rougel:    24.78\n",
    "    Rougelsum: 27.01\n",
    "\n",
    "System resources:\n",
    "\n",
    "    GPU:            GeForce GTX TITAN X\n",
    "    time per epoch: 16 min\n",
    "    batch size:     40\n",
    "    GPU memory:     10.5 GiB\n",
    "    System memory:  8.7 GiB\n",
    "\n",
    "\n",
    "| Epoch | Train Loss | Valid Loss | Model Prep Time | Rouge1 | Rouge2 | Rougel | Rougelsum | Gen Len |\n",
    "|-------|------------|------------|-----------------|--------|--------|--------|-----------|---------|\n",
    "| 1     | 3.927      | 3.792      | 0.0079          | 27.28  | 1.0652 | 25.23  | 26.72     | 20.0    |\n",
    "| 2     | 3.582      | 3.610      | 0.0079          | 29.68  | 1.4507 | 25.86  | 28.70     | 20.0    |\n",
    "| 3     | 3.428      | 3.501      | 0.0079          | 29.12  | 1.5645 | 25.80  | 28.29     | 20.0    |\n",
    "| 4     | 3.271      | 3.460      | 0.0079          | 31.55  | 1.8716 | 26.72  | 30.09     | 20.0    |\n",
    "|best 5 | 3.133      | **3.429**  | 0.0079          | 28.49  | 1.7744 | 24.78  | 27.01     | 20.0    |\n",
    "| 6     | 3.030      | 3.430      | 0.0079          | 31.69  | 2.0999 | 26.98  | 29.99     | 20.0    |\n",
    "| 7     | 2.929      | 3.441      | 0.0079          | 30.25  | 2.0315 | 25.80  | 28.44     | 20.0    |\n",
    "| 8     | 2.838      | 3.464      | 0.0079          | 31.95  | 2.2662 | 26.83  | 30.01     | 20.0    |\n",
    "| 9     | 2.760      | 3.505      | 0.0079          | 31.31  | 2.3112 | 26.09  | 29.34     | 20.0    |\n",
    "| 10    | 2.670      | 3.556      | 0.0079          | 30.73  | 2.3346 | 25.80  | 28.92     | 20.0    |\n",
    "| 11    | 2.566      | 3.619      | 0.0079          | 30.79  | 2.2847 | 25.55  | 28.78     | 20.0    |\n",
    "| 12    | 2.478      | 3.698      | 0.0079          | 31.43  | 2.4537 | 26.00  | 29.28     | 20.0    |\n",
    "| 13    | 2.420      | 3.767      | 0.0079          | 31.52  | 2.4108 | 25.90  | 29.33     | 20.0    |\n",
    "| 14    | 2.318      | 3.869      | 0.0079          | 31.19  | 2.3768 | 25.48  | 29.27     | 20.0    |\n",
    "| 15    | 2.255      | 3.971      | 0.0079          | 30.33  | 2.3828 | 25.02  | 28.37     | 20.0    |\n",
    "| 16    | 2.196      | 4.073      | 0.0079          | 30.44  | 2.3379 | 24.90  | 28.56     | 20.0    |\n",
    "\n",
    "## Option 5. The same as option 3, but reduce MAX_TOKENS from 50 to 20\n",
    "\n",
    "The best model:\n",
    "\n",
    "    path: \"temp/results-4/checkpoint-2847\"\n",
    "    val loss:  3.874\n",
    "    epoch:     3\n",
    "    Rouge1:    24.63\n",
    "    Rouge2:    0.9694\n",
    "    Rougel:    20.35\n",
    "    Rougelsum: 20.35\n",
    "\n",
    "System resources:\n",
    "\n",
    "    GPU:            GeForce GTX TITAN X\n",
    "    time per epoch: 66 min\n",
    "    batch size:     40\n",
    "    GPU memory:     8.6 GiB\n",
    "    System memory:  8.0 GiB\n",
    "\n",
    "\n",
    "| Epoch | Train Loss | Valid Loss | Model Prep Time | Rouge1 | Rouge2 | Rougel | Rougelsum | Gen Len |\n",
    "|-------|------------|------------|-----------------|--------|--------|--------|-----------|---------|\n",
    "| 1     | 3.703      | 4.098      | 0.0070          | 23.42  | 0.9370 | 20.08  | 20.09     | 20.0    |\n",
    "| 2     | 3.419      | 3.911      | 0.0070          | 23.60  | 0.7384 | 19.36  | 19.37     | 20.0    |\n",
    "|best 3 | 3.151      | **3.874**  | 0.0070          | 24.63  | 0.9694 | 20.35  | 20.35     | 20.0    |\n",
    "| 4     | 2.903      | 3.901      | 0.0070          | 24.00  | 0.7300 | 19.76  | 19.75     | 20.0    |\n",
    "| 5     | 2.699      | 4.032      | 0.0070          | 23.26  | 0.7618 | 18.78  | 18.78     | 20.0    |\n",
    "| 6     | 2.498      | 4.215      | 0.0070          | 21.29  | 0.7503 | 17.51  | 17.51     | 20.0    |\n",
    "| 7     | 2.268      | 4.441      | 0.0070          | 22.97  | 0.7909 | 18.69  | 18.70     | 20.0    |\n",
    "| 8     | 2.063      | 4.738      | 0.0070          | 21.72  | 0.8278 | 17.53  | 17.54     | 20.0    |\n",
    "\n",
    "\n",
    "## Option 6. The same as option 4, but increase MAX_TOKENS from 50 to 132\n",
    "\n",
    "### 6.1. With static augmentation. Augment only 1 time at the beginning of the training\n",
    "    path: \"temp/results-4/checkpoint-1824\"\n",
    "    val loss:  3.3687 - with static augmentation\n",
    "    epoch:     6\n",
    "    Rouge1:    29.57\n",
    "    Rouge2:    2.3208\n",
    "    Rougel:    25.21\n",
    "    Rougelsum: 28.00\n",
    "\n",
    "### 6.2. Without augmentation\n",
    "    path: \"temp/results-4/checkpoint-1824\"\n",
    "    val loss:  3.305 - without augmentation\n",
    "    epoch:     6\n",
    "    Rouge1:    29.78\n",
    "    Rouge2:    2.4775\n",
    "    Rougel:    25.12\n",
    "    Rougelsum: 28.11\n",
    "\n",
    "### 6.3. Dynamic augmentation on-the-fly during training\n",
    "    path: \"temp/results-1/checkpoint-2432\"\n",
    "    val loss:  3.270 - dynamic augmentation on-the-fly during training\n",
    "    epoch:     8\n",
    "    Rouge1:    28.12 \t \t\n",
    "    Rouge2:    2.0994\n",
    "    Rougel:    23.82\n",
    "    Rougelsum: 26.96\n",
    "\n",
    "### 6.4. ROUGE-L F1 score instead of default cross-entropy loss (\"eval_loss\")\n",
    "    path: \"temp/results-1/checkpoint-1824\"\n",
    "    val loss:  3.278 - with ROUGE-L F1 score\n",
    "    epoch:     6\n",
    "    Rouge1:    29.84\n",
    "    Rouge2:    2.5135\n",
    "    Rougel:    **25.47** - this is the main metric for now\n",
    "    Rougelsum: 28.31\n",
    "\n",
    "\n",
    "System resources:\n",
    "\n",
    "    GPU:            GTX 1080 Ti\n",
    "    time per epoch: 13.5 - 16.5 min\n",
    "    batch size:     25\n",
    "    GPU memory:     10.2 GiB\n",
    "    System memory:  7.7 - 8.0 GiB\n",
    "\n",
    "\n",
    "| Epoch | Train Loss | Valid Loss | Model Prep Time | Rouge1 | Rouge2 | Rougel | Rougelsum | Gen Len |\n",
    "|-------|------------|------------|-----------------|--------|--------|--------|-----------|---------|\n",
    "| 1     | 3.706      | 3.635      | 0.0087          | 27.49  | 0.7554 | 24.17  | 26.75     | 20.0    |\n",
    "| 2     | 3.432      | 3.467      | 0.0087          | 28.50  | 1.6885 | 24.61  | 27.10     | 20.0    |\n",
    "| 3     | 3.333      | 3.377      | 0.0087          | 27.89  | 2.0856 | 24.34  | 26.50     | 20.0    |\n",
    "| 4     | 3.177      | 3.327      | 0.0087          | 29.16  | 2.2739 | 24.80  | 27.44     | 20.0    |\n",
    "| 5     | 3.099      | 3.298      | 0.0087          | 28.21  | 2.2799 | 24.10  | 26.78     | 20.0    |\n",
    "|best 6 | 3.029      | 3.278      | 0.0087          | 29.84  | 2.5135 |**25.47**| 28.31    | 20.0    |\n",
    "| 7     | 2.950      | 3.281      | 0.0087          | 28.37  | 2.4479 | 24.27  | 26.93     | 20.0    |\n",
    "| 8     | 2.877      | 3.270      | 0.0087          | 28.12  | 2.0994 | 23.82  | 26.96     | 20.0    |\n",
    "| 9     | 2.835      | 3.277      | 0.0087          | 29.20  | 2.5044 | 24.61  | 27.57     | 20.0    |\n",
    "| 10    | 2.771      | 3.279      | 0.0087          | 29.25  | 2.4859 | 24.75  | 27.74     | 20.0    |\n",
    "\n",
    "\n",
    "## Option 7. A model fine-tuned on the COCO dataset is used\n",
    "\n",
    "    path: \"temp/results-1/checkpoint-1521\"\n",
    "    val loss:  3.693\n",
    "    epoch:     3\n",
    "    Rouge1:    32.91\n",
    "    Rouge2:    3.85\n",
    "    Rougel:    **28.00** - the main metric\n",
    "    Rougelsum: 31.32\n",
    "\n",
    "| Epoch | Train Loss | Valid Loss | Model Prep Time | Rouge1 | Rouge2 | Rougel | Rougelsum | Gen Len |\n",
    "|-------|------------|------------|-----------------|--------|--------|--------|-----------|---------|\n",
    "| 1     | 3.954      | 3.917      | 0.0114          | 32.22  | 3.91   | 27.30  | 30.77     | 20.0    |\n",
    "| 2     | 3.789      | 3.764      | 0.0114          | 32.30  | 3.78   | 27.41  | 30.91     | 20.0    |\n",
    "|best 3 | 3.760      | 3.693      | 0.0114          | 32.91  | 3.85   |**28.00**| 31.32     | 20.0    |\n",
    "| 4     | 3.701      | 3.659      | 0.0114          | 32.22  | 3.63   | 27.51  | 30.75     | 20.0    |\n",
    "| 5     | 3.677      | 3.648      | 0.0114          | 32.73  | 3.73   | 27.69  | 31.24     | 20.0    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4337f8b-131c-4cd7-836b-15e04a99ea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the current (not the best) state of the model within the Trainer\n",
    "trainer.save_model(RESULTS)  # save model checkpoint to RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc996cb0-e260-4ea9-8d12-d01256be9246",
   "metadata": {},
   "source": [
    "# Examples\n",
    "\n",
    "To run just the examples, run the **Helper Functions and Libraries** at the beginning of this Jupyter Notebook and come back here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf521446-15d8-4e12-a055-728f2ae705f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # I don't know how to view the best checkpoint\n",
    "# # Thus evaluate every checkpoint OR keep only one best model checkpoint\n",
    "# num = [380, 760, 1140, 1520, 1900, 2280, 2660, 3040, 3420,\n",
    "#        3800, 4180, 4560, 4940, 5320, 5700, 6080, 6460, 6840,\n",
    "#        7220, 7600, 7980, 8360, 8740, 9120, 9500, 9880,]\n",
    "\n",
    "# for n in num:\n",
    "#     path = f\"{RESULTS}/checkpoint-{n}/\"\n",
    "#     print(f\"Evaluate model: {path}\")\n",
    "#     model = hf.VisionEncoderDecoderModel.from_pretrained(path).to(DEVICE)\n",
    "\n",
    "#     trainer = hf.Seq2SeqTrainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         processing_class=tokenizer,\n",
    "#         compute_metrics=compute_metrics,\n",
    "#         train_dataset=ds[\"train\"],\n",
    "#         eval_dataset=ds[\"valid\"],\n",
    "#         data_collator=hf.default_data_collator,\n",
    "#     )\n",
    "\n",
    "#     metrics = trainer.evaluate()\n",
    "#     eval_loss = metrics[\"eval_loss\"]\n",
    "#     print(f\"Validation loss: {eval_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd48cafc-79af-4410-b855-6e01be45b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model and config from pretrained folder\n",
    "\n",
    "# path = f\"{RESULTS}/checkpoint-1521/\"\n",
    "# model = hf.VisionEncoderDecoderModel.from_pretrained(path).to(DEVICE)\n",
    "\n",
    "model = hf.VisionEncoderDecoderModel.from_pretrained(RESULTS).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a4a5ad-9070-40ea-bfd9-fb98858f710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not finetuned model\n",
    "non_finetuned = hf.VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    ENCODER, DECODER).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08099ebc-0817-41df-826b-0059780a15f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = hf.AutoImageProcessor.from_pretrained(ENCODER, use_fast=True)\n",
    "\n",
    "normalize = t.Normalize(\n",
    "    mean=feature_extractor.image_mean,\n",
    "    std=feature_extractor.image_std\n",
    ")\n",
    "\n",
    "# It is generally NOT recommended to use data augmentation on the validation set\n",
    "process_valid = t.Compose([\n",
    "    t.Resize(list(feature_extractor.size.values()),\n",
    "             interpolation=t.InterpolationMode.BICUBIC),\n",
    "    t.ToTensor(),  # convert to pytorch tensor\n",
    "    normalize,  # normalize pixel values\n",
    "])\n",
    "\n",
    "tokenizer = hf.GPT2TokenizerFast.from_pretrained(DECODER)\n",
    "\n",
    "# GPT2 only has bos/eos tokens but not decoder_start/pad tokens\n",
    "tokenizer.pad_token = tokenizer.eos_token  # pad with EOS token\n",
    "\n",
    "NUM_BEAMS = 5  # number of captions to the image\n",
    "\n",
    "\n",
    "def read_csv(csv_file):\n",
    "    \"\"\" Read CSV file and return dictionary of\n",
    "        keys (image path) and values (image captions) \"\"\"\n",
    "    captions = {}  # dictionary of keys (image path) and values (image captions)\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    for row in df.itertuples():\n",
    "        path, caption = row.image, row.caption\n",
    "        if path in captions:\n",
    "            captions[path].append(caption)\n",
    "        else:\n",
    "            captions[path] = [caption]\n",
    "    \n",
    "    return captions\n",
    "\n",
    "\n",
    "def get_key_value(captions, num):\n",
    "    \"\"\" Get key-value pair from the dictionary \"\"\"\n",
    "    iterator = iter(captions.items())\n",
    "    key_value = next(islice(iterator, num, num + 1), None)\n",
    "    return key_value\n",
    "\n",
    "\n",
    "def predict_captions(model, path,\n",
    "                     num_beams=NUM_BEAMS,\n",
    "                     max_new_tokens=16,  # MAX_TOKENS ??\n",
    "                     top_k=1,\n",
    "                     num_return_sequences=NUM_BEAMS,\n",
    "                     show=True,\n",
    "                     message=\"Predicted captions:\",\n",
    "    ):\n",
    "    \"\"\" Helper function for getting image captions from the Internet or from a file path \"\"\"\n",
    "    if path.startswith(\"https://\"):  # download from the Internet\n",
    "        response = requests.get(path)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        image_matrix = process_valid(img).unsqueeze(0)\n",
    "    else:  # from the file path\n",
    "        img = Image.open(path)\n",
    "        image_matrix = process_valid(img).unsqueeze(0)\n",
    "\n",
    "    generated = model.generate(\n",
    "        image_matrix.to(DEVICE),\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        num_beams=num_beams,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        early_stopping=True,\n",
    "        do_sample=True,\n",
    "        top_k=top_k,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "    )\n",
    "\n",
    "    preds = [tokenizer.decode(g, skip_special_tokens=True).strip() for g in generated]\n",
    "\n",
    "    display(img) if show else None\n",
    "    print(f\"\\n{message}\\t\", *preds, sep=\"\\n\\t\")\n",
    "\n",
    "\n",
    "captions_val = read_csv(VALID_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bade67b2-64d4-49f3-bbf3-09460fc1455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path, descriptions = get_key_value(captions_val, 0)\n",
    "predict_captions(model, path)\n",
    "print(\"\\nGround truth captions:\\t\", *descriptions, sep=\"\\n\\t\")\n",
    "predict_captions(non_finetuned, path, show=False, message=\"Untrained model:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f8cee1-0a24-4c30-8b53-14562526e12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path, descriptions = get_key_value(captions_val, 1)\n",
    "predict_captions(model, path)\n",
    "print(\"\\nGround truth captions:\\t\", *descriptions, sep=\"\\n\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e33144-a25d-4605-a0ea-cf265c979d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "path, descriptions = get_key_value(captions_val, 2)\n",
    "predict_captions(model, path)\n",
    "print(\"\\nGround truth captions:\\t\", *descriptions, sep=\"\\n\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25222044-a0ea-43f0-8d20-a679715645ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"https://minsknews.by/wp-content/uploads/2025/10/120a8130-kopiya.jpg\"\n",
    "predict_captions(model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7780bb32-43e2-49cc-9df4-1b5fae2c6752",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"https://football.by/storage/images/post_img/1759599441.jpg\"\n",
    "predict_captions(model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c1c5b7-7688-45b1-ae7f-c088b90216b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"https://raw.githubusercontent.com/foobar167/junkyard/refs/heads/master/fine_tuning/data/test_image_02.jpg\"\n",
    "predict_captions(model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fddfb9-fbc3-45aa-9290-388dc6141425",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"https://raw.githubusercontent.com/foobar167/junkyard/refs/heads/master/fine_tuning/data/test_image_03.jpg\"\n",
    "predict_captions(model, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc319f2-536c-4ec6-9dc4-90f69434fb14",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "**NOTE1: This pipeline does not work.**\n",
    "\n",
    "**NOTE2: In this pipeline I get text subtitles which are not user friendly and I don't know why.**\n",
    "\n",
    "* Option 1. Freeze all encoder layers\n",
    "* Option 2. Freeze all encoder layers except the last one\n",
    "* Option 3. Unfreeze all layers of the model\n",
    "* Option 4. Unfreeze all layers of the model. Make one caption from 5 different captions\n",
    "* Option 5. The same as option 3, but reduce MAX_TOKENS from 50 to 20\n",
    "* Option 6. The same as option 4, but increase MAX_TOKENS from 50 to 132\n",
    "   * 6.1. Static augmentation (augment 1 time at the beginning)\n",
    "   * 6.2. Without augmentation\n",
    "   * 6.3. Augmentation on-the-fly (dynamic augmentation)\n",
    "   * 6.4. ROUGE-L F1 score instead of default cross-entropy loss\n",
    "* Option 7. A model fine-tuned on the COCO dataset is used. Unfreeze only last layer of encoder and last layer of decoder. **The task with image captions has already been solved here!**\n",
    "* Option 8. The same as 6.4, but with texts augmentation.\n",
    "\n",
    "\n",
    "| Option | Val. loss | Epoch | Rouge1 | Rouge2 | Rougel | Rougelsum | min/epoch | GPU model   | Batch | Mem, GiB | RAM, GiB |\n",
    "| ------ | --------- | ----- | ------ | ------ | ------ | --------- | --------- | ----------- | ----- | -------- | -------- |\n",
    "| 1      | 3.864     | 11    | 22.35  | 0.7817 | 18.53  | 18.54     | 55        | GTX TITAN X | 100   | 8.8      | 11.1     |\n",
    "| 2      | 3.875     | 7     | 21.89  | 0.8528 | 18.61  | 18.61     | 50        | GTX 1080 Ti | 100   | 9.8      | 11.1     |\n",
    "| 3      | 3.853     | 3     | 24.76  | 0.9882 | 20.44  | 20.44     | 74        | GTX TITAN X | 40    | 10.5     | 8.7      |\n",
    "| 4      | 3.429     | 5     | 28.49  | 1.7744 | 24.78  | 27.01     | 14        | GTX TITAN X | 40    | 10.5     | 8.7      |\n",
    "| 5      | 3.874     | 3     | 24.63  | 0.9694 | 20.35  | 20.35     | 66        | GTX TITAN X | 40    | 8.6      | 8.0      |\n",
    "| 6.1    | 3.369     | 6     | 29.57  | 2.3208 | 25.21  | 28.00     | 13.5      | GTX 1080 Ti | 25    | 10.2     | 7.7      |\n",
    "| 6.2    | 3.305     | 6     | 29.78  | 2.4775 | 25.12  | 28.11     | 13.5      | GTX 1080 Ti | 25    | 10.2     | 7.7      |\n",
    "| 6.3    | 3.270     | 8     | 28.12  | 2.0994 | 23.82  | 26.96     | 16.5      | GTX 1080 Ti | 25    | 10.2     | 8.0      |\n",
    "|**best** 6.4| 3.278 | 6     | 29.84  | 2.5135 |**25.47**| 28.31    | 16.5      | GTX 1080 Ti | 25    | 10.2     | 8.0      |\n",
    "|**best** 7|   3.693 | 3     | 32.91  | 3.85   |**28.00**| 31.32    | 15        | GTX 1080 Ti | 15    | 5.0      | 7.8      |\n",
    "| 8      |           |       |        |        |        |           |           |             |       |          |          |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   01. Don't use older GPUs for modern tasks, such as the NVIDIA GeForce GTX 1080 Ti and NVIDIA GeForce GTX TITAN X. Even if the necessary libraries are present, they are full of bugs, and no one will fix them, as technical support for older GPUs has ended.\n",
    "   01. The Flickr8k dataset with 8,091 image-text examples (5 pairs for each example) is **too small** even for fine-tuning the model.\n",
    "       * Try Flickr30k dataset consisting of 30,000 images that are each paired with 5 different captions. Links: [Huggingface](https://huggingface.co/datasets/nlphuji/flickr30k), [GitHub](https://github.com/awsaf49/flickr-dataset), [Kaggle](https://www.kaggle.com/datasets/adityajn105/flickr30k).\n",
    "       * Try COCO dataset consisting of 330,000 images that are each paired with 5 different captions. Links: [Huggingface](https://huggingface.co/datasets/HuggingFaceM4/COCO), [Kaggle](https://www.kaggle.com/datasets/nikhil7280/coco-image-caption), [Homepage](https://cocodataset.org/#home).\n",
    "   01. Important for training:\n",
    "       * The longer the caption string, the better. Thus, make 1 caption from 5 different captions and increase number of tokens to the maximum.\n",
    "       * Try to find a ready-to-use trained model specific to your particular domain.\n",
    "   01. Insignificant (**not important**). However, for these options the result is slightly better:\n",
    "       * Freezing layers.\n",
    "       * Augmentation of images.\n",
    "       * Replacing the default cross-entropy metric with other metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c065c90-2b60-409c-a844-bc1508f992d3",
   "metadata": {},
   "source": [
    "# Clear Resources and Restarting the Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364e57e1-da5b-41e6-8999-a6eecdbd926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script echo \"Skip this cell\"\n",
    "\n",
    "# Clean up resources. Place this code at the end of the program\n",
    "import os, signal\n",
    "os.kill(os.getpid(), signal.SIGTERM)  # you can use signal.SIGKILL for Linux, but not for Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81a37f4-4c9d-408b-99c8-df44f6527349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
