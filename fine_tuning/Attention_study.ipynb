{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "295685aa-717e-4be7-8a4e-783decd68e52",
   "metadata": {},
   "source": [
    "# Studying the mechanism of self-attention\n",
    "\n",
    "[GitHub link](https://github.com/foobar167/junkyard/blob/master/fine_tuning/Attention_study.ipynb) and\n",
    "[Colab link](https://colab.research.google.com/drive/1912LC9Tn7lyBzIwlZd2_QuNJqBtZzZ2D) to this Python script.\n",
    "\n",
    "* Video [Understanding the Self-Attention Mechanism in **8 min**](https://youtu.be/W28LfOld44Y) - theory\n",
    "* Video [Multi-head Attention Mechanism Explained](https://youtu.be/W6s9i02EiR0) in **4 min** - theory\n",
    "* Video [Implementing the Self-Attention Mechanism from Scratch in PyTorch](https://youtu.be/ZPLym9rJtM8) - **4 min** to implement + **11 min** testing and playing with code\n",
    "* Article [Attention Is All You Need](https://ar5iv.labs.arxiv.org/html/1706.03762)\n",
    "\n",
    "![Self-Attention scheme](./data/Self-Attention_Layer.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6007c17b-8883-4579-bb41-d034a097936b",
   "metadata": {},
   "source": [
    "## Attention implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b25d972-8b71-4800-8026-ea3652df2942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\" Attention mechanism implementation \"\"\"\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.Q = nn.Linear(d_in, d_out)  # initialize fully connected layer with random values\n",
    "        self.K = nn.Linear(d_in, d_out)\n",
    "        self.V = nn.Linear(d_in, d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.Q(x)  # function performs the math operation y=xA^{T}+b\n",
    "        keys = self.K(x)\n",
    "        values = self.V(x)\n",
    "        # torch.bmm  - Batch Matrix-Matrix Product\n",
    "        # Use torch.matmul for Multi-Head Attention\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2))\n",
    "        # Normalize. Normalized scores do not depend on the number of elements\n",
    "        scores = scores * (self.d_out ** -0.5)\n",
    "        # Apply the softmax so that the sum of the values in a row equals 1.0\n",
    "        attention = F.softmax(scores, dim=-1)  # apply sortmax to the last dimension\n",
    "        hidden_states = torch.bmm(attention, values)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4362f6a7-eba5-4ab0-a984-c9cd2e1ea9fb",
   "metadata": {},
   "source": [
    "## Implementation of a very simple tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2800bd3-57dc-42ce-aaf5-d36bf3f6285c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'BOS')\n",
      "(1, 'EOS')\n",
      "(2, 'how')\n",
      "(3, 'thank')\n",
      "(4, '?')\n",
      "(5, 'are')\n",
      "(6, 'and')\n",
      "(7, 'doing')\n",
      "(8, 'good')\n",
      "(9, 'fine,')\n",
      "(10, 'am')\n",
      "(11, 'you')\n",
      "(12, 'i')\n",
      "(13, '.')\n",
      "\n",
      "\n",
      "('BOS', 0)\n",
      "('EOS', 1)\n",
      "('how', 2)\n",
      "('thank', 3)\n",
      "('?', 4)\n",
      "('are', 5)\n",
      "('and', 6)\n",
      "('doing', 7)\n",
      "('good', 8)\n",
      "('fine,', 9)\n",
      "('am', 10)\n",
      "('you', 11)\n",
      "('i', 12)\n",
      "('.', 13)\n"
     ]
    }
   ],
   "source": [
    "BOS_token = 0  # BOS (Beginning of Sequence) the same as SOS (Start of Sequence)\n",
    "EOS_token = 1  # EOS (End of Sequence)\n",
    "\n",
    "index2word = {\n",
    "    BOS_token: \"BOS\",\n",
    "    EOS_token: \"EOS\",\n",
    "}\n",
    "\n",
    "text = \"How are you doing ? I am good and you ? I am fine, thank you .\"\n",
    "vocabulary = set(text.lower().split(\" \"))  # set of unique words\n",
    "\n",
    "for word in vocabulary:\n",
    "    index2word[len(index2word)] = word\n",
    "\n",
    "print(*list(index2word.items()), sep=\"\\n\")\n",
    "\n",
    "word2index = {w: i for i, w in index2word.items()}\n",
    "print(\"\\n\", *list(word2index.items()), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d32e3be-3259-48ce-b3c8-95490c2eaeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor: tensor([[ 2,  5, 11,  7,  4]])\n",
      "Size of input tensor: torch.Size([1, 5])\n",
      "Converted sentence: How are you doing ?\n"
     ]
    }
   ],
   "source": [
    "def convert2tensor(sentence):\n",
    "    \"\"\" Convert sentence to tensor \"\"\"\n",
    "    words_list = sentence.lower().split(\" \")\n",
    "    indices = [word2index[word] for word in words_list]\n",
    "    # Add new dimension for batches using view(1,-1) at the beginning of the tensor\n",
    "    return torch.tensor(indices, dtype=torch.long).view(1, -1)\n",
    "\n",
    "\n",
    "def convert2sentence(tensor):\n",
    "    \"\"\" Convert tensor with tokens to sentence \"\"\"\n",
    "    indices = tensor.tolist()[0]\n",
    "    words_list = [index2word[index] for index in indices]\n",
    "    return \" \".join(words_list).capitalize()\n",
    "\n",
    "\n",
    "sentence = \"How are you doing ?\"\n",
    "\n",
    "input_tensor = convert2tensor(sentence)\n",
    "print(f\"Input tensor: {input_tensor}\")\n",
    "print(f\"Size of input tensor: {input_tensor.size()}\")\n",
    "\n",
    "sentence_converted = convert2sentence(input_tensor)\n",
    "print(f\"Converted sentence: {sentence_converted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bbd1d2-d243-41f7-816a-41fca333a580",
   "metadata": {},
   "source": [
    "## Create a very small neural network with attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fabe9c90-008e-463f-b04f-ffe29e1784f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.5221,  0.7157, -0.3543, -0.5256,  0.1637, -0.0708, -0.0091,\n",
      "           1.0332, -1.3994,  3.2293,  0.3880,  1.6749],\n",
      "         [-0.3050,  0.1364,  0.5668, -0.1816,  3.0615, -0.6120,  1.1956,\n",
      "          -0.5862,  0.6557,  0.8610, -0.2304, -0.8637],\n",
      "         [ 1.7445, -0.2422,  1.5841,  0.6004, -0.7200, -0.1400,  0.5470,\n",
      "           0.0795,  1.4277, -2.1220,  0.0203, -0.0942],\n",
      "         [ 0.5990, -0.2254, -0.3385,  0.4682,  0.3350, -1.9881,  0.5771,\n",
      "          -1.4849,  0.3929,  3.1484, -1.7804,  0.1608],\n",
      "         [-1.6338,  1.6158, -0.2962, -0.2781,  0.7622,  0.8868, -1.0893,\n",
      "          -0.0453, -0.1252, -0.2076,  0.1610,  0.3798]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "Size of embedded tensor: torch.Size([1, 5, 12])\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_SIZE = 12\n",
    "VOCAB_SIZE = len(word2index)\n",
    "\n",
    "sentence = \"How are you doing ?\"\n",
    "input_tensor = convert2tensor(sentence)\n",
    "\n",
    "embedding = nn.Embedding(VOCAB_SIZE, HIDDEN_SIZE)\n",
    "embedded = embedding(input_tensor)\n",
    "\n",
    "print(embedded)\n",
    "# size: [batch_size, sentence_length, hidden_size]\n",
    "print(f\"Size of embedded tensor: {embedded.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1832c7e-b92e-420d-857b-e8f8034d9e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1953,  0.2028, -0.2137, -0.0722, -0.0161, -0.0196, -0.1961,\n",
      "          -0.0577,  0.0434, -0.2525, -0.4601, -0.1460],\n",
      "         [ 0.3746,  0.3702, -0.0224, -0.2034, -0.0802, -0.2717, -0.1027,\n",
      "          -0.2241,  0.1758, -0.2005, -0.4023, -0.1239],\n",
      "         [ 0.5535,  0.4315,  0.0739, -0.3023, -0.0796, -0.2415,  0.0369,\n",
      "          -0.2942,  0.2452, -0.1164, -0.3334, -0.0777],\n",
      "         [ 0.5658,  0.5691,  0.0541, -0.2639, -0.1582, -0.4898, -0.0853,\n",
      "          -0.3069,  0.3372, -0.1678, -0.4179, -0.0766],\n",
      "         [-0.1154, -0.0788, -0.0958, -0.0858,  0.0741,  0.0317, -0.1548,\n",
      "          -0.0803, -0.1996, -0.2901, -0.3319, -0.2645]]],\n",
      "       grad_fn=<BmmBackward0>)\n",
      "torch.Size([1, 5, 12])\n"
     ]
    }
   ],
   "source": [
    "attention = Attention(HIDDEN_SIZE, HIDDEN_SIZE)  # initialize object\n",
    "\n",
    "hidden_states = attention(embedded)  # call forward funcion\n",
    "\n",
    "print(hidden_states)\n",
    "print(hidden_states.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "958b9eb8-acc9-499a-9f7f-1254b503b5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 12])\n",
      "torch.Size([1, 5, 12])\n",
      "torch.Size([1, 5, 12])\n"
     ]
    }
   ],
   "source": [
    "d_in = HIDDEN_SIZE\n",
    "d_out = HIDDEN_SIZE\n",
    "\n",
    "Q = nn.Linear(d_in, d_out)  # initialize fully connected layer with random values\n",
    "K = nn.Linear(d_in, d_out)\n",
    "V = nn.Linear(d_in, d_out)\n",
    "\n",
    "queries = Q(embedded)  # function performs the math operation y=xA^{T}+b\n",
    "keys = K(embedded)\n",
    "values = V(embedded)\n",
    "\n",
    "print(queries.size(), keys.size(), values.size(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eeb4f6f1-c456-4b59-ae4b-46906ee897c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 5])\n",
      "\n",
      "torch.Size([1, 5, 5])\n",
      "Sum of the rows of the last dimension:\n",
      "\ttensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "tensor([[[0.1559, 0.1356, 0.2756, 0.2237, 0.2092],\n",
      "         [0.2023, 0.1844, 0.2509, 0.1668, 0.1955],\n",
      "         [0.1155, 0.2493, 0.1633, 0.1428, 0.3291],\n",
      "         [0.3623, 0.1713, 0.1715, 0.2217, 0.0732],\n",
      "         [0.1743, 0.1350, 0.2164, 0.2106, 0.2637]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "torch.Size([1, 5, 12])\n",
      "tensor([[[ 0.0576, -0.3408, -0.4609, -0.2570, -0.1346, -0.2015, -0.0409,\n",
      "          -0.4859,  0.0735,  0.4759, -0.1061, -0.3496],\n",
      "         [ 0.0649, -0.3369, -0.4622, -0.3170, -0.1237, -0.1676, -0.0298,\n",
      "          -0.4763,  0.0419,  0.4923, -0.1804, -0.3864],\n",
      "         [ 0.1752, -0.3712, -0.5860, -0.3137, -0.0623, -0.0788, -0.0824,\n",
      "          -0.3972,  0.0887,  0.3973, -0.0809, -0.3958],\n",
      "         [ 0.0655, -0.2313, -0.5443, -0.5866, -0.2000, -0.3241,  0.0220,\n",
      "          -0.3987, -0.0678,  0.7846, -0.4739, -0.2767],\n",
      "         [ 0.0858, -0.3208, -0.4707, -0.2849, -0.1153, -0.2258, -0.0231,\n",
      "          -0.4034,  0.0494,  0.4909, -0.1348, -0.3403]]],\n",
      "       grad_fn=<BmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "scores = torch.bmm(queries, keys.transpose(1, 2))\n",
    "print(scores.size())\n",
    "\n",
    "scores = scores * (d_out ** -0.5)  # normalize\n",
    "attention_tensor = F.softmax(scores, dim=-1)\n",
    "print(\"\", attention_tensor.size(), sep=\"\\n\")\n",
    "print(f\"Sum of the rows of the last dimension:\\n\\t{attention_tensor.sum(dim=-1)}\")\n",
    "print(attention_tensor)\n",
    "\n",
    "hidden_states = torch.bmm(attention_tensor, values)\n",
    "print(\"\", hidden_states.size(), sep=\"\\n\")\n",
    "print(hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a894c577-42e7-40f2-8ae8-39bf478d43a9",
   "metadata": {},
   "source": [
    "## Multi-head Attention Implementaion\n",
    "\n",
    "Multi-head Attention scheme\n",
    "\n",
    "![Multi-head Attention scheme](./data/Multihead_Attention.jpg)\n",
    "\n",
    "Attention head scheme\n",
    "\n",
    "![Attention head scheme](./data/Attention_head.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "116ee44f-937e-44cd-b490-166f16a5a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "config = types.SimpleNamespace(\n",
    "    vocab_size = len(word2index),\n",
    "    embed_dim = 12,  # hidden size\n",
    "    num_heads = 3,\n",
    "    seq_len = 1024,\n",
    "    attention_dropout = 0.1,\n",
    "    residual_dropout = 0.1,\n",
    ")\n",
    "\n",
    "\n",
    "class Multihead_Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()  # initialize nn.Module\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.n_heads = config.num_heads\n",
    "        assert self.embed_dim % self.n_heads == 0, \"embedding dimension must be divisible by number of heads\"\n",
    "        self.head_size = self.embed_dim // self.n_heads  # 4 = 12 // 3\n",
    "\n",
    "        self.c_attn = nn.Linear(self.embed_dim,\n",
    "                                self.embed_dim * 3,\n",
    "                                bias=True)\n",
    "\n",
    "        self.scale = self.head_size ** -0.5\n",
    "\n",
    "        # The self.register_buffer() method in PyTorch's nn.Module is used to\n",
    "        # register a NON-LEARNABLE tensor\n",
    "        self.register_buffer(\n",
    "            \"mask\",  # self.mask\n",
    "            torch.tril(torch.ones(1, 1, config.seq_len, config.seq_len)) == 0,\n",
    "        )  # initialize non-learnable boolean (True/False) mask tensor of fize (1,1,1024,1024)\n",
    "\n",
    "        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(config.attention_dropout)  # dropout for attention matrix\n",
    "        self.resid_dropout = nn.Dropout(config.residual_dropout)  # dropout for resulting matrix\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"\\nForwarding embedded tensor...\")  # test message\n",
    "\n",
    "        b, t, c = x.shape  # batch_size, sentence_length, embed_dim\n",
    "        q, k, v = self.c_attn(x).chunk(3, dim=-1)  # query, key, value\n",
    "\n",
    "        # (0, 2, 1, 3) --> batch * n_heads * t * head_size\n",
    "        q = q.view(b, t, self.n_heads, self.head_size).permute(0, 2, 1, 3)\n",
    "        k = k.view(b, t, self.n_heads, self.head_size).permute(0, 2, 1, 3)\n",
    "        v = v.view(b, t, self.n_heads, self.head_size).permute(0, 2, 1, 3)\n",
    "\n",
    "        scores = (q@k.transpose(-2, -1)) * self.scale\n",
    "        scores = scores.masked_fill(\n",
    "            self.mask[:, :, :t, :t],  # truncate (1,1,1024,1024) to (1,1,t,t)\n",
    "            float(\"-inf\"),\n",
    "        )\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        scores = self.attn_dropout(scores)\n",
    "\n",
    "        attention = scores @ v\n",
    "        attention = attention.permute(0, 2, 1, 3).contiguous().view(b, t, c)\n",
    "\n",
    "        out = self.c_proj(attention)\n",
    "        out = self.resid_dropout(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d4ca3b-f5e4-4bde-9bcc-05c8384e6db8",
   "metadata": {},
   "source": [
    "## Test multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c72c6308-7f51-479f-99ed-b182fc0ec435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded tensor size: torch.Size([1, 5, 12])\n",
      "\n",
      "Forwarding embedded tensor...\n",
      "Hidden states size: torch.Size([1, 5, 12])\n"
     ]
    }
   ],
   "source": [
    "sentence = \"How are you doing ?\"\n",
    "input_tensor = convert2tensor(sentence)\n",
    "\n",
    "embedding = nn.Embedding(config.vocab_size, config.embed_dim)\n",
    "embedded = embedding(input_tensor)\n",
    "\n",
    "# tensor size: [batch_size, sentence_length, hidden_size]\n",
    "print(f\"Embedded tensor size: {embedded.size()}\")\n",
    "\n",
    "multihead_attention = Multihead_Attention(config)  # initialize object\n",
    "\n",
    "hidden_states = multihead_attention(embedded)  # call forward function\n",
    "print(f\"Hidden states size: {hidden_states.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85fc1d0-27b5-4b3b-a3ab-7bac15cd5178",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
