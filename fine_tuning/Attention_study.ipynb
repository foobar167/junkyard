{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "295685aa-717e-4be7-8a4e-783decd68e52",
   "metadata": {},
   "source": [
    "# Studying the mechanism of self-attention\n",
    "\n",
    "[GitHub link](https://github.com/foobar167/junkyard/blob/master/fine_tuning/Attention_study.ipynb) and\n",
    "[Colab link](https://colab.research.google.com/drive/1912LC9Tn7lyBzIwlZd2_QuNJqBtZzZ2D) to this Python script.\n",
    "\n",
    "* Video [Understanding the Self-Attention Mechanism in **8 min**](https://youtu.be/W28LfOld44Y) - theory\n",
    "* Video [Multi-head Attention Mechanism Explained](https://youtu.be/W6s9i02EiR0) in **4 min** - theory\n",
    "* Video [Implementing the Self-Attention Mechanism from Scratch in PyTorch](https://youtu.be/ZPLym9rJtM8) - **4 min** to implement + **11 min** testing and playing with code\n",
    "* Article [Attention Is All You Need](https://ar5iv.labs.arxiv.org/html/1706.03762)\n",
    "\n",
    "![Self-Attention scheme](./data/Self-Attention_Layer.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6007c17b-8883-4579-bb41-d034a097936b",
   "metadata": {},
   "source": [
    "## Attention implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b25d972-8b71-4800-8026-ea3652df2942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\" Attention mechanism implementation \"\"\"\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.Q = nn.Linear(d_in, d_out)  # initialize fully connected layer with random values\n",
    "        self.K = nn.Linear(d_in, d_out)\n",
    "        self.V = nn.Linear(d_in, d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.Q(x)  # function performs the math operation y=xA^{T}+b\n",
    "        keys = self.K(x)\n",
    "        values = self.V(x)\n",
    "        # torch.bmm  - Batch Matrix-Matrix Product\n",
    "        # Use torch.matmul for Multi-Head Attention\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2))\n",
    "        # Normalize. Normalized scores do not depend on the number of elements\n",
    "        scores = scores * (self.d_out ** -0.5)\n",
    "        # Apply the softmax so that the sum of the values in a row equals 1.0\n",
    "        attention = F.softmax(scores, dim=-1)  # apply sortmax to the last dimension\n",
    "        hidden_states = torch.bmm(attention, values)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4362f6a7-eba5-4ab0-a984-c9cd2e1ea9fb",
   "metadata": {},
   "source": [
    "## Implementation of a very simple tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2800bd3-57dc-42ce-aaf5-d36bf3f6285c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'BOS')\n",
      "(1, 'EOS')\n",
      "(2, 'how')\n",
      "(3, '?')\n",
      "(4, 'are')\n",
      "(5, 'and')\n",
      "(6, 'doing')\n",
      "(7, 'good')\n",
      "(8, 'am')\n",
      "(9, 'you')\n",
      "(10, 'i')\n",
      "\n",
      "\n",
      "('BOS', 0)\n",
      "('EOS', 1)\n",
      "('how', 2)\n",
      "('?', 3)\n",
      "('are', 4)\n",
      "('and', 5)\n",
      "('doing', 6)\n",
      "('good', 7)\n",
      "('am', 8)\n",
      "('you', 9)\n",
      "('i', 10)\n"
     ]
    }
   ],
   "source": [
    "BOS_token = 0  # BOS (Beginning of Sequence) the same as SOS (Start of Sequence)\n",
    "EOS_token = 1  # EOS (End of Sequence)\n",
    "\n",
    "index2word = {\n",
    "    BOS_token: \"BOS\",\n",
    "    EOS_token: \"EOS\",\n",
    "}\n",
    "\n",
    "text = \"How are you doing ? I am good and you ?\"\n",
    "vocabulary = set(text.lower().split(\" \"))  # set of unique words\n",
    "\n",
    "for word in vocabulary:\n",
    "    index2word[len(index2word)] = word\n",
    "\n",
    "print(*list(index2word.items()), sep=\"\\n\")\n",
    "\n",
    "word2index = {w: i for i, w in index2word.items()}\n",
    "print(\"\\n\", *list(word2index.items()), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d32e3be-3259-48ce-b3c8-95490c2eaeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor: tensor([[2, 4, 9, 6, 3]])\n",
      "Size of input tensor: torch.Size([1, 5])\n",
      "Converted sentence: How are you doing ?\n"
     ]
    }
   ],
   "source": [
    "def convert2tensor(sentence):\n",
    "    \"\"\" Convert sentence to tensor \"\"\"\n",
    "    words_list = sentence.lower().split(\" \")\n",
    "    indices = [word2index[word] for word in words_list]\n",
    "    # Add new dimension for batches using view(1,-1) at the beginning of the tensor\n",
    "    return torch.tensor(indices, dtype=torch.long).view(1, -1)\n",
    "\n",
    "\n",
    "def convert2sentence(tensor):\n",
    "    \"\"\" Convert tensor with tokens to sentence \"\"\"\n",
    "    indices = tensor.tolist()[0]\n",
    "    words_list = [index2word[index] for index in indices]\n",
    "    return \" \".join(words_list).capitalize()\n",
    "\n",
    "\n",
    "sentence = \"How are you doing ?\"\n",
    "\n",
    "input_tensor = convert2tensor(sentence)\n",
    "print(f\"Input tensor: {input_tensor}\")\n",
    "print(f\"Size of input tensor: {input_tensor.size()}\")\n",
    "\n",
    "sentence_converted = convert2sentence(input_tensor)\n",
    "print(f\"Converted sentence: {sentence_converted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bbd1d2-d243-41f7-816a-41fca333a580",
   "metadata": {},
   "source": [
    "## Create a very small neural network with attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fabe9c90-008e-463f-b04f-ffe29e1784f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.0587,  0.5892, -0.8730,  1.5204, -0.2458, -0.4056,  1.0812,\n",
      "          -0.8234,  0.0461, -1.3530, -0.4353,  0.5372],\n",
      "         [ 0.8191,  0.8516,  0.0484,  1.8933, -2.2349,  0.9433,  0.1067,\n",
      "           0.8285,  1.0547,  0.1152,  0.5180,  0.7104],\n",
      "         [-0.3938, -0.0363, -1.0047,  1.2835,  0.6363, -0.5910, -0.0508,\n",
      "           0.3778, -0.0836, -0.7666, -0.5286,  1.2667],\n",
      "         [-1.9934,  0.2951, -0.7795,  0.3300,  0.4425,  0.7721,  0.3109,\n",
      "           2.1065, -0.0852,  0.6770, -0.6713,  0.2350],\n",
      "         [ 0.4861, -1.1047, -0.8358, -0.4331, -1.5519, -0.5540,  0.7782,\n",
      "          -1.2695,  0.5603,  0.5940, -0.4143,  1.2459]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "Size of embedded tensor: torch.Size([1, 5, 12])\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_SIZE = 12\n",
    "VOCAB_SIZE = len(word2index)\n",
    "\n",
    "sentence = \"How are you doing ?\"\n",
    "input_tensor = convert2tensor(sentence)\n",
    "\n",
    "embedding = nn.Embedding(VOCAB_SIZE, HIDDEN_SIZE)\n",
    "embedded = embedding(input_tensor)\n",
    "\n",
    "print(embedded)\n",
    "# size: [batch_size, sentence_length, hidden_size]\n",
    "print(f\"Size of embedded tensor: {embedded.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1832c7e-b92e-420d-857b-e8f8034d9e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0850,  0.0218, -0.0600, -0.3665,  0.5640, -0.2356,  0.0426,\n",
      "          -0.1988,  0.4133,  0.5180,  0.4875,  0.2378],\n",
      "         [ 0.1598, -0.0544, -0.0115, -0.3971,  0.5660, -0.2071,  0.0570,\n",
      "          -0.1700,  0.3649,  0.4253,  0.4144,  0.2667],\n",
      "         [ 0.0835,  0.0121, -0.0396, -0.3072,  0.5538, -0.2886,  0.0469,\n",
      "          -0.1749,  0.3380,  0.4791,  0.4640,  0.2334],\n",
      "         [ 0.0735,  0.0074, -0.0534, -0.3287,  0.5718, -0.2741,  0.0937,\n",
      "          -0.2135,  0.3769,  0.5014,  0.4872,  0.1904],\n",
      "         [ 0.0896,  0.0123, -0.0451, -0.3228,  0.5547, -0.2576,  0.0227,\n",
      "          -0.1709,  0.3516,  0.4803,  0.4584,  0.2517]]],\n",
      "       grad_fn=<BmmBackward0>)\n",
      "torch.Size([1, 5, 12])\n"
     ]
    }
   ],
   "source": [
    "attention = Attention(HIDDEN_SIZE, HIDDEN_SIZE)  # initialize object\n",
    "\n",
    "hidden_states = attention(embedded)  # call forward funcion\n",
    "\n",
    "print(hidden_states)\n",
    "print(hidden_states.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "958b9eb8-acc9-499a-9f7f-1254b503b5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 12])\n",
      "torch.Size([1, 5, 12])\n",
      "torch.Size([1, 5, 12])\n"
     ]
    }
   ],
   "source": [
    "d_in = HIDDEN_SIZE\n",
    "d_out = HIDDEN_SIZE\n",
    "\n",
    "Q = nn.Linear(d_in, d_out)  # initialize fully connected layer with random values\n",
    "K = nn.Linear(d_in, d_out)\n",
    "V = nn.Linear(d_in, d_out)\n",
    "\n",
    "queries = Q(embedded)  # function performs the math operation y=xA^{T}+b\n",
    "keys = K(embedded)\n",
    "values = V(embedded)\n",
    "\n",
    "print(queries.size(), keys.size(), values.size(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeb4f6f1-c456-4b59-ae4b-46906ee897c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 5])\n",
      "\n",
      "torch.Size([1, 5, 5])\n",
      "Sum of the rows of the last dimension:\n",
      "\ttensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "tensor([[[0.2148, 0.2227, 0.2044, 0.1723, 0.1857],\n",
      "         [0.1697, 0.1840, 0.2207, 0.2094, 0.2162],\n",
      "         [0.1942, 0.2211, 0.2339, 0.1662, 0.1847],\n",
      "         [0.2044, 0.1757, 0.2147, 0.1637, 0.2415],\n",
      "         [0.1964, 0.1768, 0.2207, 0.1553, 0.2508]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "torch.Size([1, 5, 12])\n",
      "tensor([[[ 0.0998, -0.1397,  0.2777,  0.5193,  0.1172, -0.8438,  0.0805,\n",
      "           0.2359, -0.1140,  0.3579, -0.1809,  0.3405],\n",
      "         [ 0.1004, -0.1379,  0.2651,  0.5714,  0.1167, -0.8165,  0.0713,\n",
      "           0.2358, -0.1519,  0.3807, -0.2103,  0.2778],\n",
      "         [ 0.0875, -0.1543,  0.2816,  0.5295,  0.1123, -0.8349,  0.0684,\n",
      "           0.2363, -0.1320,  0.3502, -0.1821,  0.3394],\n",
      "         [ 0.0992, -0.1796,  0.2455,  0.5202,  0.1077, -0.8397,  0.1039,\n",
      "           0.2282, -0.1397,  0.3218, -0.2293,  0.2924],\n",
      "         [ 0.0970, -0.1899,  0.2460,  0.5145,  0.0997, -0.8447,  0.1023,\n",
      "           0.2281, -0.1376,  0.3141, -0.2283,  0.2955]]],\n",
      "       grad_fn=<BmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "scores = torch.bmm(queries, keys.transpose(1, 2))\n",
    "print(scores.size())\n",
    "\n",
    "scores = scores * (d_out ** -0.5)  # normalize\n",
    "attention_tensor = F.softmax(scores, dim=-1)\n",
    "print(\"\", attention_tensor.size(), sep=\"\\n\")\n",
    "print(f\"Sum of the rows of the last dimension:\\n\\t{attention_tensor.sum(dim=-1)}\")\n",
    "print(attention_tensor)\n",
    "\n",
    "hidden_states = torch.bmm(attention_tensor, values)\n",
    "print(\"\", hidden_states.size(), sep=\"\\n\")\n",
    "print(hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a894c577-42e7-40f2-8ae8-39bf478d43a9",
   "metadata": {},
   "source": [
    "## Multi-head Attention Implementaion\n",
    "\n",
    "Multi-head Attention scheme\n",
    "\n",
    "![Multi-head Attention scheme](./data/Multihead_Attention.jpg)\n",
    "\n",
    "Attention head scheme\n",
    "\n",
    "![Attention head scheme](./data/Attention_head.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "116ee44f-937e-44cd-b490-166f16a5a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "config = types.SimpleNamespace(\n",
    "    vocab_size = len(word2index),\n",
    "    embed_dim = 12,  # hidden size\n",
    "    num_heads = 3,\n",
    "    seq_len = 1024,\n",
    "    attention_dropout = 0.1,\n",
    "    residual_dropout = 0.1,\n",
    ")\n",
    "\n",
    "\n",
    "class Multihead_Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()  # initialize nn.Module\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.n_heads = config.num_heads\n",
    "        assert self.embed_dim % self.n_heads == 0, \"embedding dimension must be divisible by number of heads\"\n",
    "        self.head_size = self.embed_dim // self.n_heads  # 4 = 12 // 3\n",
    "\n",
    "        self.c_attn = nn.Linear(self.embed_dim,\n",
    "                                self.embed_dim * 3,\n",
    "                                bias=True)\n",
    "\n",
    "        self.scale = self.head_size ** -0.5\n",
    "\n",
    "        # The self.register_buffer() method in PyTorch's nn.Module is used to\n",
    "        # register a NON-LEARNABLE tensor\n",
    "        self.register_buffer(\n",
    "            \"mask\",  # self.mask\n",
    "            torch.tril(torch.ones(1, 1, config.seq_len, config.seq_len)) == 0,\n",
    "        )  # initialize non-learnable boolean (True/False) mask tensor of fize (1,1,1024,1024)\n",
    "\n",
    "        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(config.attention_dropout)  # dropout for attention matrix\n",
    "        self.resid_dropout = nn.Dropout(config.residual_dropout)  # dropout for resulting matrix\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"\\nForwarding embedded tensor...\")  # test message\n",
    "\n",
    "        b, t, c = x.shape  # batch_size, sentence_length, embed_dim\n",
    "        q, k, v = self.c_attn(x).chunk(3, dim=-1)  # query, key, value\n",
    "\n",
    "        # (0, 2, 1, 3) --> batch * n_heads * t * head_size\n",
    "        q = q.view(b, t, self.n_heads, self.head_size).permute(0, 2, 1, 3)\n",
    "        k = k.view(b, t, self.n_heads, self.head_size).permute(0, 2, 1, 3)\n",
    "        v = v.view(b, t, self.n_heads, self.head_size).permute(0, 2, 1, 3)\n",
    "\n",
    "        scores = (q@k.transpose(-2, -1)) * self.scale\n",
    "        scores = scores.masked_fill(\n",
    "            self.mask[:, :, :t, :t],  # truncate (1,1,1024,1024) to (1,1,t,t)\n",
    "            float(\"-inf\"),\n",
    "        )\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        scores = self.attn_dropout(scores)\n",
    "\n",
    "        attention = scores @ v\n",
    "        attention = attention.permute(0, 2, 1, 3).contiguous().view(b, t, c)\n",
    "\n",
    "        out = self.c_proj(attention)\n",
    "        out = self.resid_dropout(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d4ca3b-f5e4-4bde-9bcc-05c8384e6db8",
   "metadata": {},
   "source": [
    "## Test multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c72c6308-7f51-479f-99ed-b182fc0ec435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded tensor size: torch.Size([1, 5, 12])\n",
      "\n",
      "Forwarding embedded tensor...\n",
      "Hidden states size: torch.Size([1, 5, 12])\n"
     ]
    }
   ],
   "source": [
    "sentence = \"How are you doing ?\"\n",
    "input_tensor = convert2tensor(sentence)\n",
    "\n",
    "embedding = nn.Embedding(config.vocab_size, config.embed_dim)\n",
    "embedded = embedding(input_tensor)\n",
    "\n",
    "# tensor size: [batch_size, sentence_length, hidden_size]\n",
    "print(f\"Embedded tensor size: {embedded.size()}\")\n",
    "\n",
    "multihead_attention = Multihead_Attention(config)  # initialize object\n",
    "\n",
    "hidden_states = multihead_attention(embedded)  # call forward function\n",
    "print(f\"Hidden states size: {hidden_states.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85fc1d0-27b5-4b3b-a3ab-7bac15cd5178",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
